===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               3,072
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 2,816
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        2,230,016
│    │    └─ModuleList: 3-2                        2,230,016
│    │    └─ModuleList: 3-3                        2,230,016
│    │    └─ModuleList: 3-4                        2,230,016
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              512
│    └─Linear: 2-5                                 65,792
===========================================================================
Total params: 8,992,256
Trainable params: 8,992,256
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               3,072
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 2,816
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        2,230,016
│    │    └─ModuleList: 3-2                        2,230,016
│    │    └─ModuleList: 3-3                        2,230,016
│    │    └─ModuleList: 3-4                        2,230,016
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              512
│    └─Linear: 2-5                                 65,792
===========================================================================
Total params: 8,992,256
Trainable params: 8,992,256
Non-trainable params: 0
===========================================================================
Loading data for model
Traceback (most recent call last):
  File "src/train.py", line 189, in <module>
    main()
  File "src/train.py", line 172, in main
    test_df = torch.load(os.path.join(processed_dir, f"{app_name}.df.pt"))
  File "/home/neelesh/miniconda3/envs/comp/lib/python3.8/site-packages/torch/serialization.py", line 699, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/neelesh/miniconda3/envs/comp/lib/python3.8/site-packages/torch/serialization.py", line 231, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/neelesh/miniconda3/envs/comp/lib/python3.8/site-packages/torch/serialization.py", line 212, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'processed/619.lbm-s0.df.pt'
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Traceback (most recent call last):
  File "src/train_kd.py", line 209, in <module>
    main()
  File "src/train_kd.py", line 190, in main
    test_df = torch.load(os.path.join(processed_dir, f"{app_name}.df.pt"))
  File "/home/neelesh/miniconda3/envs/comp/lib/python3.8/site-packages/torch/serialization.py", line 699, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/neelesh/miniconda3/envs/comp/lib/python3.8/site-packages/torch/serialization.py", line 231, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/neelesh/miniconda3/envs/comp/lib/python3.8/site-packages/torch/serialization.py", line 212, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'processed/433.milc-s0.df.pt'
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6287616639 - test_loss: 0.6512961811
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5174079063 - test_loss: 0.5232031828
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4272791757 - test_loss: 0.4431728465
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3729598800 - test_loss: 0.3892825206
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3346242512 - test_loss: 0.3484262740
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3059219560 - test_loss: 0.3166210132
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2842508589 - test_loss: 0.2916125488
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2677432788 - test_loss: 0.2718859647
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2553470218 - test_loss: 0.2562608252
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2460370562 - test_loss: 0.2439465935
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2391736648 - test_loss: 0.2342415483
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2341625617 - test_loss: 0.2266142902
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2306826108 - test_loss: 0.2206937779
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2282552724 - test_loss: 0.2160753493
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2265017270 - test_loss: 0.2125511081
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2254815224 - test_loss: 0.2098657235
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2246840451 - test_loss: 0.2078243536
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2242002991 - test_loss: 0.2063968955
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2239011187 - test_loss: 0.2053425970
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2236810517 - test_loss: 0.2046075987
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2233822491 - test_loss: 0.2039029391
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2230943805 - test_loss: 0.2033493188
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2225726089 - test_loss: 0.2023783313
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2215420600 - test_loss: 0.2007350542
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2200069265 - test_loss: 0.1989687313
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2189748399 - test_loss: 0.1973969864
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2183534364 - test_loss: 0.1965424197
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2178553336 - test_loss: 0.1961026027
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2175224051 - test_loss: 0.1959673733
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2172417429 - test_loss: 0.1952259999
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2169930675 - test_loss: 0.1953448286
Early Stop Left: 4
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2168223058 - test_loss: 0.1950860206
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2166037431 - test_loss: 0.1945372264
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2162425202 - test_loss: 0.1944483232
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2159527237 - test_loss: 0.1939718812
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2157581185 - test_loss: 0.1939765141
Early Stop Left: 4
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2154100046 - test_loss: 0.1931943688
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2151492993 - test_loss: 0.1926868081
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2150290049 - test_loss: 0.1923717863
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2147547855 - test_loss: 0.1921128260
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2146356703 - test_loss: 0.1920831896
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2144539355 - test_loss: 0.1918250141
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2141896792 - test_loss: 0.1914716822
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2137958428 - test_loss: 0.1909091342
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2133856534 - test_loss: 0.1903733550
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2130021553 - test_loss: 0.1899714487
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2126117222 - test_loss: 0.1895327005
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2122843984 - test_loss: 0.1888534220
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2119051721 - test_loss: 0.1886327198
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2115733907 - test_loss: 0.1880988942
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2112460240 - test_loss: 0.1877668541
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2109442220 - test_loss: 0.1872550706
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2106111798 - test_loss: 0.1867621196
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2101850393 - test_loss: 0.1861773485
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2097313795 - test_loss: 0.1856090164
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2093208415 - test_loss: 0.1850166054
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2088438093 - test_loss: 0.1842813335
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2082294195 - test_loss: 0.1832097479
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2076374753 - test_loss: 0.1828362957
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2069075308 - test_loss: 0.1814782208
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2058749079 - test_loss: 0.1803335518
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2046107012 - test_loss: 0.1784458357
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2031132339 - test_loss: 0.1762485810
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2014127418 - test_loss: 0.1738525001
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.1992190532 - test_loss: 0.1716003319
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.1968802792 - test_loss: 0.1675214008
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.1946289004 - test_loss: 0.1648608005
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.1927305599 - test_loss: 0.1629415981
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.1910483227 - test_loss: 0.1609012225
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.1892906438 - test_loss: 0.1581516282
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.1877304914 - test_loss: 0.1561634161
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.1863300640 - test_loss: 0.1545391184
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.1850248714 - test_loss: 0.1528598180
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.1838027393 - test_loss: 0.1505754105
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.1827827313 - test_loss: 0.1503500878
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.1818753527 - test_loss: 0.1493128689
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.1810849725 - test_loss: 0.1478762827
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.1802877496 - test_loss: 0.1464399023
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.1794946306 - test_loss: 0.1460899234
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.1789056925 - test_loss: 0.1445416125
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.1781805997 - test_loss: 0.1441387949
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.1776666136 - test_loss: 0.1434495756
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.1771096114 - test_loss: 0.1422739388
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.1766514050 - test_loss: 0.1424517946
Early Stop Left: 4
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.1762184055 - test_loss: 0.1413761210
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.1758101350 - test_loss: 0.1408727045
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.1753563178 - test_loss: 0.1407945460
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.1750706556 - test_loss: 0.1399804845
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.1747399689 - test_loss: 0.1393812790
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.1743606447 - test_loss: 0.1390083673
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.1741513128 - test_loss: 0.1384395193
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.1738726501 - test_loss: 0.1382298074
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.1735403707 - test_loss: 0.1374595034
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.1733360836 - test_loss: 0.1376596232
Early Stop Left: 4
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.1731758522 - test_loss: 0.1375073246
Early Stop Left: 3
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.1728839212 - test_loss: 0.1374249534
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.1726649293 - test_loss: 0.1365774832
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.1725088974 - test_loss: 0.1364080429
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.1722256591 - test_loss: 0.1355357336
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.1720676831 - test_loss: 0.1357701121
Early Stop Left: 4
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.1719054014 - test_loss: 0.1359459716
Early Stop Left: 3
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.1718016488 - test_loss: 0.1350262819
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.1716209702 - test_loss: 0.1348415238
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.1714876378 - test_loss: 0.1351421618
Early Stop Left: 4
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.1713375718 - test_loss: 0.1345826325
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.1710395128 - test_loss: 0.1338221121
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.1710092752 - test_loss: 0.1345101028
Early Stop Left: 4
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.1708825695 - test_loss: 0.1342085642
Early Stop Left: 3
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.1706766207 - test_loss: 0.1344867922
Early Stop Left: 2
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.1705313626 - test_loss: 0.1343111241
Early Stop Left: 1
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.1703411553 - test_loss: 0.1333490693
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.1701915897 - test_loss: 0.1333638056
Early Stop Left: 4
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.1701026631 - test_loss: 0.1329070985
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.1699425258 - test_loss: 0.1327695020
-------- Save Best Model! --------
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.1698207152 - test_loss: 0.1326730258
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.1697219833 - test_loss: 0.1336058229
Early Stop Left: 4
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.1695629038 - test_loss: 0.1327874430
Early Stop Left: 3
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.1694615238 - test_loss: 0.1319981788
-------- Save Best Model! --------
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.1693830221 - test_loss: 0.1325061666
Early Stop Left: 4
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.1693142918 - test_loss: 0.1322761971
Early Stop Left: 3
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.1691218715 - test_loss: 0.1318780645
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.1689965876 - test_loss: 0.1320640131
Early Stop Left: 4
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.1688526890 - test_loss: 0.1326691732
Early Stop Left: 3
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.1687390296 - test_loss: 0.1312264611
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.1686654113 - test_loss: 0.1311545131
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.1684709045 - test_loss: 0.1306596658
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.1684360398 - test_loss: 0.1309893794
Early Stop Left: 4
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.1683236529 - test_loss: 0.1319983550
Early Stop Left: 3
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.1681698994 - test_loss: 0.1306801839
Early Stop Left: 2
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.1681256741 - test_loss: 0.1310801061
Early Stop Left: 1
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.1679701842 - test_loss: 0.1309843100
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 16%|█▌        | 19/121 [00:00<00:00, 186.20it/s] 32%|███▏      | 39/121 [00:00<00:00, 190.44it/s] 49%|████▉     | 59/121 [00:00<00:00, 192.54it/s] 65%|██████▌   | 79/121 [00:00<00:00, 193.77it/s] 82%|████████▏ | 99/121 [00:00<00:00, 193.12it/s] 98%|█████████▊| 119/121 [00:00<00:00, 192.95it/s]100%|██████████| 121/121 [00:00<00:00, 193.38it/s]
Best micro threshold=0.351270, fscore=0.717
p,r,f1: 0.6723834199699708 0.7690092603355418 0.7174576137619805
throttleing by fixed threshold: 0.5
p,r,f1: 0.779113262448416 0.619156108334302 0.6899854142684553
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3512699007987976,
                 'p': 0.6723834199699708,
                 'r': 0.7690092603355418,
                 'f1': 0.7174576137619805},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.779113262448416,
                 'r': 0.619156108334302,
                 'f1': 0.6899854142684553}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
Traceback (most recent call last):
  File "src/train_kd.py", line 209, in <module>
    main()
  File "src/train_kd.py", line 163, in main
    gpu_id = sys.argv[4]
IndexError: list index out of range
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5398671101 - test_loss: 0.4420080249
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.3415066090 - test_loss: 0.3127396764
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.2705932272 - test_loss: 0.2568343946
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2414662101 - test_loss: 0.2301207639
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2289126252 - test_loss: 0.2162931766
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2233756468 - test_loss: 0.2087469762
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2210436394 - test_loss: 0.2045495663
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2199086436 - test_loss: 0.2022206140
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2193805785 - test_loss: 0.2006286811
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2188034218 - test_loss: 0.1994712151
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2171414481 - test_loss: 0.1959373036
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2140912755 - test_loss: 0.1926343068
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2126305446 - test_loss: 0.1909767645
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2111636787 - test_loss: 0.1892446485
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2094471098 - test_loss: 0.1871457308
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2081086493 - test_loss: 0.1862075863
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2070585297 - test_loss: 0.1843460515
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2059840723 - test_loss: 0.1828282366
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2043110572 - test_loss: 0.1803077681
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2018819748 - test_loss: 0.1765351957
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.1979784983 - test_loss: 0.1705655993
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.1934109825 - test_loss: 0.1650512714
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.1888676824 - test_loss: 0.1591487337
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.1850453374 - test_loss: 0.1559539229
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.1820095787 - test_loss: 0.1520716150
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.1791777336 - test_loss: 0.1488777155
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.1769406042 - test_loss: 0.1452788901
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.1750309474 - test_loss: 0.1422392360
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.1733617869 - test_loss: 0.1423679788
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.1718473273 - test_loss: 0.1390506627
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.1705072577 - test_loss: 0.1370628128
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.1692960726 - test_loss: 0.1365275062
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.1679190442 - test_loss: 0.1354029394
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.1665682757 - test_loss: 0.1322537238
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.1653886471 - test_loss: 0.1305530327
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.1644230708 - test_loss: 0.1302268260
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.1632051109 - test_loss: 0.1270530228
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.1621098427 - test_loss: 0.1278696359
Early Stop Left: 4
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.1609891654 - test_loss: 0.1245448267
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.1599469437 - test_loss: 0.1222416710
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.1589169630 - test_loss: 0.1226107694
Early Stop Left: 4
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.1579110577 - test_loss: 0.1215172240
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.1569667025 - test_loss: 0.1205663909
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.1559838188 - test_loss: 0.1202467512
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.1553148616 - test_loss: 0.1180200542
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.1545420031 - test_loss: 0.1168850795
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.1539244198 - test_loss: 0.1149741775
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.1532997734 - test_loss: 0.1143375615
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.1528367120 - test_loss: 0.1141369545
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.1522854917 - test_loss: 0.1132535446
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.1518834583 - test_loss: 0.1125797867
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.1514372882 - test_loss: 0.1134472625
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.1510738368 - test_loss: 0.1123670633
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.1506594403 - test_loss: 0.1106248650
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.1502797649 - test_loss: 0.1111406738
Early Stop Left: 4
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.1500503324 - test_loss: 0.1103446340
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.1496239029 - test_loss: 0.1107326862
Early Stop Left: 4
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.1492527457 - test_loss: 0.1102607678
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.1489294704 - test_loss: 0.1100438247
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.1485852465 - test_loss: 0.1092287681
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.1482443238 - test_loss: 0.1084532959
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.1478636906 - test_loss: 0.1072309222
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.1476587626 - test_loss: 0.1073412698
Early Stop Left: 4
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.1474260188 - test_loss: 0.1079082349
Early Stop Left: 3
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.1470517388 - test_loss: 0.1065316843
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.1468614367 - test_loss: 0.1069322893
Early Stop Left: 4
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.1464990749 - test_loss: 0.1063520219
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.1463990308 - test_loss: 0.1062117797
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.1461778897 - test_loss: 0.1058899080
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.1458253385 - test_loss: 0.1062254758
Early Stop Left: 4
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.1456884491 - test_loss: 0.1058767302
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.1455433535 - test_loss: 0.1045482294
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.1452022390 - test_loss: 0.1043969489
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.1449866513 - test_loss: 0.1046296859
Early Stop Left: 4
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.1447549679 - test_loss: 0.1051220531
Early Stop Left: 3
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.1445361664 - test_loss: 0.1032611816
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.1444194846 - test_loss: 0.1041881629
Early Stop Left: 4
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.1442286996 - test_loss: 0.1033063452
Early Stop Left: 3
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.1438802728 - test_loss: 0.1023672017
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.1437458977 - test_loss: 0.1035503073
Early Stop Left: 4
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.1435187466 - test_loss: 0.1023451133
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.1433078840 - test_loss: 0.1020157541
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.1431094797 - test_loss: 0.1022018221
Early Stop Left: 4
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.1429271382 - test_loss: 0.1014266443
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.1427648379 - test_loss: 0.1024385163
Early Stop Left: 4
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.1426930468 - test_loss: 0.1018895052
Early Stop Left: 3
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.1423267600 - test_loss: 0.1007271709
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.1422400280 - test_loss: 0.1003625308
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.1419542824 - test_loss: 0.1000731605
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.1417116961 - test_loss: 0.1014166179
Early Stop Left: 4
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.1415919857 - test_loss: 0.1002524366
Early Stop Left: 3
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.1414136309 - test_loss: 0.1000200015
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.1411640017 - test_loss: 0.0991183719
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.1410320090 - test_loss: 0.1019596519
Early Stop Left: 4
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.1408304554 - test_loss: 0.1009681019
Early Stop Left: 3
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.1406404083 - test_loss: 0.1003153612
Early Stop Left: 2
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.1404068049 - test_loss: 0.0983613130
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.1402865232 - test_loss: 0.0974082885
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.1399807273 - test_loss: 0.0967381962
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.1399149321 - test_loss: 0.0983953786
Early Stop Left: 4
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.1396036883 - test_loss: 0.0966109755
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.1395583444 - test_loss: 0.0974704904
Early Stop Left: 4
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.1393260202 - test_loss: 0.0957746922
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.1392479445 - test_loss: 0.0984702631
Early Stop Left: 4
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.1390420760 - test_loss: 0.0969372605
Early Stop Left: 3
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.1387267866 - test_loss: 0.0947981493
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.1387318700 - test_loss: 0.0969681911
Early Stop Left: 4
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.1385235207 - test_loss: 0.0969169078
Early Stop Left: 3
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.1383631189 - test_loss: 0.0966150511
Early Stop Left: 2
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.1381510877 - test_loss: 0.0956168353
Early Stop Left: 1
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.1380225476 - test_loss: 0.0973192782
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 17%|█▋        | 20/121 [00:00<00:00, 194.28it/s] 33%|███▎      | 40/121 [00:00<00:00, 191.52it/s] 50%|████▉     | 60/121 [00:00<00:00, 191.17it/s] 66%|██████▌   | 80/121 [00:00<00:00, 191.53it/s] 83%|████████▎ | 100/121 [00:00<00:00, 191.44it/s] 99%|█████████▉| 120/121 [00:00<00:00, 191.36it/s]100%|██████████| 121/121 [00:00<00:00, 192.40it/s]
Best micro threshold=0.332581, fscore=0.789
p,r,f1: 0.7350960726888831 0.8503793250416521 0.7885464549355112
throttleing by fixed threshold: 0.5
p,r,f1: 0.8484761269995152 0.6944926188538881 0.7638008340201116
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.33258095383644104,
                 'p': 0.7350960726888831,
                 'r': 0.8503793250416521,
                 'f1': 0.7885464549355112},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.8484761269995152,
                 'r': 0.6944926188538881,
                 'f1': 0.7638008340201116}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5671666559 - test_loss: 0.4423499775
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.3539368326 - test_loss: 0.3107631985
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.2736314265 - test_loss: 0.2536471741
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2388358093 - test_loss: 0.2261758545
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2225907432 - test_loss: 0.2117509604
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2146212149 - test_loss: 0.2036638001
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2107384758 - test_loss: 0.1989276303
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2086375898 - test_loss: 0.1960697154
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2075502965 - test_loss: 0.1940993451
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2066580033 - test_loss: 0.1924827414
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2047053739 - test_loss: 0.1884947875
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2010066177 - test_loss: 0.1852602540
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.1993316691 - test_loss: 0.1832242399
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.1977032536 - test_loss: 0.1813210352
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.1958072296 - test_loss: 0.1793343346
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.1943781056 - test_loss: 0.1783685774
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.1931888990 - test_loss: 0.1764960492
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.1919393737 - test_loss: 0.1748967855
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.1898865867 - test_loss: 0.1719403643
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.1868817632 - test_loss: 0.1677161951
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.1820470732 - test_loss: 0.1609697392
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.1761063406 - test_loss: 0.1539973681
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.1701172292 - test_loss: 0.1476668965
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.1656225475 - test_loss: 0.1439198752
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.1622303849 - test_loss: 0.1398279006
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.1595153806 - test_loss: 0.1379235052
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.1575863062 - test_loss: 0.1348664844
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.1560165517 - test_loss: 0.1327386111
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.1546780822 - test_loss: 0.1329696379
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.1535228775 - test_loss: 0.1304219931
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.1524761357 - test_loss: 0.1291533565
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.1514777600 - test_loss: 0.1287573798
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.1504263028 - test_loss: 0.1281538483
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.1493672216 - test_loss: 0.1259949303
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.1482135148 - test_loss: 0.1239958476
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.1469880739 - test_loss: 0.1228661006
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.1454439483 - test_loss: 0.1204124746
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.1441324126 - test_loss: 0.1204130480
Early Stop Left: 4
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.1427857384 - test_loss: 0.1170329122
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.1414197277 - test_loss: 0.1154080116
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.1401570775 - test_loss: 0.1147640097
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.1389998660 - test_loss: 0.1134011968
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.1379728375 - test_loss: 0.1125123573
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.1369684655 - test_loss: 0.1120065677
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.1362109956 - test_loss: 0.1105103606
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.1353862132 - test_loss: 0.1094348324
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.1346523376 - test_loss: 0.1077181081
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.1338853246 - test_loss: 0.1071776150
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.1332742894 - test_loss: 0.1072820033
Early Stop Left: 4
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.1325997010 - test_loss: 0.1056486303
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.1319875609 - test_loss: 0.1050599150
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.1313898643 - test_loss: 0.1054472878
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.1308857938 - test_loss: 0.1042058874
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.1303118161 - test_loss: 0.1027399350
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.1297605089 - test_loss: 0.1027881548
Early Stop Left: 4
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.1294923357 - test_loss: 0.1018041750
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.1289262802 - test_loss: 0.1013232363
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.1284694731 - test_loss: 0.1018327010
Early Stop Left: 4
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.1280988243 - test_loss: 0.1008322078
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.1277814957 - test_loss: 0.1012558108
Early Stop Left: 4
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.1273218456 - test_loss: 0.0996763448
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.1268784786 - test_loss: 0.0989379374
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.1266063215 - test_loss: 0.0994579295
Early Stop Left: 4
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.1263435702 - test_loss: 0.0988291924
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.1258888270 - test_loss: 0.0978340922
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.1255997531 - test_loss: 0.0986118260
Early Stop Left: 4
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.1251693618 - test_loss: 0.0981195099
Early Stop Left: 3
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.1249763765 - test_loss: 0.0969689379
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.1246903586 - test_loss: 0.0971906735
Early Stop Left: 4
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.1243116927 - test_loss: 0.0971194281
Early Stop Left: 3
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.1241568300 - test_loss: 0.0957539779
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.1239104592 - test_loss: 0.0960418746
Early Stop Left: 4
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.1236325134 - test_loss: 0.0948437841
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.1233496771 - test_loss: 0.0946315747
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.1231379900 - test_loss: 0.0967897876
Early Stop Left: 4
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.1228904694 - test_loss: 0.0941099343
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.1226933774 - test_loss: 0.0946055702
Early Stop Left: 4
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.1225484289 - test_loss: 0.0941324105
Early Stop Left: 3
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.1221807184 - test_loss: 0.0938257645
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.1220984720 - test_loss: 0.0944978148
Early Stop Left: 4
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.1218047100 - test_loss: 0.0931144053
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.1216062824 - test_loss: 0.0954196569
Early Stop Left: 4
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.1215000394 - test_loss: 0.0934091593
Early Stop Left: 3
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.1212611335 - test_loss: 0.0925136189
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.1211097663 - test_loss: 0.0932971046
Early Stop Left: 4
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.1210472150 - test_loss: 0.0914350764
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.1206103501 - test_loss: 0.0924683283
Early Stop Left: 4
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.1206491658 - test_loss: 0.0918362550
Early Stop Left: 3
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.1202984944 - test_loss: 0.0920781487
Early Stop Left: 2
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.1200719991 - test_loss: 0.0917672655
Early Stop Left: 1
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.1200086267 - test_loss: 0.0918426905
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 16%|█▌        | 19/121 [00:00<00:00, 187.16it/s] 31%|███▏      | 38/121 [00:00<00:00, 186.20it/s] 47%|████▋     | 57/121 [00:00<00:00, 186.01it/s] 63%|██████▎   | 76/121 [00:00<00:00, 186.29it/s] 79%|███████▊  | 95/121 [00:00<00:00, 186.45it/s] 94%|█████████▍| 114/121 [00:00<00:00, 185.92it/s]100%|██████████| 121/121 [00:00<00:00, 186.96it/s]
Best micro threshold=0.335948, fscore=0.774
p,r,f1: 0.7213171409970985 0.8345646867371848 0.7738194547731599
throttleing by fixed threshold: 0.5
p,r,f1: 0.8204301814311269 0.6845100546321051 0.7463322394546603
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.33594757318496704,
                 'p': 0.7213171409970985,
                 'r': 0.8345646867371848,
                 'f1': 0.7738194547731599},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.8204301814311269,
                 'r': 0.6845100546321051,
                 'f1': 0.7463322394546603}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6152160513 - test_loss: 0.6181722278
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4794932400 - test_loss: 0.4775367832
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3905821515 - test_loss: 0.4002859221
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3382764381 - test_loss: 0.3475915750
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3025876685 - test_loss: 0.3095608031
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2776284403 - test_loss: 0.2816267156
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2602126920 - test_loss: 0.2609453996
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2479803125 - test_loss: 0.2455883231
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2395887057 - test_loss: 0.2340799894
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2338232810 - test_loss: 0.2255235809
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2299705857 - test_loss: 0.2191495301
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2274239124 - test_loss: 0.2144209316
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2258967606 - test_loss: 0.2110320117
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2249589112 - test_loss: 0.2085498815
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2242900622 - test_loss: 0.2068085776
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2240202189 - test_loss: 0.2055644271
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2236869481 - test_loss: 0.2046072345
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2234184964 - test_loss: 0.2039538633
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2230470198 - test_loss: 0.2031954070
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2223132840 - test_loss: 0.2018863277
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2205439874 - test_loss: 0.1994186906
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2191686584 - test_loss: 0.1976066593
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2183546135 - test_loss: 0.1964845917
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2177869492 - test_loss: 0.1960448663
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2173792282 - test_loss: 0.1956820998
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2169754190 - test_loss: 0.1952361325
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2166780211 - test_loss: 0.1947174253
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2162600505 - test_loss: 0.1942270221
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2157426711 - test_loss: 0.1938887271
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2150960667 - test_loss: 0.1924497098
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2144223600 - test_loss: 0.1917705216
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2138779911 - test_loss: 0.1911207452
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2134149349 - test_loss: 0.1901821678
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2129328831 - test_loss: 0.1899489166
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2126061103 - test_loss: 0.1895583516
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2124279955 - test_loss: 0.1895303597
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2120722616 - test_loss: 0.1888536537
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2117979692 - test_loss: 0.1884391912
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2116252402 - test_loss: 0.1878734822
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2112577058 - test_loss: 0.1876918645
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2110175064 - test_loss: 0.1871605022
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2107254018 - test_loss: 0.1871776778
Early Stop Left: 4
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2104247660 - test_loss: 0.1863166864
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2099865066 - test_loss: 0.1859370261
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2096028291 - test_loss: 0.1851036691
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2092416580 - test_loss: 0.1844602373
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2088087589 - test_loss: 0.1840547717
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2083907280 - test_loss: 0.1832771852
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2077025484 - test_loss: 0.1824229116
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2067971370 - test_loss: 0.1811708192
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2057089367 - test_loss: 0.1794664194
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2046777160 - test_loss: 0.1788804071
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2036973821 - test_loss: 0.1772800743
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2026253304 - test_loss: 0.1761983676
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2014500696 - test_loss: 0.1747144165
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2000603945 - test_loss: 0.1724327599
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.1982304401 - test_loss: 0.1693467857
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.1960539305 - test_loss: 0.1669616543
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.1939926772 - test_loss: 0.1653763306
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.1921881543 - test_loss: 0.1610304336
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.1904959509 - test_loss: 0.1590634507
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.1889284146 - test_loss: 0.1573287225
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.1876405709 - test_loss: 0.1560572396
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.1865075132 - test_loss: 0.1541838164
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.1853967854 - test_loss: 0.1530886943
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.1844740553 - test_loss: 0.1523503974
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.1835705764 - test_loss: 0.1503950007
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.1828073031 - test_loss: 0.1497264234
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.1821790333 - test_loss: 0.1486628379
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.1815018997 - test_loss: 0.1477813475
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.1809156994 - test_loss: 0.1475268465
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.1804103396 - test_loss: 0.1467908716
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.1798288201 - test_loss: 0.1456394345
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.1793117852 - test_loss: 0.1459473497
Early Stop Left: 4
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.1788329090 - test_loss: 0.1440275620
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.1783887658 - test_loss: 0.1441583869
Early Stop Left: 4
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.1780309021 - test_loss: 0.1430993031
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.1775895046 - test_loss: 0.1435229615
Early Stop Left: 4
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.1771113997 - test_loss: 0.1416093125
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.1767621506 - test_loss: 0.1413456976
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.1762740298 - test_loss: 0.1410551777
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.1759170985 - test_loss: 0.1403427115
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.1754900183 - test_loss: 0.1393319460
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.1750946985 - test_loss: 0.1391404627
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.1746244243 - test_loss: 0.1385567713
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.1741234572 - test_loss: 0.1374775935
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.1735843495 - test_loss: 0.1368191340
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.1731558812 - test_loss: 0.1350993123
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.1726205803 - test_loss: 0.1360982241
Early Stop Left: 4
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.1721352860 - test_loss: 0.1345015995
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.1717300552 - test_loss: 0.1352670618
Early Stop Left: 4
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.1712820944 - test_loss: 0.1341248532
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.1707942353 - test_loss: 0.1331784519
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.1704012679 - test_loss: 0.1334093717
Early Stop Left: 4
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.1700244433 - test_loss: 0.1321263671
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.1695881177 - test_loss: 0.1315137249
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.1691816388 - test_loss: 0.1315184944
Early Stop Left: 4
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.1688982720 - test_loss: 0.1309559274
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.1684724259 - test_loss: 0.1290555143
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.1681464266 - test_loss: 0.1296685767
Early Stop Left: 4
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.1678485882 - test_loss: 0.1298758882
Early Stop Left: 3
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.1676423864 - test_loss: 0.1278567870
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.1673047932 - test_loss: 0.1278714793
Early Stop Left: 4
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.1671020800 - test_loss: 0.1286270471
Early Stop Left: 3
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.1668124465 - test_loss: 0.1272362449
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.1664169682 - test_loss: 0.1263043406
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.1663290708 - test_loss: 0.1271905773
Early Stop Left: 4
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.1660701244 - test_loss: 0.1266654077
Early Stop Left: 3
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.1658168617 - test_loss: 0.1271146339
Early Stop Left: 2
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.1656158692 - test_loss: 0.1254728371
-------- Save Best Model! --------
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.1653339694 - test_loss: 0.1255183908
Early Stop Left: 4
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.1651386679 - test_loss: 0.1255758971
Early Stop Left: 3
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.1649717239 - test_loss: 0.1252435223
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.1647197105 - test_loss: 0.1250163155
-------- Save Best Model! --------
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.1645790358 - test_loss: 0.1250322136
Early Stop Left: 4
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.1643759917 - test_loss: 0.1247033812
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.1641605990 - test_loss: 0.1248582912
Early Stop Left: 4
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.1640112705 - test_loss: 0.1240837140
-------- Save Best Model! --------
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.1638571366 - test_loss: 0.1229376338
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.1637431866 - test_loss: 0.1242708028
Early Stop Left: 4
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.1635064976 - test_loss: 0.1230140448
Early Stop Left: 3
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.1633479631 - test_loss: 0.1232932943
Early Stop Left: 2
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.1631476288 - test_loss: 0.1239412268
Early Stop Left: 1
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.1629654480 - test_loss: 0.1223191147
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.1628003099 - test_loss: 0.1217285085
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.1625607011 - test_loss: 0.1221394020
Early Stop Left: 4
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.1625294095 - test_loss: 0.1230110315
Early Stop Left: 3
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.1623071703 - test_loss: 0.1226987626
Early Stop Left: 2
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.1621386323 - test_loss: 0.1209993154
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.1619906952 - test_loss: 0.1217885624
Early Stop Left: 4
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.1618187387 - test_loss: 0.1212615240
Early Stop Left: 3
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.1617128176 - test_loss: 0.1218256940
Early Stop Left: 2
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.1616048314 - test_loss: 0.1204447390
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.1614280449 - test_loss: 0.1199717770
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.1613105478 - test_loss: 0.1196756883
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.1611646652 - test_loss: 0.1202596622
Early Stop Left: 4
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.1609717557 - test_loss: 0.1204452425
Early Stop Left: 3
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.1608916547 - test_loss: 0.1201404754
Early Stop Left: 2
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.1606791626 - test_loss: 0.1188036046
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.1606500195 - test_loss: 0.1202946398
Early Stop Left: 4
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.1604037411 - test_loss: 0.1196203350
Early Stop Left: 3
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.1602431265 - test_loss: 0.1198192324
Early Stop Left: 2
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.1601399784 - test_loss: 0.1187656099
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.1599770040 - test_loss: 0.1184748275
-------- Save Best Model! --------
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.1598154947 - test_loss: 0.1183010434
-------- Save Best Model! --------
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.1597121816 - test_loss: 0.1178886044
-------- Save Best Model! --------
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.1596113323 - test_loss: 0.1184746650
Early Stop Left: 4
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.1594270504 - test_loss: 0.1172901405
-------- Save Best Model! --------
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.1592765071 - test_loss: 0.1180548086
Early Stop Left: 4
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.1591881496 - test_loss: 0.1170880192
-------- Save Best Model! --------
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.1590618816 - test_loss: 0.1189021072
Early Stop Left: 4
------- START EPOCH 152 -------
Epoch: 152 - loss: 0.1588871533 - test_loss: 0.1179769251
Early Stop Left: 3
------- START EPOCH 153 -------
Epoch: 153 - loss: 0.1587739760 - test_loss: 0.1170212751
-------- Save Best Model! --------
------- START EPOCH 154 -------
Epoch: 154 - loss: 0.1586435922 - test_loss: 0.1174243764
Early Stop Left: 4
------- START EPOCH 155 -------
Epoch: 155 - loss: 0.1585351261 - test_loss: 0.1180776939
Early Stop Left: 3
------- START EPOCH 156 -------
Epoch: 156 - loss: 0.1583965556 - test_loss: 0.1161642704
-------- Save Best Model! --------
------- START EPOCH 157 -------
Epoch: 157 - loss: 0.1582307766 - test_loss: 0.1173903638
Early Stop Left: 4
------- START EPOCH 158 -------
Epoch: 158 - loss: 0.1581411871 - test_loss: 0.1169197651
Early Stop Left: 3
------- START EPOCH 159 -------
Epoch: 159 - loss: 0.1579868605 - test_loss: 0.1150935923
-------- Save Best Model! --------
------- START EPOCH 160 -------
Epoch: 160 - loss: 0.1578515104 - test_loss: 0.1153094935
Early Stop Left: 4
------- START EPOCH 161 -------
Epoch: 161 - loss: 0.1577396341 - test_loss: 0.1148575327
-------- Save Best Model! --------
------- START EPOCH 162 -------
Epoch: 162 - loss: 0.1575363875 - test_loss: 0.1157394828
Early Stop Left: 4
------- START EPOCH 163 -------
Epoch: 163 - loss: 0.1574701462 - test_loss: 0.1155534049
Early Stop Left: 3
------- START EPOCH 164 -------
Epoch: 164 - loss: 0.1573534802 - test_loss: 0.1149943816
Early Stop Left: 2
------- START EPOCH 165 -------
Epoch: 165 - loss: 0.1572228871 - test_loss: 0.1154084685
Early Stop Left: 1
------- START EPOCH 166 -------
Epoch: 166 - loss: 0.1570765787 - test_loss: 0.1136238095
-------- Save Best Model! --------
------- START EPOCH 167 -------
Epoch: 167 - loss: 0.1569702156 - test_loss: 0.1146873167
Early Stop Left: 4
------- START EPOCH 168 -------
Epoch: 168 - loss: 0.1567933283 - test_loss: 0.1138105532
Early Stop Left: 3
------- START EPOCH 169 -------
Epoch: 169 - loss: 0.1566646643 - test_loss: 0.1143592664
Early Stop Left: 2
------- START EPOCH 170 -------
Epoch: 170 - loss: 0.1566417679 - test_loss: 0.1135451373
-------- Save Best Model! --------
------- START EPOCH 171 -------
Epoch: 171 - loss: 0.1564645423 - test_loss: 0.1141337866
Early Stop Left: 4
------- START EPOCH 172 -------
Epoch: 172 - loss: 0.1563525609 - test_loss: 0.1146914724
Early Stop Left: 3
------- START EPOCH 173 -------
Epoch: 173 - loss: 0.1561517314 - test_loss: 0.1134554995
-------- Save Best Model! --------
------- START EPOCH 174 -------
Epoch: 174 - loss: 0.1561173340 - test_loss: 0.1137343644
Early Stop Left: 4
------- START EPOCH 175 -------
Epoch: 175 - loss: 0.1559498833 - test_loss: 0.1139133520
Early Stop Left: 3
------- START EPOCH 176 -------
Epoch: 176 - loss: 0.1558591720 - test_loss: 0.1136379555
Early Stop Left: 2
------- START EPOCH 177 -------
Epoch: 177 - loss: 0.1556949165 - test_loss: 0.1120314412
-------- Save Best Model! --------
------- START EPOCH 178 -------
Epoch: 178 - loss: 0.1556058564 - test_loss: 0.1131015927
Early Stop Left: 4
------- START EPOCH 179 -------
Epoch: 179 - loss: 0.1554990539 - test_loss: 0.1127543492
Early Stop Left: 3
------- START EPOCH 180 -------
Epoch: 180 - loss: 0.1553866858 - test_loss: 0.1115741372
-------- Save Best Model! --------
------- START EPOCH 181 -------
Epoch: 181 - loss: 0.1552945125 - test_loss: 0.1125356337
Early Stop Left: 4
------- START EPOCH 182 -------
Epoch: 182 - loss: 0.1551109281 - test_loss: 0.1113082246
-------- Save Best Model! --------
------- START EPOCH 183 -------
Epoch: 183 - loss: 0.1551056997 - test_loss: 0.1114846437
Early Stop Left: 4
------- START EPOCH 184 -------
Epoch: 184 - loss: 0.1549830229 - test_loss: 0.1121896024
Early Stop Left: 3
------- START EPOCH 185 -------
Epoch: 185 - loss: 0.1548158017 - test_loss: 0.1115036127
Early Stop Left: 2
------- START EPOCH 186 -------
Epoch: 186 - loss: 0.1547833078 - test_loss: 0.1111331300
-------- Save Best Model! --------
------- START EPOCH 187 -------
Epoch: 187 - loss: 0.1546033083 - test_loss: 0.1113988526
Early Stop Left: 4
------- START EPOCH 188 -------
Epoch: 188 - loss: 0.1545180695 - test_loss: 0.1118137322
Early Stop Left: 3
------- START EPOCH 189 -------
Epoch: 189 - loss: 0.1544682939 - test_loss: 0.1108093374
-------- Save Best Model! --------
------- START EPOCH 190 -------
Epoch: 190 - loss: 0.1543890961 - test_loss: 0.1113573831
Early Stop Left: 4
------- START EPOCH 191 -------
Epoch: 191 - loss: 0.1542173131 - test_loss: 0.1103948361
-------- Save Best Model! --------
------- START EPOCH 192 -------
Epoch: 192 - loss: 0.1541049183 - test_loss: 0.1097425933
-------- Save Best Model! --------
------- START EPOCH 193 -------
Epoch: 193 - loss: 0.1540733874 - test_loss: 0.1099738447
Early Stop Left: 4
------- START EPOCH 194 -------
Epoch: 194 - loss: 0.1539494415 - test_loss: 0.1110296333
Early Stop Left: 3
------- START EPOCH 195 -------
Epoch: 195 - loss: 0.1538514579 - test_loss: 0.1104278281
Early Stop Left: 2
------- START EPOCH 196 -------
Epoch: 196 - loss: 0.1537992499 - test_loss: 0.1110494888
Early Stop Left: 1
------- START EPOCH 197 -------
Epoch: 197 - loss: 0.1536296083 - test_loss: 0.1108099018
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 13%|█▎        | 16/121 [00:00<00:00, 155.81it/s] 26%|██▋       | 32/121 [00:00<00:00, 146.81it/s] 39%|███▉      | 47/121 [00:00<00:00, 140.97it/s] 52%|█████▏    | 63/121 [00:00<00:00, 146.12it/s] 65%|██████▌   | 79/121 [00:00<00:00, 149.66it/s] 79%|███████▊  | 95/121 [00:00<00:00, 151.52it/s] 92%|█████████▏| 111/121 [00:00<00:00, 149.31it/s]100%|██████████| 121/121 [00:00<00:00, 148.27it/s]
Best micro threshold=0.334549, fscore=0.771
p,r,f1: 0.7203391771116066 0.828810105002131 0.7707770915458956
throttleing by fixed threshold: 0.5
p,r,f1: 0.824837214206897 0.6788996086636445 0.7447867917037536
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.33454906940460205,
                 'p': 0.7203391771116066,
                 'r': 0.828810105002131,
                 'f1': 0.7707770915458956},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.824837214206897,
                 'r': 0.6788996086636445,
                 'f1': 0.7447867917037536}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6735729092 - test_loss: 0.7132715282
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.6187177014 - test_loss: 0.6509739870
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.5606570100 - test_loss: 0.5845011719
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.5036356670 - test_loss: 0.5254729671
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.4579686199 - test_loss: 0.4811972981
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.4238100916 - test_loss: 0.4467580796
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3964235280 - test_loss: 0.4177736271
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3730196387 - test_loss: 0.3923739232
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3525523939 - test_loss: 0.3697303228
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.3343924152 - test_loss: 0.3494184576
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.3182698789 - test_loss: 0.3311684284
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.3039557240 - test_loss: 0.3147918028
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2913876580 - test_loss: 0.3001197798
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2803198496 - test_loss: 0.2869967057
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2705307504 - test_loss: 0.2752862124
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2621182745 - test_loss: 0.2648526112
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2546922882 - test_loss: 0.2555927239
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2483147740 - test_loss: 0.2474098480
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2429002646 - test_loss: 0.2401937985
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2383029903 - test_loss: 0.2338531201
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2343548018 - test_loss: 0.2282973690
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2311409787 - test_loss: 0.2234686233
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2284729428 - test_loss: 0.2192527917
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2262805221 - test_loss: 0.2156402817
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2245278780 - test_loss: 0.2125554364
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2230421404 - test_loss: 0.2098990150
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2219807209 - test_loss: 0.2076786952
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2210894112 - test_loss: 0.2058498431
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2204606788 - test_loss: 0.2042803892
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2199283334 - test_loss: 0.2030289357
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2195293318 - test_loss: 0.2020249109
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2192702305 - test_loss: 0.2011909054
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2190246763 - test_loss: 0.2005282216
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2187376919 - test_loss: 0.2000126416
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2185371724 - test_loss: 0.1994618117
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2184678461 - test_loss: 0.1990590656
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2181412515 - test_loss: 0.1986820268
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2178913778 - test_loss: 0.1981879922
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2175796469 - test_loss: 0.1977620221
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2171821573 - test_loss: 0.1971910118
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2167620671 - test_loss: 0.1965951376
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2161486938 - test_loss: 0.1957520484
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2153642793 - test_loss: 0.1948245687
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2146818152 - test_loss: 0.1941190857
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2142292373 - test_loss: 0.1932342761
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2138430475 - test_loss: 0.1927803757
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2134772493 - test_loss: 0.1923515177
-------- Save Best Model! --------
------- START EPOCH 48 -------
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6553038920 - test_loss: 0.7132935588
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.6020534226 - test_loss: 0.6510561557
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.5457979482 - test_loss: 0.5846654714
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4906120326 - test_loss: 0.5256478185
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.4465191953 - test_loss: 0.4814289929
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.4136580096 - test_loss: 0.4470787671
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3874115451 - test_loss: 0.4181945203
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3650627010 - test_loss: 0.3929101332
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3455883559 - test_loss: 0.3703795519
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.3283725110 - test_loss: 0.3501834709
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.3131455081 - test_loss: 0.3320407390
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2996792562 - test_loss: 0.3157585512
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2879075939 - test_loss: 0.3011741931
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2775917646 - test_loss: 0.2881362266
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2685180494 - test_loss: 0.2765138093
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2607796439 - test_loss: 0.2661714390
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2539999918 - test_loss: 0.2570051636
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2482344993 - test_loss: 0.2489283175
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2433951818 - test_loss: 0.2418274650
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2393390517 - test_loss: 0.2356120780
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2359005807 - test_loss: 0.2301911835
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2331552019 - test_loss: 0.2255169552
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2309163893 - test_loss: 0.2214605163
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2291126928 - test_loss: 0.2180245115
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2277052069 - test_loss: 0.2151258545
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2265259592 - test_loss: 0.2126508250
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2257230530 - test_loss: 0.2106232419
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2250528042 - test_loss: 0.2090013062
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2246031982 - test_loss: 0.2076125385
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2242181733 - test_loss: 0.2065482215
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2239336873 - test_loss: 0.2057287939
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2237602530 - test_loss: 0.2050527715
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2235795633 - test_loss: 0.2045322839
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2233449989 - test_loss: 0.2041397233
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2231806191 - test_loss: 0.2036592617
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2231316996 - test_loss: 0.2033381313
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2228370428 - test_loss: 0.2030175184
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2226133095 - test_loss: 0.2025624358
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2223340614 - test_loss: 0.2021726745
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2219662300 - test_loss: 0.2016113232
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2215535009 - test_loss: 0.2010240811
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2209510138 - test_loss: 0.2002353384
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2202360375 - test_loss: 0.1993273208
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2196324208 - test_loss: 0.1986672643
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2192167690 - test_loss: 0.1977725027
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2188519399 - test_loss: 0.1973441441
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2185033307 - test_loss: 0.1969345035
-------- Save Best Model! --------
------- START EPOCH 48 -------
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6030325649 - test_loss: 0.7133688538
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5543609194 - test_loss: 0.6513307804
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.5032417041 - test_loss: 0.5851900238
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4533357215 - test_loss: 0.5263267219
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.4137678957 - test_loss: 0.4823144960
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3845866266 - test_loss: 0.4482453763
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3615578136 - test_loss: 0.4197043038
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3421757638 - test_loss: 0.3947991569
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3254822323 - test_loss: 0.3726600212
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.3108997968 - test_loss: 0.3528628081
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2981589532 - test_loss: 0.3351139113
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2870334996 - test_loss: 0.3192122320
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2774449353 - test_loss: 0.3049965205
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2691739426 - test_loss: 0.2923177907
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2620287818 - test_loss: 0.2810741308
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2560843897 - test_loss: 0.2711358129
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2509993592 - test_loss: 0.2623987636
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2468087862 - test_loss: 0.2548071432
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2434184038 - test_loss: 0.2482249390
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2406890282 - test_loss: 0.2425711481
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2384622920 - test_loss: 0.2377384073
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2367877875 - test_loss: 0.2337096735
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2354907372 - test_loss: 0.2302767182
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2345026057 - test_loss: 0.2275027564
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2337839847 - test_loss: 0.2252773393
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2331861189 - test_loss: 0.2234258891
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2328394988 - test_loss: 0.2220201794
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2325370889 - test_loss: 0.2210204011
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2323624958 - test_loss: 0.2201229198
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2321921412 - test_loss: 0.2195250441
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2320629610 - test_loss: 0.2191278444
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2319956152 - test_loss: 0.2187668613
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2318938605 - test_loss: 0.2185056620
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2317291275 - test_loss: 0.2183045800
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2316112361 - test_loss: 0.2179070970
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2315824707 - test_loss: 0.2177042523
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2313451642 - test_loss: 0.2174359029
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2311648017 - test_loss: 0.2170507620
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2309314518 - test_loss: 0.2167063403
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2305861391 - test_loss: 0.2161091129
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2301539445 - test_loss: 0.2154806573
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2295957458 - test_loss: 0.2147885192
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2290455222 - test_loss: 0.2138267691
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2285888215 - test_loss: 0.2132787813
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2282535502 - test_loss: 0.2123503025
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2279539920 - test_loss: 0.2120048889
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2276683400 - test_loss: 0.2116898046
-------- Save Best Model! --------
------- START EPOCH 48 -------
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5677566247 - test_loss: 0.7134428640
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5221622734 - test_loss: 0.6515675079
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4744817516 - test_loss: 0.5855979279
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4281395088 - test_loss: 0.5269387975
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3916250784 - test_loss: 0.4831065058
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3649038157 - test_loss: 0.4492813639
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3440101287 - test_loss: 0.4210292832
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3265819264 - test_loss: 0.3964510065
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3117052696 - test_loss: 0.3746557041
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2988305009 - test_loss: 0.3552279620
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2876908818 - test_loss: 0.3378523603
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2780620823 - test_loss: 0.3223212191
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2698558332 - test_loss: 0.3084831233
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2628629699 - test_loss: 0.2961849430
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2569025720 - test_loss: 0.2853417532
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2520348057 - test_loss: 0.2758227134
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2479413459 - test_loss: 0.2675343966
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2446451660 - test_loss: 0.2604240238
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2420498540 - test_loss: 0.2543396951
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2400206406 - test_loss: 0.2492083124
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2384073305 - test_loss: 0.2448909184
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2372470011 - test_loss: 0.2413983578
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2363773394 - test_loss: 0.2384679239
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2357368627 - test_loss: 0.2362037065
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2352920118 - test_loss: 0.2344687043
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2349116624 - test_loss: 0.2330362541
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2347198275 - test_loss: 0.2320167681
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2345360107 - test_loss: 0.2313701895
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2344420093 - test_loss: 0.2307153694
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2343336873 - test_loss: 0.2303206247
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2342478555 - test_loss: 0.2300986373
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2342083688 - test_loss: 0.2298548983
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2341306101 - test_loss: 0.2296794606
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2339933727 - test_loss: 0.2295323146
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2338947309 - test_loss: 0.2291566284
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2338734564 - test_loss: 0.2290031116
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2336665779 - test_loss: 0.2287494703
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2335059296 - test_loss: 0.2284064957
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2332889734 - test_loss: 0.2280735961
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2329557129 - test_loss: 0.2274563639
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2325388361 - test_loss: 0.2267860934
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2320297861 - test_loss: 0.2261184006
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2315499611 - test_loss: 0.2251431958
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2311564332 - test_loss: 0.2246528572
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2308715076 - test_loss: 0.2237214574
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2306227517 - test_loss: 0.2234119284
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2303890393 - test_loss: 0.2231794576
-------- Save Best Model! --------
------- START EPOCH 48 -------
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5317698579 - test_loss: 0.7135314848
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4893081302 - test_loss: 0.6518567372
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4451183787 - test_loss: 0.5861272438
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4024008936 - test_loss: 0.5277385446
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3689875675 - test_loss: 0.4841545655
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3447503323 - test_loss: 0.4506444197
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3259955893 - test_loss: 0.4227598076
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3105077188 - test_loss: 0.3985916844
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2974188755 - test_loss: 0.3772279731
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2862089712 - test_loss: 0.3582622951
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2766178820 - test_loss: 0.3413793179
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2684283204 - test_loss: 0.3263674046
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2615445017 - test_loss: 0.3130919189
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2557636536 - test_loss: 0.3013686651
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2509095453 - test_loss: 0.2911165805
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2470225347 - test_loss: 0.2821905320
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2438078246 - test_loss: 0.2744832366
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2412788614 - test_loss: 0.2679631289
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2393424946 - test_loss: 0.2624669218
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2378723359 - test_loss: 0.2579173377
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2367307419 - test_loss: 0.2541573772
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2359469531 - test_loss: 0.2512379496
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2353762935 - test_loss: 0.2488199783
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2349684674 - test_loss: 0.2470427373
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2346989638 - test_loss: 0.2457569158
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2344548233 - test_loss: 0.2446871124
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2343534977 - test_loss: 0.2439747817
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2342398709 - test_loss: 0.2435993631
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2341922497 - test_loss: 0.2431148436
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2341227977 - test_loss: 0.2428534601
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2340668918 - test_loss: 0.2427579456
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2340492501 - test_loss: 0.2426032157
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2339956662 - test_loss: 0.2424934679
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2338921805 - test_loss: 0.2424072200
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2338244488 - test_loss: 0.2420844803
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2338297838 - test_loss: 0.2420077778
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2336781090 - test_loss: 0.2418236148
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2335747028 - test_loss: 0.2415663253
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2334344643 - test_loss: 0.2413491960
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2332187338 - test_loss: 0.2409255022
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2329600617 - test_loss: 0.2405018180
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2326024962 - test_loss: 0.2399312877
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2321659694 - test_loss: 0.2390316783
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2317459419 - test_loss: 0.2384298619
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2314377460 - test_loss: 0.2373114717
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2311727844 - test_loss: 0.2369493762
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2309321916 - test_loss: 0.2366535442
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2131769537 - test_loss: 0.1919478017
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2128288631 - test_loss: 0.1916943740
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2125374208 - test_loss: 0.1914099534
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2122445906 - test_loss: 0.1909527435
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2120196816 - test_loss: 0.1907385393
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2117942008 - test_loss: 0.1904416167
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2115303611 - test_loss: 0.1900274292
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2113122127 - test_loss: 0.1899540461
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2110797478 - test_loss: 0.1897363032
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2107749149 - test_loss: 0.1892871162
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2102939173 - test_loss: 0.1886336260
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2097498395 - test_loss: 0.1881676409
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2090360678 - test_loss: 0.1871856185
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2083481357 - test_loss: 0.1865298210
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2077713680 - test_loss: 0.1858506546
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2072345309 - test_loss: 0.1850920029
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2068087830 - test_loss: 0.1846224184
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2062292360 - test_loss: 0.1842435888
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2057649592 - test_loss: 0.1833933780
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2052496978 - test_loss: 0.1826832005
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2048455618 - test_loss: 0.1821314992
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2044401201 - test_loss: 0.1816131277
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2039584447 - test_loss: 0.1811646769
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2035252745 - test_loss: 0.1804299697
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2030929178 - test_loss: 0.1801387038
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2025352948 - test_loss: 0.1790668059
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2019378437 - test_loss: 0.1780594729
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2012640916 - test_loss: 0.1773779194
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2004722579 - test_loss: 0.1762009853
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.1996483371 - test_loss: 0.1752121181
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.1985719275 - test_loss: 0.1738367291
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.1974512255 - test_loss: 0.1724681085
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.1963913845 - test_loss: 0.1710171624
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.1951398848 - test_loss: 0.1695982261
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.1939733929 - test_loss: 0.1679639992
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.1927996183 - test_loss: 0.1663077312
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.1916192325 - test_loss: 0.1648633938
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.1904678095 - test_loss: 0.1634003356
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.1893159236 - test_loss: 0.1622687151
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.1881256619 - test_loss: 0.1606383617
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.1870059564 - test_loss: 0.1592807176
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.1858188608 - test_loss: 0.1577645369
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.1845989172 - test_loss: 0.1562383044
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.1834374224 - test_loss: 0.1548804955
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.1822173440 - test_loss: 0.1531686787
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.1810545922 - test_loss: 0.1517227766
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.1800760840 - test_loss: 0.1505480954
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.1791998778 - test_loss: 0.1493906238
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.1782291186 - test_loss: 0.1483037449
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.1774425044 - test_loss: 0.1472387216
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.1766854717 - test_loss: 0.1463490750
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.1759102291 - test_loss: 0.1452607055
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.1752970223 - test_loss: 0.1442017818
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.1746282915 - test_loss: 0.1437374286
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.1741303378 - test_loss: 0.1431590803
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.1734666893 - test_loss: 0.1420585535
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.1728971707 - test_loss: 0.1415901866
-------- Save Best Model! --------
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.1723810924 - test_loss: 0.1407342483
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.1717347150 - test_loss: 0.1397274923
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.1713304856 - test_loss: 0.1394011948
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.1708431469 - test_loss: 0.1388078626
-------- Save Best Model! --------
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.1702623272 - test_loss: 0.1380000235
-------- Save Best Model! --------
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.1698183903 - test_loss: 0.1373841618
-------- Save Best Model! --------
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.1693598562 - test_loss: 0.1367638389
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.1689204173 - test_loss: 0.1362700932
-------- Save Best Model! --------
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.1685563134 - test_loss: 0.1356960863
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 48 - loss: 0.2274359948 - test_loss: 0.2112372764
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2271681831 - test_loss: 0.2110847992
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2269529929 - test_loss: 0.2108151657
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2267444151 - test_loss: 0.2103586342
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2265953375 - test_loss: 0.2101838277
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2264504649 - test_loss: 0.2099429671
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2262845386 - test_loss: 0.2095942111
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2261659601 - test_loss: 0.2097242685
Early Stop Left: 4
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2260522898 - test_loss: 0.2096097939
Early Stop Left: 3
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2259193537 - test_loss: 0.2093594849
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2257314777 - test_loss: 0.2089528854
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2256098884 - test_loss: 0.2090008714
Early Stop Left: 4
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2254408407 - test_loss: 0.2085081488
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2252448467 - test_loss: 0.2084124662
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2250378734 - test_loss: 0.2080604190
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2248229343 - test_loss: 0.2077409912
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2246461400 - test_loss: 0.2074833727
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2243221777 - test_loss: 0.2072304650
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2240457987 - test_loss: 0.2067801472
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2236814009 - test_loss: 0.2060965277
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2233796849 - test_loss: 0.2058882239
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2229888816 - test_loss: 0.2050517390
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2225590509 - test_loss: 0.2045039437
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2221408490 - test_loss: 0.2036489827
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2217105388 - test_loss: 0.2030546936
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2212028658 - test_loss: 0.2020426229
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2206883705 - test_loss: 0.2009117108
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2201679836 - test_loss: 0.2001215642
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2196128026 - test_loss: 0.1994549370
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2191060126 - test_loss: 0.1987600020
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2184699127 - test_loss: 0.1977044245
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2178553153 - test_loss: 0.1969548535
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2173288082 - test_loss: 0.1958802414
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2166869346 - test_loss: 0.1953924598
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2161471032 - test_loss: 0.1943629797
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.2156213731 - test_loss: 0.1930981775
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.2151156661 - test_loss: 0.1927015406
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.2146217107 - test_loss: 0.1922477793
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.2141041951 - test_loss: 0.1916314168
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.2135357072 - test_loss: 0.1906566440
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.2129716180 - test_loss: 0.1900118586
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.2124071142 - test_loss: 0.1889043780
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.2117181505 - test_loss: 0.1882524971
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.2110647388 - test_loss: 0.1873048867
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.2102880404 - test_loss: 0.1860148055
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.2094593321 - test_loss: 0.1845526110
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.2087038705 - test_loss: 0.1834915221
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.2079252868 - test_loss: 0.1824810877
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.2070602634 - test_loss: 0.1814838161
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.2062979210 - test_loss: 0.1802004469
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.2056046051 - test_loss: 0.1790930621
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.2048771369 - test_loss: 0.1781439982
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.2043082248 - test_loss: 0.1772654032
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.2037435444 - test_loss: 0.1764407931
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.2032840849 - test_loss: 0.1755845500
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.2027738655 - test_loss: 0.1746628617
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.2023447976 - test_loss: 0.1739964970
-------- Save Best Model! --------
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.2019551503 - test_loss: 0.1736637322
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.2014797526 - test_loss: 0.1728495038
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.2012138993 - test_loss: 0.1724565427
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.2009255027 - test_loss: 0.1716469862
-------- Save Best Model! --------
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.2004703745 - test_loss: 0.1710493267
-------- Save Best Model! --------
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.2002180878 - test_loss: 0.1707437026
-------- Save Best Model! --------
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.1998661725 - test_loss: 0.1704540888
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.1995461849 - test_loss: 0.1698419771
-------- Save Best Model! --------
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.1993041390 - test_loss: 0.1694822394
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.1990093556 - test_loss: 0.1687288199
-------- Save Best Model! --------
Epoch: 48 - loss: 0.2307452617 - test_loss: 0.2361704141
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2305333339 - test_loss: 0.2360729496
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2303684402 - test_loss: 0.2358053352
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2302090535 - test_loss: 0.2353446422
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2300968577 - test_loss: 0.2351503513
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2299867188 - test_loss: 0.2349308313
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2298651720 - test_loss: 0.2346529165
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2297831884 - test_loss: 0.2348371526
Early Stop Left: 4
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2297062791 - test_loss: 0.2347396814
Early Stop Left: 3
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2296205082 - test_loss: 0.2345450796
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2295015972 - test_loss: 0.2341457440
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2294320632 - test_loss: 0.2342637106
Early Stop Left: 4
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2293347005 - test_loss: 0.2338280672
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2292288979 - test_loss: 0.2337794803
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2291189032 - test_loss: 0.2335812943
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2290142800 - test_loss: 0.2333420737
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2289460233 - test_loss: 0.2332382737
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2287799717 - test_loss: 0.2331418114
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2286576769 - test_loss: 0.2328804609
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2284706119 - test_loss: 0.2324375243
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2283459038 - test_loss: 0.2324124395
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2281448079 - test_loss: 0.2319360843
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2279190625 - test_loss: 0.2316702846
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2276987630 - test_loss: 0.2311680505
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2274584664 - test_loss: 0.2309376840
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2271444956 - test_loss: 0.2300241106
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2268203020 - test_loss: 0.2290434900
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2264704564 - test_loss: 0.2283376603
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2260939322 - test_loss: 0.2277413549
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2257333990 - test_loss: 0.2268676478
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2252620080 - test_loss: 0.2259813206
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2247843177 - test_loss: 0.2253068163
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2243805542 - test_loss: 0.2242140132
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2238759107 - test_loss: 0.2237118100
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2234425602 - test_loss: 0.2227563756
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.2230092616 - test_loss: 0.2215774185
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.2225770125 - test_loss: 0.2211657878
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.2221638004 - test_loss: 0.2204887166
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.2217555823 - test_loss: 0.2198149972
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.2213306942 - test_loss: 0.2190669647
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.2209491403 - test_loss: 0.2187895504
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.2206064972 - test_loss: 0.2175581264
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.2202138318 - test_loss: 0.2173806157
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.2198744595 - test_loss: 0.2169513588
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.2194682884 - test_loss: 0.2160963878
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.2190295835 - test_loss: 0.2151032860
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.2186544657 - test_loss: 0.2144338012
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.2182228382 - test_loss: 0.2141653850
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.2176926609 - test_loss: 0.2134303165
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.2171790286 - test_loss: 0.2122339249
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.2166538837 - test_loss: 0.2111137121
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.2160438553 - test_loss: 0.2102198426
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.2155076422 - test_loss: 0.2094479150
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.2149483424 - test_loss: 0.2082649170
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.2144462934 - test_loss: 0.2071731949
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.2138937792 - test_loss: 0.2059965466
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.2134114378 - test_loss: 0.2051935427
-------- Save Best Model! --------
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.2129814497 - test_loss: 0.2044218281
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.2124841749 - test_loss: 0.2038269375
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.2121544061 - test_loss: 0.2032123584
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.2118302396 - test_loss: 0.2020739736
-------- Save Best Model! --------
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.2113833258 - test_loss: 0.2014947547
-------- Save Best Model! --------
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.2110993623 - test_loss: 0.2010595641
-------- Save Best Model! --------
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.2107460271 - test_loss: 0.2004764229
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.2104383007 - test_loss: 0.2000401899
-------- Save Best Model! --------
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.2101924734 - test_loss: 0.1994106990
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.2099240076 - test_loss: 0.1986969701
-------- Save Best Model! --------
Epoch: 48 - loss: 0.2182153375 - test_loss: 0.1965235179
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2178807328 - test_loss: 0.1962820461
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2176010263 - test_loss: 0.1959927693
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2173230458 - test_loss: 0.1955293168
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2171113181 - test_loss: 0.1953244730
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2169015454 - test_loss: 0.1950287559
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2166597415 - test_loss: 0.1946220690
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2164667537 - test_loss: 0.1945930223
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2162696194 - test_loss: 0.1944172875
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2160281080 - test_loss: 0.1940389038
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2156733370 - test_loss: 0.1935141834
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2153202277 - test_loss: 0.1932332325
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2147597838 - test_loss: 0.1923286794
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2140546738 - test_loss: 0.1916355882
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2134078433 - test_loss: 0.1908657141
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2128503725 - test_loss: 0.1900591791
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2124259430 - test_loss: 0.1895699516
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2118646682 - test_loss: 0.1892162962
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2114116452 - test_loss: 0.1883247678
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2109094742 - test_loss: 0.1875704752
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2105152357 - test_loss: 0.1870028074
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2101208626 - test_loss: 0.1864957839
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2096615782 - test_loss: 0.1860657259
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2092493114 - test_loss: 0.1853111905
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2088442124 - test_loss: 0.1850607596
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2083252654 - test_loss: 0.1839833241
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2077773008 - test_loss: 0.1829768203
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2071656018 - test_loss: 0.1823592119
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2064560948 - test_loss: 0.1812181488
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2057288331 - test_loss: 0.1803395108
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2047744476 - test_loss: 0.1790560957
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2037726840 - test_loss: 0.1777842962
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2028106741 - test_loss: 0.1764418728
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2016559949 - test_loss: 0.1750781711
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2005804332 - test_loss: 0.1735225480
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.1995021415 - test_loss: 0.1718850258
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.1984199736 - test_loss: 0.1705859560
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.1973515773 - test_loss: 0.1691274491
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.1962709852 - test_loss: 0.1679890391
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.1951602825 - test_loss: 0.1663959778
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.1941263008 - test_loss: 0.1651209317
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.1930400079 - test_loss: 0.1636430471
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.1919303773 - test_loss: 0.1621795797
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.1908692231 - test_loss: 0.1609488888
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.1897456825 - test_loss: 0.1592923270
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.1886569184 - test_loss: 0.1577973719
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.1877168644 - test_loss: 0.1566003990
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.1868636841 - test_loss: 0.1554479423
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.1859225997 - test_loss: 0.1542536074
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.1851616103 - test_loss: 0.1531964095
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.1844409321 - test_loss: 0.1523651124
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.1837059147 - test_loss: 0.1512099555
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.1831331209 - test_loss: 0.1501703642
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.1825061953 - test_loss: 0.1497410209
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.1820430752 - test_loss: 0.1491517609
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.1814271890 - test_loss: 0.1481553944
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.1808979938 - test_loss: 0.1476109458
-------- Save Best Model! --------
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.1804189689 - test_loss: 0.1467660421
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.1798162303 - test_loss: 0.1458249066
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.1794345255 - test_loss: 0.1454762134
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.1789831372 - test_loss: 0.1449061165
-------- Save Best Model! --------
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.1784328439 - test_loss: 0.1441091526
-------- Save Best Model! --------
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.1780155002 - test_loss: 0.1435283150
-------- Save Best Model! --------
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.1775813747 - test_loss: 0.1429104500
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.1771611094 - test_loss: 0.1424057699
-------- Save Best Model! --------
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.1768158567 - test_loss: 0.1417947247
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 48 - loss: 0.2302041502 - test_loss: 0.2227404736
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2299875392 - test_loss: 0.2226583168
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2298175770 - test_loss: 0.2224113009
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2296496901 - test_loss: 0.2219826771
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2295329093 - test_loss: 0.2218213313
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2294182805 - test_loss: 0.2215949382
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2292856426 - test_loss: 0.2212898677
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2291957043 - test_loss: 0.2214823645
Early Stop Left: 4
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2291098185 - test_loss: 0.2213868974
Early Stop Left: 3
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2290092039 - test_loss: 0.2211872062
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2288639967 - test_loss: 0.2207841004
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2287751458 - test_loss: 0.2208678954
Early Stop Left: 4
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2286486252 - test_loss: 0.2204085292
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2285043208 - test_loss: 0.2203522236
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2283515493 - test_loss: 0.2200702612
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2281973962 - test_loss: 0.2198111000
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2280792732 - test_loss: 0.2196290628
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2278386920 - test_loss: 0.2194381646
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2276440352 - test_loss: 0.2191029995
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2273712546 - test_loss: 0.2185306926
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2271644790 - test_loss: 0.2184423107
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2268735332 - test_loss: 0.2177446728
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2265528891 - test_loss: 0.2173313496
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2262412305 - test_loss: 0.2166301763
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2259100020 - test_loss: 0.2161883798
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2255027908 - test_loss: 0.2152320001
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2250868172 - test_loss: 0.2141464918
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2246577667 - test_loss: 0.2133724760
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2242048811 - test_loss: 0.2128409567
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2237930308 - test_loss: 0.2121156788
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2232656513 - test_loss: 0.2111947220
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2227515025 - test_loss: 0.2105308306
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2223150472 - test_loss: 0.2094858038
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2217622361 - test_loss: 0.2089627406
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2212784183 - test_loss: 0.2080023024
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.2207912250 - test_loss: 0.2066724124
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.2203117475 - test_loss: 0.2062989802
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.2198557622 - test_loss: 0.2056962289
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.2194094918 - test_loss: 0.2051123341
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.2189484044 - test_loss: 0.2042614550
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.2185212898 - test_loss: 0.2038848219
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.2181238526 - test_loss: 0.2028405718
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.2176405010 - test_loss: 0.2025300145
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.2172089787 - test_loss: 0.2017904465
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.2166845158 - test_loss: 0.2008187454
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.2161021593 - test_loss: 0.1996781074
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.2155670399 - test_loss: 0.1989260461
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.2149584466 - test_loss: 0.1981956714
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.2142143406 - test_loss: 0.1972517714
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.2134877966 - test_loss: 0.1955950536
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.2127727452 - test_loss: 0.1943829555
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.2120115224 - test_loss: 0.1934746024
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.2113849780 - test_loss: 0.1922733896
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.2107708080 - test_loss: 0.1913631589
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.2102518930 - test_loss: 0.1903695761
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.2096981353 - test_loss: 0.1893410608
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.2092384491 - test_loss: 0.1885387670
-------- Save Best Model! --------
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.2088243133 - test_loss: 0.1882175792
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.2083436589 - test_loss: 0.1872835992
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.2080564267 - test_loss: 0.1868623820
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.2077600408 - test_loss: 0.1859519560
-------- Save Best Model! --------
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.2073290692 - test_loss: 0.1855042408
-------- Save Best Model! --------
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.2070804617 - test_loss: 0.1850710573
-------- Save Best Model! --------
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.2067518012 - test_loss: 0.1845954589
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.2064622516 - test_loss: 0.1839575691
-------- Save Best Model! --------
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.2062450281 - test_loss: 0.1836390306
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.2059868381 - test_loss: 0.1831549184
-------- Save Best Model! --------
Epoch: 114 - loss: 0.1681341424 - test_loss: 0.1353155303
-------- Save Best Model! --------
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.1677837446 - test_loss: 0.1345318249
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.1674484278 - test_loss: 0.1344338704
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.1670938559 - test_loss: 0.1335909470
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.1667872063 - test_loss: 0.1334630630
-------- Save Best Model! --------
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.1665067658 - test_loss: 0.1331435076
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.1662384571 - test_loss: 0.1329028924
-------- Save Best Model! --------
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.1658836584 - test_loss: 0.1322366588
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.1656072795 - test_loss: 0.1317464901
-------- Save Best Model! --------
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.1653298352 - test_loss: 0.1316331490
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.1650413973 - test_loss: 0.1314198239
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.1648428898 - test_loss: 0.1306107193
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.1645063515 - test_loss: 0.1302018517
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.1643785220 - test_loss: 0.1302365274
Early Stop Left: 4
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.1640984228 - test_loss: 0.1301293595
-------- Save Best Model! --------
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.1638757358 - test_loss: 0.1294859694
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.1637434643 - test_loss: 0.1300665932
Early Stop Left: 4
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.1634236355 - test_loss: 0.1289008438
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.1633343961 - test_loss: 0.1291668005
Early Stop Left: 4
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.1630995017 - test_loss: 0.1284981065
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.1629421984 - test_loss: 0.1280379862
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.1627684643 - test_loss: 0.1284845298
Early Stop Left: 4
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.1625794890 - test_loss: 0.1279920245
-------- Save Best Model! --------
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.1623994857 - test_loss: 0.1281252345
Early Stop Left: 4
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.1622973744 - test_loss: 0.1276153820
-------- Save Best Model! --------
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.1620189697 - test_loss: 0.1270902890
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.1619137218 - test_loss: 0.1271219040
Early Stop Left: 4
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.1617697678 - test_loss: 0.1267244747
-------- Save Best Model! --------
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.1615398945 - test_loss: 0.1273713058
Early Stop Left: 4
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.1614242670 - test_loss: 0.1263262330
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.1612187774 - test_loss: 0.1259098855
-------- Save Best Model! --------
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.1610787089 - test_loss: 0.1259411071
Early Stop Left: 4
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.1609107590 - test_loss: 0.1260647325
Early Stop Left: 3
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.1608228507 - test_loss: 0.1253525531
-------- Save Best Model! --------
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.1606648596 - test_loss: 0.1253069937
-------- Save Best Model! --------
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.1604941083 - test_loss: 0.1257361137
Early Stop Left: 4
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.1603383321 - test_loss: 0.1249772691
-------- Save Best Model! --------
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.1602406777 - test_loss: 0.1252584520
Early Stop Left: 4
------- START EPOCH 152 -------
Epoch: 152 - loss: 0.1600364794 - test_loss: 0.1242060228
-------- Save Best Model! --------
------- START EPOCH 153 -------
Epoch: 153 - loss: 0.1599091502 - test_loss: 0.1245522969
Early Stop Left: 4
------- START EPOCH 154 -------
Epoch: 154 - loss: 0.1597983543 - test_loss: 0.1246400885
Early Stop Left: 3
------- START EPOCH 155 -------
Epoch: 155 - loss: 0.1597153464 - test_loss: 0.1243240440
Early Stop Left: 2
------- START EPOCH 156 -------
Epoch: 156 - loss: 0.1595257459 - test_loss: 0.1241944398
-------- Save Best Model! --------
------- START EPOCH 157 -------
Epoch: 157 - loss: 0.1594317934 - test_loss: 0.1242254924
Early Stop Left: 4
------- START EPOCH 158 -------
Epoch: 158 - loss: 0.1592922347 - test_loss: 0.1237021803
-------- Save Best Model! --------
------- START EPOCH 159 -------
Epoch: 159 - loss: 0.1591682022 - test_loss: 0.1231708855
-------- Save Best Model! --------
------- START EPOCH 160 -------
Epoch: 160 - loss: 0.1590403518 - test_loss: 0.1233121751
Early Stop Left: 4
------- START EPOCH 161 -------
Epoch: 161 - loss: 0.1588592426 - test_loss: 0.1224337178
-------- Save Best Model! --------
------- START EPOCH 162 -------
Epoch: 162 - loss: 0.1587209309 - test_loss: 0.1227813028
Early Stop Left: 4
------- START EPOCH 163 -------
Epoch: 163 - loss: 0.1587280480 - test_loss: 0.1226573782
Early Stop Left: 3
------- START EPOCH 164 -------
Epoch: 164 - loss: 0.1585051406 - test_loss: 0.1230030747
Early Stop Left: 2
------- START EPOCH 165 -------
Epoch: 165 - loss: 0.1583822835 - test_loss: 0.1225273557
Early Stop Left: 1
------- START EPOCH 166 -------
Epoch: 166 - loss: 0.1582417583 - test_loss: 0.1222585827
-------- Save Best Model! --------
------- START EPOCH 167 -------
Epoch: 167 - loss: 0.1581017420 - test_loss: 0.1218151626
-------- Save Best Model! --------
------- START EPOCH 168 -------
Epoch: 168 - loss: 0.1579572228 - test_loss: 0.1221710810
Early Stop Left: 4
------- START EPOCH 169 -------
Epoch: 169 - loss: 0.1577727968 - test_loss: 0.1228843278
Early Stop Left: 3
------- START EPOCH 170 -------
Epoch: 170 - loss: 0.1576968428 - test_loss: 0.1215739973
-------- Save Best Model! --------
------- START EPOCH 171 -------
Epoch: 171 - loss: 0.1575405293 - test_loss: 0.1214638223
-------- Save Best Model! --------
------- START EPOCH 172 -------
Epoch: 172 - loss: 0.1573742719 - test_loss: 0.1213552029
-------- Save Best Model! --------
------- START EPOCH 173 -------
Epoch: 173 - loss: 0.1571733290 - test_loss: 0.1212762036
-------- Save Best Model! --------
------- START EPOCH 174 -------
Epoch: 174 - loss: 0.1571479763 - test_loss: 0.1214765272
Early Stop Left: 4
------- START EPOCH 175 -------
Epoch: 175 - loss: 0.1569295197 - test_loss: 0.1207428660
-------- Save Best Model! --------
------- START EPOCH 176 -------
Epoch: 176 - loss: 0.1567900822 - test_loss: 0.1202737153
-------- Save Best Model! --------
------- START EPOCH 177 -------
Epoch: 177 - loss: 0.1565958371 - test_loss: 0.1202598813
-------- Save Best Model! --------
------- START EPOCH 178 -------
Epoch: 178 - loss: 0.1564302307 - test_loss: 0.1199582794
-------- Save Best Model! --------
------- START EPOCH 179 -------
Epoch: 179 - loss: 0.1563016985 - test_loss: 0.1198660461
-------- Save Best Model! --------
------- START EPOCH 180 -------
Epoch: 180 - loss: 0.1561385848 - test_loss: 0.1196226042
-------- Save Best Model! --------
------- START EPOCH 181 -------
Epoch: 181 - loss: 0.1559373170 - test_loss: 0.1198312169
Early Stop Left: 4
------- START EPOCH 182 -------
Epoch: 182 - loss: 0.1557272049 - test_loss: 0.1186522978
-------- Save Best Model! --------
------- START EPOCH 183 -------
Epoch: 183 - loss: 0.1555928956 - test_loss: 0.1189514268
Early Stop Left: 4
------- START EPOCH 184 -------
Epoch: 184 - loss: 0.1553360056 - test_loss: 0.1184271479
-------- Save Best Model! --------
------- START EPOCH 185 -------
Epoch: 185 - loss: 0.1551790822 - test_loss: 0.1181815266
-------- Save Best Model! --------
------- START EPOCH 186 -------
Epoch: 186 - loss: 0.1550184943 - test_loss: 0.1178389472
-------- Save Best Model! --------
------- START EPOCH 187 -------
Epoch: 187 - loss: 0.1547597858 - test_loss: 0.1181082747
Early Stop Left: 4
------- START EPOCH 188 -------
Epoch: 188 - loss: 0.1545439277 - test_loss: 0.1171661790
-------- Save Best Model! --------
------- START EPOCH 189 -------
Epoch: 189 - loss: 0.1543886004 - test_loss: 0.1177996299
Early Stop Left: 4
------- START EPOCH 190 -------
Epoch: 190 - loss: 0.1541875046 - test_loss: 0.1170184852
-------- Save Best Model! --------
------- START EPOCH 191 -------
Epoch: 191 - loss: 0.1539423682 - test_loss: 0.1164852711
-------- Save Best Model! --------
------- START EPOCH 192 -------
Epoch: 192 - loss: 0.1537398792 - test_loss: 0.1163876025
-------- Save Best Model! --------
------- START EPOCH 193 -------
Epoch: 193 - loss: 0.1535408350 - test_loss: 0.1166845389
Early Stop Left: 4
------- START EPOCH 194 -------
Epoch: 194 - loss: 0.1534018132 - test_loss: 0.1155615720
-------- Save Best Model! --------
------- START EPOCH 195 -------
Epoch: 195 - loss: 0.1531910997 - test_loss: 0.1157443870
Early Stop Left: 4
------- START EPOCH 196 -------
Epoch: 196 - loss: 0.1530106838 - test_loss: 0.1161425019
Early Stop Left: 3
------- START EPOCH 197 -------
Epoch: 197 - loss: 0.1528121231 - test_loss: 0.1153917882
-------- Save Best Model! --------
------- START EPOCH 198 -------
Epoch: 198 - loss: 0.1526318210 - test_loss: 0.1153228902
-------- Save Best Model! --------
------- START EPOCH 199 -------
Epoch: 199 - loss: 0.1524467063 - test_loss: 0.1143621816
-------- Save Best Model! --------
------- START EPOCH 200 -------
Epoch: 200 - loss: 0.1522202455 - test_loss: 0.1145361819
Early Stop Left: 4
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 17%|█▋        | 20/121 [00:00<00:00, 194.15it/s] 33%|███▎      | 40/121 [00:00<00:00, 194.21it/s] 50%|████▉     | 60/121 [00:00<00:00, 194.41it/s] 66%|██████▌   | 80/121 [00:00<00:00, 194.71it/s] 83%|████████▎ | 100/121 [00:00<00:00, 189.96it/s] 99%|█████████▉| 120/121 [00:00<00:00, 189.38it/s]100%|██████████| 121/121 [00:00<00:00, 192.20it/s]
Best micro threshold=0.349917, fscore=0.738
p,r,f1: 0.6944158154176296 0.7883993955596885 0.7384291786182949
throttleing by fixed threshold: 0.5
p,r,f1: 0.7918142912431126 0.6563741330543609 0.7177607695723355
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3499171733856201,
                 'p': 0.6944158154176296,
                 'r': 0.7883993955596885,
                 'f1': 0.7384291786182949},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.7918142912431126,
                 'r': 0.6563741330543609,
                 'f1': 0.7177607695723355}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
Epoch: 114 - loss: 0.1764174368 - test_loss: 0.1413882669
-------- Save Best Model! --------
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.1760794003 - test_loss: 0.1406105475
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.1757563083 - test_loss: 0.1405162676
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.1754144940 - test_loss: 0.1396034458
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.1751270952 - test_loss: 0.1395704878
-------- Save Best Model! --------
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.1748533951 - test_loss: 0.1392028425
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.1745939819 - test_loss: 0.1389779727
-------- Save Best Model! --------
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.1742597180 - test_loss: 0.1383290164
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.1739961320 - test_loss: 0.1377738486
-------- Save Best Model! --------
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.1737326372 - test_loss: 0.1376788414
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.1734568150 - test_loss: 0.1374515866
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.1732665502 - test_loss: 0.1366423340
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.1729543892 - test_loss: 0.1362121090
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.1728307294 - test_loss: 0.1362475555
Early Stop Left: 4
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.1725643961 - test_loss: 0.1361312127
-------- Save Best Model! --------
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.1723530852 - test_loss: 0.1354959724
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.1722292365 - test_loss: 0.1360616734
Early Stop Left: 4
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.1719267260 - test_loss: 0.1349365855
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.1718461810 - test_loss: 0.1351302458
Early Stop Left: 4
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.1716223648 - test_loss: 0.1345661435
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.1714725536 - test_loss: 0.1340989832
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.1713154567 - test_loss: 0.1345073999
Early Stop Left: 4
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.1711401664 - test_loss: 0.1339436136
-------- Save Best Model! --------
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.1709691917 - test_loss: 0.1342355849
Early Stop Left: 4
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.1708731811 - test_loss: 0.1337080372
-------- Save Best Model! --------
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.1706182559 - test_loss: 0.1331269442
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.1705203291 - test_loss: 0.1331795812
Early Stop Left: 4
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.1703846405 - test_loss: 0.1328005907
-------- Save Best Model! --------
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.1701716985 - test_loss: 0.1335005600
Early Stop Left: 4
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.1700638694 - test_loss: 0.1324815464
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.1698765464 - test_loss: 0.1320945023
-------- Save Best Model! --------
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.1697440871 - test_loss: 0.1320934625
-------- Save Best Model! --------
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.1695850594 - test_loss: 0.1321316004
Early Stop Left: 4
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.1695064272 - test_loss: 0.1315000102
-------- Save Best Model! --------
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.1693633255 - test_loss: 0.1314441794
-------- Save Best Model! --------
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.1692006662 - test_loss: 0.1320112685
Early Stop Left: 4
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.1690585426 - test_loss: 0.1310878346
-------- Save Best Model! --------
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.1689669769 - test_loss: 0.1314109826
Early Stop Left: 4
------- START EPOCH 152 -------
Epoch: 152 - loss: 0.1687759659 - test_loss: 0.1303394776
-------- Save Best Model! --------
------- START EPOCH 153 -------
Epoch: 153 - loss: 0.1686577931 - test_loss: 0.1307213111
Early Stop Left: 4
------- START EPOCH 154 -------
Epoch: 154 - loss: 0.1685523057 - test_loss: 0.1309068519
Early Stop Left: 3
------- START EPOCH 155 -------
Epoch: 155 - loss: 0.1684765307 - test_loss: 0.1305131684
Early Stop Left: 2
------- START EPOCH 156 -------
Epoch: 156 - loss: 0.1682997875 - test_loss: 0.1304219018
Early Stop Left: 1
------- START EPOCH 157 -------
Epoch: 157 - loss: 0.1682071846 - test_loss: 0.1304326827
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 16%|█▌        | 19/121 [00:00<00:00, 188.39it/s] 31%|███▏      | 38/121 [00:00<00:00, 184.54it/s] 47%|████▋     | 57/121 [00:00<00:00, 185.19it/s] 63%|██████▎   | 76/121 [00:00<00:00, 181.55it/s] 79%|███████▊  | 95/121 [00:00<00:00, 180.34it/s] 94%|█████████▍| 114/121 [00:00<00:00, 174.86it/s]100%|██████████| 121/121 [00:00<00:00, 178.84it/s]------- START EPOCH 115 -------
Epoch: 115 - loss: 0.1987391742 - test_loss: 0.1682957732
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.1984845444 - test_loss: 0.1679433377
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.1981987135 - test_loss: 0.1671842585
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.1979307071 - test_loss: 0.1673175585
Early Stop Left: 4
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.1977008621 - test_loss: 0.1667052419
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.1974321094 - test_loss: 0.1662345248
-------- Save Best Model! --------
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.1970761298 - test_loss: 0.1655831284
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.1967757350 - test_loss: 0.1653921172
-------- Save Best Model! --------
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.1964663440 - test_loss: 0.1645543950
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.1961494767 - test_loss: 0.1640813136
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.1958841882 - test_loss: 0.1639318774
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.1954981427 - test_loss: 0.1633736316
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.1952607258 - test_loss: 0.1630395757
-------- Save Best Model! --------
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.1949204269 - test_loss: 0.1627708916
-------- Save Best Model! --------
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.1946261123 - test_loss: 0.1618964192
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.1944080957 - test_loss: 0.1622766395
Early Stop Left: 4
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.1940510482 - test_loss: 0.1613667361
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.1938411766 - test_loss: 0.1610304302
-------- Save Best Model! --------
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.1935686459 - test_loss: 0.1602248126
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.1933424916 - test_loss: 0.1599971690
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.1931075583 - test_loss: 0.1596843986
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.1928302836 - test_loss: 0.1592400371
-------- Save Best Model! --------
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.1925896827 - test_loss: 0.1590589008
-------- Save Best Model! --------
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.1924182225 - test_loss: 0.1585702431
-------- Save Best Model! --------
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.1921243289 - test_loss: 0.1580517493
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.1919558161 - test_loss: 0.1577379646
-------- Save Best Model! --------
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.1917290390 - test_loss: 0.1574361188
-------- Save Best Model! --------
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.1914641836 - test_loss: 0.1569477089
-------- Save Best Model! --------
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.1912805356 - test_loss: 0.1569363082
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.1910889526 - test_loss: 0.1562135622
-------- Save Best Model! --------
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.1908593357 - test_loss: 0.1558591938
-------- Save Best Model! --------
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.1906773969 - test_loss: 0.1561773264
Early Stop Left: 4
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.1905223994 - test_loss: 0.1555856167
-------- Save Best Model! --------
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.1903509892 - test_loss: 0.1546735777
-------- Save Best Model! --------
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.1901400184 - test_loss: 0.1546853962
Early Stop Left: 4
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.1899723627 - test_loss: 0.1545129759
-------- Save Best Model! --------
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.1898210597 - test_loss: 0.1546756911
Early Stop Left: 4
------- START EPOCH 152 -------
Epoch: 152 - loss: 0.1896029326 - test_loss: 0.1537474893
-------- Save Best Model! --------
------- START EPOCH 153 -------
Epoch: 153 - loss: 0.1894618840 - test_loss: 0.1537087621
-------- Save Best Model! --------
------- START EPOCH 154 -------
Epoch: 154 - loss: 0.1893088942 - test_loss: 0.1538401930
Early Stop Left: 4
------- START EPOCH 155 -------
Epoch: 155 - loss: 0.1891846231 - test_loss: 0.1534946310
-------- Save Best Model! --------
------- START EPOCH 156 -------
Epoch: 156 - loss: 0.1889851514 - test_loss: 0.1525511005
-------- Save Best Model! --------
------- START EPOCH 157 -------
Epoch: 157 - loss: 0.1888604891 - test_loss: 0.1535542133
Early Stop Left: 4
------- START EPOCH 158 -------
Epoch: 158 - loss: 0.1887113999 - test_loss: 0.1524910661
-------- Save Best Model! --------
------- START EPOCH 159 -------
Epoch: 159 - loss: 0.1885755164 - test_loss: 0.1521786808
-------- Save Best Model! --------
------- START EPOCH 160 -------
Epoch: 160 - loss: 0.1884365064 - test_loss: 0.1523791486
Early Stop Left: 4
------- START EPOCH 161 -------
Epoch: 161 - loss: 0.1882645388 - test_loss: 0.1510614695
-------- Save Best Model! --------
------- START EPOCH 162 -------
Epoch: 162 - loss: 0.1881226029 - test_loss: 0.1515938523
Early Stop Left: 4
------- START EPOCH 163 -------
Epoch: 163 - loss: 0.1880832316 - test_loss: 0.1514831097
Early Stop Left: 3
------- START EPOCH 164 -------
Epoch: 164 - loss: 0.1878803968 - test_loss: 0.1517569383
Early Stop Left: 2
------- START EPOCH 165 -------
Epoch: 165 - loss: 0.1877764746 - test_loss: 0.1510664904
Early Stop Left: 1
------- START EPOCH 166 -------
Epoch: 166 - loss: 0.1876223887 - test_loss: 0.1506906987
-------- Save Best Model! --------
------- START EPOCH 167 -------
Epoch: 167 - loss: 0.1875203118 - test_loss: 0.1503038456
-------- Save Best Model! --------
------- START EPOCH 168 -------
Epoch: 168 - loss: 0.1873408976 - test_loss: 0.1507192449
Early Stop Left: 4
------- START EPOCH 169 -------
Epoch: 169 - loss: 0.1871999938 - test_loss: 0.1505489474
Early Stop Left: 3
------- START EPOCH 170 -------
Epoch: 170 - loss: 0.1871137569 - test_loss: 0.1503259194
Early Stop Left: 2
------- START EPOCH 171 -------
Epoch: 171 - loss: 0.1869835283 - test_loss: 0.1498495037
-------- Save Best Model! --------
------- START EPOCH 172 -------
Epoch: 172 - loss: 0.1868672240 - test_loss: 0.1498461154
-------- Save Best Model! --------
------- START EPOCH 173 -------
Epoch: 173 - loss: 0.1866906618 - test_loss: 0.1496136333
-------- Save Best Model! --------
------- START EPOCH 174 -------
Epoch: 174 - loss: 0.1866694545 - test_loss: 0.1495219414
-------- Save Best Model! --------
------- START EPOCH 175 -------
Epoch: 175 - loss: 0.1864901847 - test_loss: 0.1493426558
-------- Save Best Model! --------
------- START EPOCH 176 -------
Epoch: 176 - loss: 0.1863942647 - test_loss: 0.1487362228
-------- Save Best Model! --------
------- START EPOCH 177 -------
Epoch: 177 - loss: 0.1862616990 - test_loss: 0.1492027294
Early Stop Left: 4
------- START EPOCH 178 -------
Epoch: 178 - loss: 0.1861358998 - test_loss: 0.1484687960
-------- Save Best Model! --------
------- START EPOCH 179 -------
Epoch: 179 - loss: 0.1860556216 - test_loss: 0.1482861243
-------- Save Best Model! --------
------- START EPOCH 180 -------
Epoch: 180 - loss: 0.1859667632 - test_loss: 0.1482506673
-------- Save Best Model! --------
------- START EPOCH 181 -------
Epoch: 181 - loss: 0.1858657635 - test_loss: 0.1484052862
Early Stop Left: 4
------- START EPOCH 182 -------
Epoch: 182 - loss: 0.1857325055 - test_loss: 0.1478455290
Best micro threshold=0.336526, fscore=0.715
p,r,f1: 0.6628878139603769 0.7769274284164439 0.715391405050908
throttleing by fixed threshold: 0.5
p,r,f1: 0.7778493010730463 0.6124096245495757 0.6852858696274252
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3365262448787689,
                 'p': 0.6628878139603769,
                 'r': 0.7769274284164439,
                 'f1': 0.715391405050908},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.7778493010730463,
                 'r': 0.6124096245495757,
                 'f1': 0.6852858696274252}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.2096746957 - test_loss: 0.1982832943
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.2094483237 - test_loss: 0.1974969216
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.2092129549 - test_loss: 0.1969428719
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.2090005273 - test_loss: 0.1970443318
Early Stop Left: 4
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.2088306100 - test_loss: 0.1964939658
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.2086318053 - test_loss: 0.1960565242
-------- Save Best Model! --------
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.2083862458 - test_loss: 0.1953635016
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.2081856685 - test_loss: 0.1952932876
-------- Save Best Model! --------
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.2079941626 - test_loss: 0.1946566500
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.2077907945 - test_loss: 0.1941753344
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.2076386310 - test_loss: 0.1943879118
Early Stop Left: 4
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.2073910769 - test_loss: 0.1938477646
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.2072508895 - test_loss: 0.1935140787
-------- Save Best Model! --------
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.2070380694 - test_loss: 0.1930797849
-------- Save Best Model! --------
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.2068593592 - test_loss: 0.1928217045
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.2067323668 - test_loss: 0.1930476968
Early Stop Left: 4
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.2064821783 - test_loss: 0.1924853382
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.2063475763 - test_loss: 0.1921025336
-------- Save Best Model! --------
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.2061610344 - test_loss: 0.1914275010
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.2060111303 - test_loss: 0.1914423624
Early Stop Left: 4
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.2058528962 - test_loss: 0.1910969945
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.2056518997 - test_loss: 0.1907128954
-------- Save Best Model! --------
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.2054805384 - test_loss: 0.1903390641
-------- Save Best Model! --------
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.2053573768 - test_loss: 0.1896800407
-------- Save Best Model! --------
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.2051574261 - test_loss: 0.1897368346
Early Stop Left: 4
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.2050307158 - test_loss: 0.1893085467
-------- Save Best Model! --------
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.2048727604 - test_loss: 0.1894422923
Early Stop Left: 4
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.2046924080 - test_loss: 0.1889386680
-------- Save Best Model! --------
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.2045546635 - test_loss: 0.1889452028
Early Stop Left: 4
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.2044281510 - test_loss: 0.1885678586
-------- Save Best Model! --------
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.2042520413 - test_loss: 0.1878653395
-------- Save Best Model! --------
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.2041391633 - test_loss: 0.1878928816
Early Stop Left: 4
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.2040418500 - test_loss: 0.1880076368
Early Stop Left: 3
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.2039250226 - test_loss: 0.1867912963
-------- Save Best Model! --------
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.2037796808 - test_loss: 0.1869387250
Early Stop Left: 4
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.2036757101 - test_loss: 0.1866719295
-------- Save Best Model! --------
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.2035916872 - test_loss: 0.1869482929
Early Stop Left: 4
------- START EPOCH 152 -------
Epoch: 152 - loss: 0.2034367180 - test_loss: 0.1861426302
-------- Save Best Model! --------
------- START EPOCH 153 -------
Epoch: 153 - loss: 0.2033653095 - test_loss: 0.1861707037
Early Stop Left: 4
------- START EPOCH 154 -------
Epoch: 154 - loss: 0.2032593823 - test_loss: 0.1863566721
Early Stop Left: 3
------- START EPOCH 155 -------
Epoch: 155 - loss: 0.2031869703 - test_loss: 0.1860082494
-------- Save Best Model! --------
------- START EPOCH 156 -------
Epoch: 156 - loss: 0.2030501084 - test_loss: 0.1853142224
-------- Save Best Model! --------
------- START EPOCH 157 -------
Epoch: 157 - loss: 0.2029686938 - test_loss: 0.1862935908
Early Stop Left: 4
------- START EPOCH 158 -------
Epoch: 158 - loss: 0.2028914253 - test_loss: 0.1857874607
Early Stop Left: 3
------- START EPOCH 159 -------
Epoch: 159 - loss: 0.2028089293 - test_loss: 0.1854459121
Early Stop Left: 2
------- START EPOCH 160 -------
Epoch: 160 - loss: 0.2027125908 - test_loss: 0.1854190102
Early Stop Left: 1
------- START EPOCH 161 -------
Epoch: 161 - loss: 0.2026006003 - test_loss: 0.1840112443
-------- Save Best Model! --------
------- START EPOCH 162 -------
Epoch: 162 - loss: 0.2025221648 - test_loss: 0.1850839847
Early Stop Left: 4
------- START EPOCH 163 -------
Epoch: 163 - loss: 0.2025026178 - test_loss: 0.1843085985
Early Stop Left: 3
------- START EPOCH 164 -------
Epoch: 164 - loss: 0.2023800557 - test_loss: 0.1848147274
Early Stop Left: 2
------- START EPOCH 165 -------
Epoch: 165 - loss: 0.2023001403 - test_loss: 0.1846690583
Early Stop Left: 1
------- START EPOCH 166 -------
Epoch: 166 - loss: 0.2022062417 - test_loss: 0.1839330976
-------- Save Best Model! --------
------- START EPOCH 167 -------
Epoch: 167 - loss: 0.2021257133 - test_loss: 0.1837027430
-------- Save Best Model! --------
------- START EPOCH 168 -------
Epoch: 168 - loss: 0.2020249151 - test_loss: 0.1842510869
Early Stop Left: 4
------- START EPOCH 169 -------
Epoch: 169 - loss: 0.2019266053 - test_loss: 0.1836899250
-------- Save Best Model! --------
------- START EPOCH 170 -------
Epoch: 170 - loss: 0.2018705287 - test_loss: 0.1839745615
Early Stop Left: 4
------- START EPOCH 171 -------
Epoch: 171 - loss: 0.2017818767 - test_loss: 0.1833240277
-------- Save Best Model! --------
------- START EPOCH 172 -------
Epoch: 172 - loss: 0.2017163381 - test_loss: 0.1831790197
-------- Save Best Model! --------
------- START EPOCH 173 -------
Epoch: 173 - loss: 0.2015794853 - test_loss: 0.1828545719
-------- Save Best Model! --------
------- START EPOCH 174 -------
Epoch: 174 - loss: 0.2015594713 - test_loss: 0.1824643792
-------- Save Best Model! --------
------- START EPOCH 175 -------
Epoch: 175 - loss: 0.2014370427 - test_loss: 0.1822074421
-------- Save Best Model! --------
------- START EPOCH 176 -------
Epoch: 176 - loss: 0.2013630893 - test_loss: 0.1822505316
Early Stop Left: 4
------- START EPOCH 177 -------
Epoch: 177 - loss: 0.2012576419 - test_loss: 0.1826503643
Early Stop Left: 3
------- START EPOCH 178 -------
Epoch: 178 - loss: 0.2011720698 - test_loss: 0.1822402390
Early Stop Left: 2
------- START EPOCH 179 -------
Epoch: 179 - loss: 0.2011151557 - test_loss: 0.1816807881
-------- Save Best Model! --------
------- START EPOCH 180 -------
Epoch: 180 - loss: 0.2010354969 - test_loss: 0.1815544586
-------- Save Best Model! --------
------- START EPOCH 181 -------
Epoch: 181 - loss: 0.2009632027 - test_loss: 0.1815886473
Early Stop Left: 4
------- START EPOCH 182 -------
Epoch: 182 - loss: 0.2008538376 - test_loss: 0.1816835951
Early Stop Left: 3
------- START EPOCH 183 -------
Epoch: 183 - loss: 0.2007870607 - test_loss: 0.1816018798
Early Stop Left: 2
------- START EPOCH 184 -------
-------- Save Best Model! --------
------- START EPOCH 183 -------
Epoch: 183 - loss: 0.1856512866 - test_loss: 0.1481119627
Early Stop Left: 4
------- START EPOCH 184 -------
Epoch: 184 - loss: 0.1855255342 - test_loss: 0.1481651994
Early Stop Left: 3
------- START EPOCH 185 -------
Epoch: 185 - loss: 0.1854816264 - test_loss: 0.1477071048
-------- Save Best Model! --------
------- START EPOCH 186 -------
Epoch: 186 - loss: 0.1853801845 - test_loss: 0.1470893533
-------- Save Best Model! --------
------- START EPOCH 187 -------
Epoch: 187 - loss: 0.1852523735 - test_loss: 0.1474432296
Early Stop Left: 4
------- START EPOCH 188 -------
Epoch: 188 - loss: 0.1851480259 - test_loss: 0.1470425275
-------- Save Best Model! --------
------- START EPOCH 189 -------
Epoch: 189 - loss: 0.1850984812 - test_loss: 0.1470701453
Early Stop Left: 4
------- START EPOCH 190 -------
Epoch: 190 - loss: 0.1849821361 - test_loss: 0.1468304526
-------- Save Best Model! --------
------- START EPOCH 191 -------
Epoch: 191 - loss: 0.1848716468 - test_loss: 0.1465991027
-------- Save Best Model! --------
------- START EPOCH 192 -------
Epoch: 192 - loss: 0.1848213370 - test_loss: 0.1463214359
-------- Save Best Model! --------
------- START EPOCH 193 -------
Epoch: 193 - loss: 0.1846883435 - test_loss: 0.1472030432
Early Stop Left: 4
------- START EPOCH 194 -------
Epoch: 194 - loss: 0.1846253938 - test_loss: 0.1464637144
Early Stop Left: 3
------- START EPOCH 195 -------
Epoch: 195 - loss: 0.1845393694 - test_loss: 0.1464519232
Early Stop Left: 2
------- START EPOCH 196 -------
Epoch: 196 - loss: 0.1844899098 - test_loss: 0.1463926609
Early Stop Left: 1
------- START EPOCH 197 -------
Epoch: 197 - loss: 0.1843869855 - test_loss: 0.1460819789
-------- Save Best Model! --------
------- START EPOCH 198 -------
Epoch: 198 - loss: 0.1843124488 - test_loss: 0.1460994972
Early Stop Left: 4
------- START EPOCH 199 -------
Epoch: 199 - loss: 0.1842265687 - test_loss: 0.1456358800
-------- Save Best Model! --------
------- START EPOCH 200 -------
Epoch: 200 - loss: 0.1841020817 - test_loss: 0.1452849155
-------- Save Best Model! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 15%|█▍        | 18/121 [00:00<00:00, 172.12it/s] 30%|██▉       | 36/121 [00:00<00:00, 173.79it/s] 45%|████▍     | 54/121 [00:00<00:00, 176.08it/s] 60%|█████▉    | 72/121 [00:00<00:00, 173.25it/s] 74%|███████▍  | 90/121 [00:00<00:00, 173.21it/s] 89%|████████▉ | 108/121 [00:00<00:00, 172.25it/s]100%|██████████| 121/121 [00:00<00:00, 173.80it/s]
Epoch: 184 - loss: 0.2006877819 - test_loss: 0.1812551031
-------- Save Best Model! --------
------- START EPOCH 185 -------
Epoch: 185 - loss: 0.2006472682 - test_loss: 0.1815057196
Early Stop Left: 4
------- START EPOCH 186 -------
Epoch: 186 - loss: 0.2005620147 - test_loss: 0.1807502875
-------- Save Best Model! --------
------- START EPOCH 187 -------
Epoch: 187 - loss: 0.2004705886 - test_loss: 0.1808883718
Early Stop Left: 4
------- START EPOCH 188 -------
Epoch: 188 - loss: 0.2003593610 - test_loss: 0.1807567607
Early Stop Left: 3
------- START EPOCH 189 -------
Epoch: 189 - loss: 0.2003479083 - test_loss: 0.1802434197
-------- Save Best Model! --------
------- START EPOCH 190 -------
Epoch: 190 - loss: 0.2002305357 - test_loss: 0.1805177655
Early Stop Left: 4
------- START EPOCH 191 -------
Epoch: 191 - loss: 0.2001495634 - test_loss: 0.1799928531
-------- Save Best Model! --------
------- START EPOCH 192 -------
Epoch: 192 - loss: 0.2000871331 - test_loss: 0.1795547825
-------- Save Best Model! --------
------- START EPOCH 193 -------
Epoch: 193 - loss: 0.1999854732 - test_loss: 0.1804306273
Early Stop Left: 4
------- START EPOCH 194 -------
Epoch: 194 - loss: 0.1999280148 - test_loss: 0.1801343984
Early Stop Left: 3
------- START EPOCH 195 -------
Epoch: 195 - loss: 0.1998579886 - test_loss: 0.1800150784
Early Stop Left: 2
------- START EPOCH 196 -------
Epoch: 196 - loss: 0.1998018366 - test_loss: 0.1801895276
Early Stop Left: 1
------- START EPOCH 197 -------
Epoch: 197 - loss: 0.1997235162 - test_loss: 0.1793843758
-------- Save Best Model! --------
------- START EPOCH 198 -------
Epoch: 198 - loss: 0.1996384302 - test_loss: 0.1790416057
-------- Save Best Model! --------
------- START EPOCH 199 -------
Epoch: 199 - loss: 0.1995937431 - test_loss: 0.1789999295
-------- Save Best Model! --------
------- START EPOCH 200 -------
Epoch: 200 - loss: 0.1994543215 - test_loss: 0.1788245581
-------- Save Best Model! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 17%|█▋        | 20/121 [00:00<00:00, 195.75it/s] 33%|███▎      | 40/121 [00:00<00:00, 195.37it/s] 50%|████▉     | 60/121 [00:00<00:00, 195.62it/s] 66%|██████▌   | 80/121 [00:00<00:00, 195.54it/s] 83%|████████▎ | 100/121 [00:00<00:00, 195.43it/s] 99%|█████████▉| 120/121 [00:00<00:00, 195.05it/s]100%|██████████| 121/121 [00:00<00:00, 196.18it/s]
Best micro threshold=0.364787, fscore=0.725
p,r,f1: 0.6732072926031523 0.785694912627378 0.725114463526864
throttleing by fixed threshold: 0.5
p,r,f1: 0.7777909262687763 0.6315750319655934 0.6970983317011002
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3647867441177368,
                 'p': 0.6732072926031523,
                 'r': 0.785694912627378,
                 'f1': 0.725114463526864},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.7777909262687763,
                 'r': 0.6315750319655934,
                 'f1': 0.6970983317011002}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm

Best micro threshold=0.373910, fscore=0.710
p,r,f1: 0.652840813035619 0.7780464179162307 0.7099657402267721
throttleing by fixed threshold: 0.5
p,r,f1: 0.7691682742028223 0.6074656127707388 0.678819955594196
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3739103376865387,
                 'p': 0.652840813035619,
                 'r': 0.7780464179162307,
                 'f1': 0.7099657402267721},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.7691682742028223,
                 'r': 0.6074656127707388,
                 'f1': 0.678819955594196}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.2057534385 - test_loss: 0.1824841050
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.2055354664 - test_loss: 0.1821636111
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.2052993432 - test_loss: 0.1814994606
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.2050828818 - test_loss: 0.1815249969
Early Stop Left: 4
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.2049175625 - test_loss: 0.1809804231
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.2047087840 - test_loss: 0.1807458849
-------- Save Best Model! --------
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.2044408738 - test_loss: 0.1801109832
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.2042231042 - test_loss: 0.1799627432
-------- Save Best Model! --------
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.2040086700 - test_loss: 0.1793055566
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.2037948103 - test_loss: 0.1790273581
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.2036281615 - test_loss: 0.1790061572
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.2033407292 - test_loss: 0.1785651193
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.2031867233 - test_loss: 0.1785417444
-------- Save Best Model! --------
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.2029372769 - test_loss: 0.1776991792
-------- Save Best Model! --------
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.2027348245 - test_loss: 0.1775064510
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.2025892658 - test_loss: 0.1779202228
Early Stop Left: 4
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.2022989956 - test_loss: 0.1768982805
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.2021469973 - test_loss: 0.1772847548
Early Stop Left: 4
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.2019394090 - test_loss: 0.1762932613
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.2017561879 - test_loss: 0.1761887066
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.2015744743 - test_loss: 0.1758130013
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.2013413556 - test_loss: 0.1754161727
-------- Save Best Model! --------
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.2011406596 - test_loss: 0.1751786824
-------- Save Best Model! --------
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.2009927103 - test_loss: 0.1744443923
-------- Save Best Model! --------
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.2007538989 - test_loss: 0.1742784950
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.2005997826 - test_loss: 0.1740474226
-------- Save Best Model! --------
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.2004058891 - test_loss: 0.1735445422
-------- Save Best Model! --------
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.2001975255 - test_loss: 0.1738194131
Early Stop Left: 4
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.2000298946 - test_loss: 0.1731842173
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.1998653313 - test_loss: 0.1729295047
-------- Save Best Model! --------
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.1996663995 - test_loss: 0.1724185452
-------- Save Best Model! --------
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.1995146663 - test_loss: 0.1725604649
Early Stop Left: 4
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.1993834155 - test_loss: 0.1720688491
-------- Save Best Model! --------
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.1992421910 - test_loss: 0.1719079215
-------- Save Best Model! --------
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.1990652233 - test_loss: 0.1715393600
-------- Save Best Model! --------
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.1989288595 - test_loss: 0.1709807364
-------- Save Best Model! --------
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.1988132765 - test_loss: 0.1710841242
Early Stop Left: 4
------- START EPOCH 152 -------
Epoch: 152 - loss: 0.1986299260 - test_loss: 0.1706061485
-------- Save Best Model! --------
------- START EPOCH 153 -------
Epoch: 153 - loss: 0.1985331126 - test_loss: 0.1703972648
-------- Save Best Model! --------
------- START EPOCH 154 -------
Epoch: 154 - loss: 0.1983977489 - test_loss: 0.1706203486
Early Stop Left: 4
------- START EPOCH 155 -------
Epoch: 155 - loss: 0.1983042236 - test_loss: 0.1703937805
-------- Save Best Model! --------
------- START EPOCH 156 -------
Epoch: 156 - loss: 0.1981464265 - test_loss: 0.1699876844
-------- Save Best Model! --------
------- START EPOCH 157 -------
Epoch: 157 - loss: 0.1980393261 - test_loss: 0.1705689004
Early Stop Left: 4
------- START EPOCH 158 -------
Epoch: 158 - loss: 0.1979266381 - test_loss: 0.1702123012
Early Stop Left: 3
------- START EPOCH 159 -------
Epoch: 159 - loss: 0.1978229922 - test_loss: 0.1697845431
-------- Save Best Model! --------
------- START EPOCH 160 -------
Epoch: 160 - loss: 0.1977243330 - test_loss: 0.1698556137
Early Stop Left: 4
------- START EPOCH 161 -------
Epoch: 161 - loss: 0.1975695877 - test_loss: 0.1682892012
-------- Save Best Model! --------
------- START EPOCH 162 -------
Epoch: 162 - loss: 0.1974700776 - test_loss: 0.1689667805
Early Stop Left: 4
------- START EPOCH 163 -------
Epoch: 163 - loss: 0.1974453342 - test_loss: 0.1685856754
Early Stop Left: 3
------- START EPOCH 164 -------
Epoch: 164 - loss: 0.1972863568 - test_loss: 0.1686700251
Early Stop Left: 2
------- START EPOCH 165 -------
Epoch: 165 - loss: 0.1972015459 - test_loss: 0.1684204969
Early Stop Left: 1
------- START EPOCH 166 -------
Epoch: 166 - loss: 0.1970808731 - test_loss: 0.1677410359
-------- Save Best Model! --------
------- START EPOCH 167 -------
Epoch: 167 - loss: 0.1969843082 - test_loss: 0.1677626453
Early Stop Left: 4
------- START EPOCH 168 -------
Epoch: 168 - loss: 0.1968588972 - test_loss: 0.1679720872
Early Stop Left: 3
------- START EPOCH 169 -------
Epoch: 169 - loss: 0.1967434911 - test_loss: 0.1677731023
Early Stop Left: 2
------- START EPOCH 170 -------
Epoch: 170 - loss: 0.1966744854 - test_loss: 0.1676341975
-------- Save Best Model! --------
------- START EPOCH 171 -------
Epoch: 171 - loss: 0.1965675613 - test_loss: 0.1672518220
-------- Save Best Model! --------
------- START EPOCH 172 -------
Epoch: 172 - loss: 0.1964954926 - test_loss: 0.1676997902
Early Stop Left: 4
------- START EPOCH 173 -------
Epoch: 173 - loss: 0.1963325707 - test_loss: 0.1668484949
-------- Save Best Model! --------
------- START EPOCH 174 -------
Epoch: 174 - loss: 0.1963201545 - test_loss: 0.1670333739
Early Stop Left: 4
------- START EPOCH 175 -------
Epoch: 175 - loss: 0.1961754714 - test_loss: 0.1665727697
-------- Save Best Model! --------
------- START EPOCH 176 -------
Epoch: 176 - loss: 0.1960940635 - test_loss: 0.1665548596
-------- Save Best Model! --------
------- START EPOCH 177 -------
Epoch: 177 - loss: 0.1959735524 - test_loss: 0.1664831758
-------- Save Best Model! --------
------- START EPOCH 178 -------
Epoch: 178 - loss: 0.1958852087 - test_loss: 0.1658232158
-------- Save Best Model! --------
------- START EPOCH 179 -------
Epoch: 179 - loss: 0.1958189054 - test_loss: 0.1662357575
Early Stop Left: 4
------- START EPOCH 180 -------
Epoch: 180 - loss: 0.1957293876 - test_loss: 0.1646750849
-------- Save Best Model! --------
------- START EPOCH 181 -------
Epoch: 181 - loss: 0.1956530682 - test_loss: 0.1656332560
Early Stop Left: 4
------- START EPOCH 182 -------
Epoch: 182 - loss: 0.1955256461 - test_loss: 0.1650292088
Early Stop Left: 3
------- START EPOCH 183 -------
Epoch: 183 - loss: 0.1954553792 - test_loss: 0.1656967171
Early Stop Left: 2
------- START EPOCH 184 -------
Epoch: 184 - loss: 0.1953438686 - test_loss: 0.1655504106
Early Stop Left: 1
------- START EPOCH 185 -------
Epoch: 185 - loss: 0.1953069181 - test_loss: 0.1651311554
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 16%|█▌        | 19/121 [00:00<00:00, 186.21it/s] 31%|███▏      | 38/121 [00:00<00:00, 186.72it/s] 47%|████▋     | 57/121 [00:00<00:00, 187.35it/s] 63%|██████▎   | 76/121 [00:00<00:00, 186.59it/s] 79%|███████▊  | 95/121 [00:00<00:00, 185.31it/s] 94%|█████████▍| 114/121 [00:00<00:00, 185.25it/s]100%|██████████| 121/121 [00:00<00:00, 186.59it/s]
Best micro threshold=0.354876, fscore=0.708
p,r,f1: 0.6496155794946358 0.7769320779573017 0.7075924584975997
throttleing by fixed threshold: 0.5
p,r,f1: 0.777434778004936 0.5917191677321865 0.6719814981206829
{'model': 'vit',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3548758327960968,
                 'p': 0.6496155794946358,
                 'r': 0.7769320779573017,
                 'f1': 0.7075924584975997},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.777434778004936,
                 'r': 0.5917191677321865,
                 'f1': 0.6719814981206829}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
Generation start
preprocessing_gen with context
b4 model prediction col0
 Index(['id', 'cycle', 'addr', 'ip', 'hit', 'raw', 'block_address',
       'page_address', 'page_offset', 'block_index', 'block_addr_delta',
       'patch', 'past', 'past_ip', 'past_page'],
      dtype='object')
predicting
  0%|          | 0/122 [00:00<?, ?it/s]  1%|          | 1/122 [00:00<01:11,  1.70it/s] 27%|██▋       | 33/122 [00:00<00:01, 63.63it/s] 54%|█████▍    | 66/122 [00:00<00:00, 121.10it/s] 81%|████████  | 99/122 [00:00<00:00, 169.30it/s]100%|██████████| 122/122 [00:00<00:00, 126.45it/s]
after model prediction col1
 Index(['id', 'cycle', 'addr', 'ip', 'block_address', 'y_score'], dtype='object')
post_processing, opt_threshold<0.9
after delta filter
 Index(['id', 'pred_hex'], dtype='object')
           app  mean  max  min  median
0  433.milc-s0   1.0  1.0  1.0     1.0
Done: results saved at: res/433.milc-s0.vit.stu.90.0.pkl.degree_stats.csv
Generation start
preprocessing_gen with context
b4 model prediction col0
 Index(['id', 'cycle', 'addr', 'ip', 'hit', 'raw', 'block_address',
       'page_address', 'page_offset', 'block_index', 'block_addr_delta',
       'patch', 'past', 'past_ip', 'past_page'],
      dtype='object')
predicting
  0%|          | 0/122 [00:00<?, ?it/s]  1%|          | 1/122 [00:00<01:18,  1.54it/s] 13%|█▎        | 16/122 [00:00<00:03, 28.09it/s] 25%|██▌       | 31/122 [00:00<00:01, 52.79it/s] 39%|███▊      | 47/122 [00:00<00:00, 75.92it/s] 52%|█████▏    | 63/122 [00:01<00:00, 94.83it/s] 65%|██████▍   | 79/122 [00:01<00:00, 109.65it/s] 77%|███████▋  | 94/122 [00:01<00:00, 120.05it/s] 90%|█████████ | 110/122 [00:01<00:00, 129.03it/s]100%|██████████| 122/122 [00:01<00:00, 83.76it/s] 
after model prediction col1
 Index(['id', 'cycle', 'addr', 'ip', 'block_address', 'y_score'], dtype='object')
post_processing, opt_threshold<0.9
after delta filter
 Index(['id', 'pred_hex'], dtype='object')
           app  mean  max  min  median
0  433.milc-s0   1.0  1.0  1.0     1.0
Done: results saved at: res/433.milc-s0.vitt.pkl.degree_stats.csv
Traceback (most recent call last):
  File "src/3_vit.py", line 5, in <module>
    import vq_amm
  File "/data/neelesh/DART_by_app/433/src/vq_amm.py", line 5, in <module>
    import vquantizers as vq
  File "/data/neelesh/DART_by_app/433/src/vquantizers.py", line 11, in <module>
    from utils import kmeans
ImportError: cannot import name 'kmeans' from 'utils' (/data/neelesh/DART_by_app/433/src/utils.py)
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000004
Manual and Torch results cosine similarity (Test): 0.99999994
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.31, 0.446
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0431, 0.0241
--- total mse / var(X): 0.235
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0139, 0.0142
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0122, 0.0119
--- total mse / var(X): 0.013
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.182, 0.193
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.173, 0.163
--- total mse / var(X): 0.178
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.146, 0.15
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.173, 0.168
--- total mse / var(X): 0.159
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0307, 0.0202
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0168, 0.0226
--- total mse / var(X): 0.0214
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.228, 0.188
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.208, 0.244
--- total mse / var(X): 0.216
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00229, 0.00207
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0401, 0.044
--- total mse / var(X): 0.023
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0688, 0.0652
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.1, 0.106
--- total mse / var(X): 0.0854
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00162, 0.00124
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00244, 0.00301
--- total mse / var(X): 0.00212
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 1.01, 1.15
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 1.42, 1.22
--- total mse / var(X): 1.19
start table evaluation...
Elapsed time: 63.49085879325867 seconds
Cosine similarity between AMM and exact (Train): 0.6371986
Cosine similarity between AMM and exact (Test): 0.63790816
p,r,f1: 0.6494858848247294 0.7771041109690419 0.7075868365848615
p,r,f1: 0.2660928873010718 0.26665581773799835 0.2663740551085101
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.6494858848247294, 0.7771041109690419, 0.7075868365848615],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16],
               'cossim_layer_train': [0.9878972768783569,
                                      0.9797797203063965,
                                      0.9774407744407654,
                                      0.6371985673904419],
               'cossim_layer_test': [0.9871790409088135,
                                     0.9796370267868042,
                                     0.9771984219551086,
                                     0.6379081606864929],
               'cossim_amm_train': [0.9784223437309265,
                                    0.9939465522766113,
                                    0.9337385296821594,
                                    0.9030436277389526,
                                    0.9539899826049805,
                                    0.8732619285583496,
                                    0.9692774415016174,
                                    0.9013750553131104],
               'cossim_amm_test': [0.9774486422538757,
                                   0.9948012828826904,
                                   0.9340847730636597,
                                   0.9049980640411377,
                                   0.9537076950073242,
                                   0.8707717657089233,
                                   0.9700609445571899,
                                   0.9007870554924011],
               'f1': [0.2660928873010718,
                      0.26665581773799835,
                      0.2663740551085101],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 16),
                              (192, 2, 16),
                              (16, 16, 2),
                              (16, 16, 2),
                              (32, 2, 16),
                              (32, 2, 16),
                              (32, 2, 16),
                              (256, 2, 16)],
               'lut_total_size': 19456}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (6). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000004
Manual and Torch results cosine similarity (Test): 0.99999994
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.216, 0.312
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0143, 0.00802
--- total mse / var(X): 0.16
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0101, 0.0103
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0101, 0.00983
--- total mse / var(X): 0.0101
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0485, 0.0512
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0354, 0.0334
--- total mse / var(X): 0.0423
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00554, 0.00567
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00486, 0.00475
--- total mse / var(X): 0.00521
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0067, 0.00643
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.004, 0.00416
--- total mse / var(X): 0.0053
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0951, 0.0787
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.104, 0.121
--- total mse / var(X): 0.1
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.01, 0.00928
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00136, 0.00147
--- total mse / var(X): 0.00537
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0118, 0.0112
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00964, 0.0102
--- total mse / var(X): 0.0107
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00194, 0.00147
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00633, 0.00787
--- total mse / var(X): 0.00467
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 1.93e-05, 1.79e-05
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 4.77e-06, 5.12e-06
--- total mse / var(X): 1.15e-05
start table evaluation...
Elapsed time: 141.05197954177856 seconds
Cosine similarity between AMM and exact (Train): 0.7314537
Cosine similarity between AMM and exact (Test): 0.7337199
p,r,f1: 0.6494858848247294 0.7771041109690419 0.7075868365848615
p,r,f1: 0.31480518759712867 0.5274392653725445 0.39428129516615357
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.6494858848247294, 0.7771041109690419, 0.7075868365848615],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32],
               'cossim_layer_train': [0.9908063411712646,
                                      0.9923540353775024,
                                      0.9916070103645325,
                                      0.7314536571502686],
               'cossim_layer_test': [0.9904956817626953,
                                     0.9922200441360474,
                                     0.991584300994873,
                                     0.7337198257446289],
               'cossim_amm_train': [0.9836227893829346,
                                    0.9958662986755371,
                                    0.9853373169898987,
                                    0.9648828506469727,
                                    0.9854074716567993,
                                    0.9310585856437683,
                                    0.9819021821022034,
                                    0.884823203086853],
               'cossim_amm_test': [0.9832166433334351,
                                   0.9965816140174866,
                                   0.9857707619667053,
                                   0.9663742184638977,
                                   0.9852882027626038,
                                   0.9310212731361389,
                                   0.9828363656997681,
                                   0.8857997059822083],
               'f1': [0.31480518759712867,
                      0.5274392653725445,
                      0.39428129516615357],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 32),
                              (192, 2, 32),
                              (32, 32, 2),
                              (32, 32, 2),
                              (32, 2, 32),
                              (32, 2, 32),
                              (32, 2, 32),
                              (256, 2, 32)],
               'lut_total_size': 40960}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999999
Manual and Torch results cosine similarity (Test): 0.99999994
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.153, 0.22
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0049, 0.00274
--- total mse / var(X): 0.111
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00789, 0.00807
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.008, 0.00781
--- total mse / var(X): 0.00794
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00192, 0.00203
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00312, 0.00294
--- total mse / var(X): 0.00248
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00363, 0.00371
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00343, 0.00334
--- total mse / var(X): 0.00353
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00523, 0.00506
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00889, 0.00918
--- total mse / var(X): 0.00712
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0269, 0.0222
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0512, 0.06
--- total mse / var(X): 0.0411
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00104, 0.00102
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00241, 0.00246
--- total mse / var(X): 0.00174
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00932, 0.00884
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.009, 0.00946
--- total mse / var(X): 0.00915
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00379, 0.00284
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00351, 0.00439
--- total mse / var(X): 0.00362
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 4.58e-05, 4.54e-05
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.000139, 0.00014
--- total mse / var(X): 9.29e-05
start table evaluation...
Elapsed time: 107.69911909103394 seconds
Cosine similarity between AMM and exact (Train): 0.72727835
Cosine similarity between AMM and exact (Test): 0.72964305
p,r,f1: 0.6494858848247294 0.7771041109690419 0.7075868365848615
p,r,f1: 0.32214314332935057 0.5582626215661204 0.4085399775544311
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.6494858848247294, 0.7771041109690419, 0.7075868365848615],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64],
               'cossim_layer_train': [0.9935547113418579,
                                      0.9956998229026794,
                                      0.9947658777236938,
                                      0.7272783517837524],
               'cossim_layer_test': [0.9928322434425354,
                                     0.995557427406311,
                                     0.9946909546852112,
                                     0.7296430468559265],
               'cossim_amm_train': [0.9885321259498596,
                                    0.9968128800392151,
                                    0.9935378432273865,
                                    0.9831244945526123,
                                    0.9911244511604309,
                                    0.9351315498352051,
                                    0.987337052822113,
                                    0.8820642232894897],
               'cossim_amm_test': [0.987364649772644,
                                   0.9971989393234253,
                                   0.9939655661582947,
                                   0.9846240282058716,
                                   0.9911443591117859,
                                   0.933971107006073,
                                   0.9875293374061584,
                                   0.882313072681427],
               'f1': [0.32214314332935057,
                      0.5582626215661204,
                      0.4085399775544311],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 64),
                              (192, 2, 64),
                              (64, 64, 2),
                              (64, 64, 2),
                              (32, 2, 64),
                              (32, 2, 64),
                              (32, 2, 64),
                              (256, 2, 64)],
               'lut_total_size': 90112}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000001
Manual and Torch results cosine similarity (Test): 0.99999994
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0997, 0.144
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000837, 0.000467
--- total mse / var(X): 0.0721
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0064, 0.00655
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00655, 0.0064
--- total mse / var(X): 0.00648
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00187, 0.00197
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00225, 0.00212
--- total mse / var(X): 0.00205
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00285, 0.00292
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00257, 0.00251
--- total mse / var(X): 0.00271
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00309, 0.00296
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00525, 0.00548
--- total mse / var(X): 0.00422
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0128, 0.0106
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0127, 0.015
--- total mse / var(X): 0.0128
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0041, 0.00389
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00298, 0.00313
--- total mse / var(X): 0.00351
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0113, 0.0109
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0099, 0.0102
--- total mse / var(X): 0.0106
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00526, 0.00398
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00561, 0.00698
--- total mse / var(X): 0.00548
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00294, 0.00238
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00242, 0.00288
--- total mse / var(X): 0.00263
start table evaluation...
Elapsed time: 162.8422019481659 seconds
Cosine similarity between AMM and exact (Train): 0.7735973
Cosine similarity between AMM and exact (Test): 0.7693518
p,r,f1: 0.6494858848247294 0.7771041109690419 0.7075868365848615
p,r,f1: 0.37072755199776136 0.5584904490681545 0.445638798989388
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.6494858848247294, 0.7771041109690419, 0.7075868365848615],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9954763650894165,
                                      0.9973763227462769,
                                      0.996517539024353,
                                      0.7735974788665771],
               'cossim_layer_test': [0.9941223859786987,
                                     0.9971451163291931,
                                     0.9963867664337158,
                                     0.7693517804145813],
               'cossim_amm_train': [0.9919555187225342,
                                    0.9974570870399475,
                                    0.9952235817909241,
                                    0.9919753074645996,
                                    0.9948382377624512,
                                    0.9685657620429993,
                                    0.9916895031929016,
                                    0.9032127857208252],
               'cossim_amm_test': [0.9896330237388611,
                                   0.9976732730865479,
                                   0.995641827583313,
                                   0.9929438233375549,
                                   0.9949477910995483,
                                   0.9678125381469727,
                                   0.9918568730354309,
                                   0.9008933305740356],
               'f1': [0.37072755199776136,
                      0.5584904490681545,
                      0.445638798989388],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999964
Manual and Torch results cosine similarity (Test): 0.99999994
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.063, 0.0903
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 8.81e-05, 4.99e-05
--- total mse / var(X): 0.0452
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00489, 0.00501
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00463, 0.00452
--- total mse / var(X): 0.00477
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00182, 0.00193
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00234, 0.0022
--- total mse / var(X): 0.00207
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00248, 0.00254
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00259, 0.00253
--- total mse / var(X): 0.00253
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00267, 0.00254
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00385, 0.00403
--- total mse / var(X): 0.00329
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.006, 0.00496
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00922, 0.0108
--- total mse / var(X): 0.00789
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00797, 0.00761
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00581, 0.00607
--- total mse / var(X): 0.00684
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0102, 0.00992
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00924, 0.00949
--- total mse / var(X): 0.0097
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0033, 0.00249
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00262, 0.00326
--- total mse / var(X): 0.00288
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00578, 0.00492
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00347, 0.00399
--- total mse / var(X): 0.00445
start table evaluation...
Elapsed time: 174.0098967552185 seconds
Cosine similarity between AMM and exact (Train): 0.7965148
Cosine similarity between AMM and exact (Test): 0.7918368
p,r,f1: 0.6494858848247294 0.7771041109690419 0.7075868365848615
p,r,f1: 0.395028844439099 0.590490139098764 0.4733762437807082
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.6494858848247294, 0.7771041109690419, 0.7075868365848615],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256],
               'cossim_layer_train': [0.9971052408218384,
                                      0.9981481432914734,
                                      0.9973316788673401,
                                      0.7965149879455566],
               'cossim_layer_test': [0.9956114888191223,
                                     0.9979177713394165,
                                     0.9971502423286438,
                                     0.7918367385864258],
               'cossim_amm_train': [0.9948557019233704,
                                    0.9980908632278442,
                                    0.9962274432182312,
                                    0.9944407343864441,
                                    0.9962545037269592,
                                    0.9758994579315186,
                                    0.992824137210846,
                                    0.9227927923202515],
               'cossim_amm_test': [0.9922624826431274,
                                   0.9980869889259338,
                                   0.996577262878418,
                                   0.99500972032547,
                                   0.9963438510894775,
                                   0.9750922322273254,
                                   0.9925522208213806,
                                   0.9201642274856567],
               'f1': [0.395028844439099, 0.590490139098764, 0.4733762437807082],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 256),
                              (192, 2, 256),
                              (256, 256, 2),
                              (256, 256, 2),
                              (32, 2, 256),
                              (32, 2, 256),
                              (32, 2, 256),
                              (256, 2, 256)],
               'lut_total_size': 557056}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000001
Manual and Torch results cosine similarity (Test): 0.99999994
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.0371, 0.0534
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.000113, 6.32e-05
--- total mse / var(X): 0.0267
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00371, 0.00379
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00321, 0.00313
--- total mse / var(X): 0.00346
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00156, 0.00165
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.0019, 0.0018
--- total mse / var(X): 0.00172
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00206, 0.00211
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00224, 0.00218
--- total mse / var(X): 0.00215
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00192, 0.00182
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00263, 0.00276
--- total mse / var(X): 0.00229
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00414, 0.00342
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00692, 0.00812
--- total mse / var(X): 0.00577
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00859, 0.00814
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00694, 0.0073
--- total mse / var(X): 0.00772
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.0073, 0.00713
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00608, 0.00621
--- total mse / var(X): 0.00667
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00328, 0.0025
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00225, 0.00279
--- total mse / var(X): 0.00264
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00817, 0.00674
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00672, 0.00789
--- total mse / var(X): 0.00732
start table evaluation...
Elapsed time: 286.08894181251526 seconds
Cosine similarity between AMM and exact (Train): 0.8375823
Cosine similarity between AMM and exact (Test): 0.8350864
p,r,f1: 0.6494858848247294 0.7771041109690419 0.7075868365848615
p,r,f1: 0.4344913779635725 0.6250083304273703 0.5126206804502437
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.6494858848247294, 0.7771041109690419, 0.7075868365848615],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512],
               'cossim_layer_train': [0.9982388615608215,
                                      0.9987006187438965,
                                      0.998073399066925,
                                      0.8375823497772217],
               'cossim_layer_test': [0.9967085123062134,
                                     0.9984531998634338,
                                     0.9978481531143188,
                                     0.835086464881897],
               'cossim_amm_train': [0.9968646168708801,
                                    0.9986317753791809,
                                    0.9971203207969666,
                                    0.9960930347442627,
                                    0.9972721934318542,
                                    0.9851861000061035,
                                    0.994856595993042,
                                    0.9427594542503357],
               'cossim_amm_test': [0.9942018985748291,
                                   0.9984524846076965,
                                   0.9970795512199402,
                                   0.9962807297706604,
                                   0.9972729086875916,
                                   0.9850061535835266,
                                   0.9947063326835632,
                                   0.9410176873207092],
               'f1': [0.4344913779635725,
                      0.6250083304273703,
                      0.5126206804502437],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 512),
                              (192, 2, 512),
                              (512, 512, 2),
                              (512, 512, 2),
                              (32, 2, 512),
                              (32, 2, 512),
                              (32, 2, 512),
                              (256, 2, 512)],
               'lut_total_size': 1638400}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999998
Manual and Torch results cosine similarity (Test): 0.99999994
start table training...
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.124, 0.124
--- total mse / var(X): 0.124
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00563, 0.00563
--- total mse / var(X): 0.00563
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00167, 0.00167
--- total mse / var(X): 0.00167
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00204, 0.00204
--- total mse / var(X): 0.00204
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00574, 0.00574
--- total mse / var(X): 0.00574
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0107, 0.0107
--- total mse / var(X): 0.0107
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0036, 0.0036
--- total mse / var(X): 0.0036
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00704, 0.00704
--- total mse / var(X): 0.00704
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00308, 0.00308
--- total mse / var(X): 0.00308
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 7.18e-11, 7.18e-11
--- total mse / var(X): 7.18e-11
start table evaluation...
Elapsed time: 86.19292092323303 seconds
Cosine similarity between AMM and exact (Train): 0.771143
Cosine similarity between AMM and exact (Test): 0.7783934
p,r,f1: 0.6494858848247294 0.7771041109690419 0.7075868365848615
p,r,f1: 0.40765827409186217 0.530937269944593 0.4612017870569633
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.6494858848247294, 0.7771041109690419, 0.7075868365848615],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9913756251335144,
                                      0.9963303804397583,
                                      0.9954569339752197,
                                      0.7711430191993713],
               'cossim_layer_test': [0.9877694249153137,
                                     0.995760440826416,
                                     0.9949716925621033,
                                     0.7783933877944946],
               'cossim_amm_train': [0.9846142530441284,
                                    0.9970248341560364,
                                    0.9938597679138184,
                                    0.9915515184402466,
                                    0.9938444495201111,
                                    0.9541508555412292,
                                    0.9894399642944336,
                                    0.9213464260101318],
               'cossim_amm_test': [0.9783852696418762,
                                   0.9972457885742188,
                                   0.99436354637146,
                                   0.9924200177192688,
                                   0.9938374757766724,
                                   0.9532180428504944,
                                   0.9890625476837158,
                                   0.9246610403060913],
               'f1': [0.40765827409186217,
                      0.530937269944593,
                      0.4612017870569633],
               'lut_num': 8,
               'lut_shapes': [(32, 1, 128),
                              (192, 1, 128),
                              (128, 128, 1),
                              (128, 128, 1),
                              (32, 1, 128),
                              (32, 1, 128),
                              (32, 1, 128),
                              (256, 1, 128)],
               'lut_total_size': 106496}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (32). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0
Manual and Torch results cosine similarity (Test): 0.99999994
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0211, 0.0304
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000182, 0.000102
--- total mse / var(X): 0.0153
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00269, 0.00275
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00226, 0.00221
--- total mse / var(X): 0.00248
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00115, 0.00121
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00147, 0.00138
--- total mse / var(X): 0.0013
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00153, 0.00156
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00162, 0.00158
--- total mse / var(X): 0.00157
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00155, 0.00147
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00201, 0.00211
--- total mse / var(X): 0.00179
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00279, 0.0023
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.005, 0.00587
--- total mse / var(X): 0.00409
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0086, 0.00815
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00696, 0.00732
--- total mse / var(X): 0.00774
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00604, 0.00591
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00513, 0.00524
--- total mse / var(X): 0.00557
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00282, 0.00216
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00201, 0.00248
--- total mse / var(X): 0.00232
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0105, 0.00863
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00863, 0.0102
--- total mse / var(X): 0.00941
start table evaluation...
Elapsed time: 460.83703446388245 seconds
Cosine similarity between AMM and exact (Train): 0.86860853
Cosine similarity between AMM and exact (Test): 0.8717929
p,r,f1: 0.6494858848247294 0.7771041109690419 0.7075868365848615
p,r,f1: 0.4676337016200095 0.6843132240691232 0.5555949131033581
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.6494858848247294, 0.7771041109690419, 0.7075868365848615],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024],
               'cossim_layer_train': [0.9989238977432251,
                                      0.9990423321723938,
                                      0.9985196590423584,
                                      0.8686084747314453],
               'cossim_layer_test': [0.9974369406700134,
                                     0.9988295435905457,
                                     0.9982988834381104,
                                     0.8717929720878601],
               'cossim_amm_train': [0.9980906248092651,
                                    0.9990271329879761,
                                    0.9978659749031067,
                                    0.9971277117729187,
                                    0.9979020357131958,
                                    0.9892740845680237,
                                    0.99603271484375,
                                    0.9536267518997192],
               'cossim_amm_test': [0.995484471321106,
                                   0.9987649321556091,
                                   0.9977666139602661,
                                   0.9971380233764648,
                                   0.9979224801063538,
                                   0.9890916347503662,
                                   0.9958789944648743,
                                   0.9537338614463806],
               'f1': [0.4676337016200095,
                      0.6843132240691232,
                      0.5555949131033581],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 1024),
                              (192, 2, 1024),
                              (1024, 1024, 2),
                              (1024, 1024, 2),
                              (32, 2, 1024),
                              (32, 2, 1024),
                              (32, 2, 1024),
                              (256, 2, 1024)],
               'lut_total_size': 5373952}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999964
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.31, 0.446
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0536, 0.0302
--- total mse / var(X): 0.238
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0148, 0.0152
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.012, 0.0117
--- total mse / var(X): 0.0134
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.217, 0.233
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.117, 0.108
--- total mse / var(X): 0.17
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.128, 0.127
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.146, 0.147
--- total mse / var(X): 0.137
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0311, 0.0178
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0157, 0.0224
--- total mse / var(X): 0.0201
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.169, 0.13
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.232, 0.285
--- total mse / var(X): 0.208
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 1.93e-05, 5.78e-06
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00047, 0.000799
--- total mse / var(X): 0.000402
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0643, 0.0699
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0623, 0.0568
--- total mse / var(X): 0.0634
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00313, 0.00177
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.025, 0.0359
--- total mse / var(X): 0.0188
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 1.47, 1.41
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0869, 0.0904
--- total mse / var(X): 0.752
start table evaluation...
Elapsed time: 69.89440274238586 seconds
Cosine similarity between AMM and exact (Train): 0.5776095
Cosine similarity between AMM and exact (Test): 0.5788657
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.4735574320666883 0.27117672129877174 0.34486870568293243
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16],
               'cossim_layer_train': [0.9875732660293579,
                                      0.9874938130378723,
                                      0.983875036239624,
                                      0.5776095390319824],
               'cossim_layer_test': [0.9863836765289307,
                                     0.9870771765708923,
                                     0.9834534525871277,
                                     0.578865647315979],
               'cossim_amm_train': [0.9775798320770264,
                                    0.9937397241592407,
                                    0.9332008957862854,
                                    0.9316028952598572,
                                    0.9813161492347717,
                                    0.8836132287979126,
                                    0.9485527873039246,
                                    0.8937797546386719],
               'cossim_amm_test': [0.975557804107666,
                                   0.9944851398468018,
                                   0.9335542917251587,
                                   0.9323338866233826,
                                   0.9813670516014099,
                                   0.8837419748306274,
                                   0.9485553503036499,
                                   0.8932503461837769],
               'f1': [0.4735574320666883,
                      0.27117672129877174,
                      0.34486870568293243],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 16),
                              (192, 2, 16),
                              (16, 16, 2),
                              (16, 16, 2),
                              (32, 2, 16),
                              (32, 2, 16),
                              (32, 2, 16),
                              (256, 2, 16)],
               'lut_total_size': 19456}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (6). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000002
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.218, 0.314
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0139, 0.00776
--- total mse / var(X): 0.161
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0079, 0.0081
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00947, 0.00923
--- total mse / var(X): 0.00866
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0234, 0.0251
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0171, 0.0159
--- total mse / var(X): 0.0205
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.044, 0.0436
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0374, 0.0377
--- total mse / var(X): 0.0407
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0124, 0.00473
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00385, 0.00623
--- total mse / var(X): 0.00548
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.0841, 0.0645
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.128, 0.157
--- total mse / var(X): 0.111
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00259, 0.000852
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.000438, 0.000732
--- total mse / var(X): 0.000792
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00914, 0.00906
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00737, 0.00744
--- total mse / var(X): 0.00825
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00379, 0.00221
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 0.00139, 0.00196
--- total mse / var(X): 0.00209
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 4.14e-05, 5.01e-05
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 32, 6
mse / {var(X_subs), var(X)}: 2.06e-05, 1.63e-05
--- total mse / var(X): 3.32e-05
start table evaluation...
Elapsed time: 71.61960506439209 seconds
Cosine similarity between AMM and exact (Train): 0.6132514
Cosine similarity between AMM and exact (Test): 0.6130336
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.3976484731201712 0.37834243868417994 0.38775529653949886
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32,
                             32],
               'cossim_layer_train': [0.9902910590171814,
                                      0.9932231903076172,
                                      0.9932984709739685,
                                      0.6132513284683228],
               'cossim_layer_test': [0.9897655844688416,
                                     0.9931018948554993,
                                     0.9931914806365967,
                                     0.6130335927009583],
               'cossim_amm_train': [0.9824998378753662,
                                    0.9965243935585022,
                                    0.9767887592315674,
                                    0.963599681854248,
                                    0.9913296699523926,
                                    0.9214041233062744,
                                    0.9792671203613281,
                                    0.8949563503265381],
               'cossim_amm_test': [0.9815345406532288,
                                   0.9971693158149719,
                                   0.9774357676506042,
                                   0.9646643400192261,
                                   0.9913536906242371,
                                   0.9204085469245911,
                                   0.979308545589447,
                                   0.8941923975944519],
               'f1': [0.3976484731201712,
                      0.37834243868417994,
                      0.38775529653949886],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 32),
                              (192, 2, 32),
                              (32, 32, 2),
                              (32, 32, 2),
                              (32, 2, 32),
                              (32, 2, 32),
                              (32, 2, 32),
                              (256, 2, 32)],
               'lut_total_size': 40960}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999994
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.159, 0.229
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00479, 0.00269
--- total mse / var(X): 0.116
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00713, 0.00731
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00718, 0.007
--- total mse / var(X): 0.00716
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00202, 0.00217
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00265, 0.00245
--- total mse / var(X): 0.00231
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00447, 0.00443
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00278, 0.0028
--- total mse / var(X): 0.00361
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0158, 0.00638
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0072, 0.0115
--- total mse / var(X): 0.00894
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.034, 0.026
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0409, 0.0504
--- total mse / var(X): 0.0382
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00839, 0.00295
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0025, 0.00411
--- total mse / var(X): 0.00353
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0102, 0.00962
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00816, 0.00865
--- total mse / var(X): 0.00913
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0027, 0.00176
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00287, 0.00387
--- total mse / var(X): 0.00281
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.000597, 0.000446
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.000396, 0.000496
--- total mse / var(X): 0.000471
start table evaluation...
Elapsed time: 91.30909752845764 seconds
Cosine similarity between AMM and exact (Train): 0.5919894
Cosine similarity between AMM and exact (Test): 0.59690416
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.29319811980426863 0.6128125847572552 0.3966299663758998
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64],
               'cossim_layer_train': [0.9928405284881592,
                                      0.9961068034172058,
                                      0.9953845143318176,
                                      0.5919893980026245],
               'cossim_layer_test': [0.9924003481864929,
                                     0.9960025548934937,
                                     0.9953427314758301,
                                     0.5969042181968689],
               'cossim_amm_train': [0.9870964288711548,
                                    0.9971357583999634,
                                    0.9924576282501221,
                                    0.9838634729385376,
                                    0.994744598865509,
                                    0.9523658156394958,
                                    0.9846212267875671,
                                    0.8051377534866333],
               'cossim_amm_test': [0.9862964153289795,
                                   0.9975379705429077,
                                   0.9929926991462708,
                                   0.9844653606414795,
                                   0.9947194457054138,
                                   0.9525458812713623,
                                   0.984921395778656,
                                   0.8054195046424866],
               'f1': [0.29319811980426863,
                      0.6128125847572552,
                      0.3966299663758998],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 64),
                              (192, 2, 64),
                              (64, 64, 2),
                              (64, 64, 2),
                              (32, 2, 64),
                              (32, 2, 64),
                              (32, 2, 64),
                              (256, 2, 64)],
               'lut_total_size': 90112}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000004
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.099, 0.142
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000807, 0.000455
--- total mse / var(X): 0.0713
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00572, 0.00586
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0056, 0.00546
--- total mse / var(X): 0.00566
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00198, 0.00212
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00221, 0.00205
--- total mse / var(X): 0.00209
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00227, 0.00226
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.002, 0.00202
--- total mse / var(X): 0.00214
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0111, 0.00439
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00499, 0.00801
--- total mse / var(X): 0.0062
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00807, 0.00618
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0107, 0.0132
--- total mse / var(X): 0.0097
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.012, 0.00449
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00641, 0.0104
--- total mse / var(X): 0.00746
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0116, 0.0113
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00993, 0.0102
--- total mse / var(X): 0.0107
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00536, 0.00321
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00302, 0.00423
--- total mse / var(X): 0.00372
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00129, 0.000984
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000623, 0.00077
--- total mse / var(X): 0.000877
start table evaluation...
Elapsed time: 109.7908284664154 seconds
Cosine similarity between AMM and exact (Train): 0.5988091
Cosine similarity between AMM and exact (Test): 0.6019082
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.37892770428134404 0.4615010267736061 0.4161578921255887
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9949589967727661,
                                      0.9976250529289246,
                                      0.996631920337677,
                                      0.5988091230392456],
               'cossim_layer_test': [0.9935961961746216,
                                     0.9974846839904785,
                                     0.9965250492095947,
                                     0.601908266544342],
               'cossim_amm_train': [0.9909312129020691,
                                    0.9977379441261292,
                                    0.9950193762779236,
                                    0.9931632280349731,
                                    0.9968830943107605,
                                    0.9711732864379883,
                                    0.9869337677955627,
                                    0.8727630376815796],
               'cossim_amm_test': [0.9884644746780396,
                                   0.9979252815246582,
                                   0.9954376220703125,
                                   0.9939900040626526,
                                   0.9969183802604675,
                                   0.9710255265235901,
                                   0.9871726036071777,
                                   0.8736341595649719],
               'f1': [0.37892770428134404,
                      0.4615010267736061,
                      0.4161578921255887],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999964
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0636, 0.0914
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.000191, 0.000108
--- total mse / var(X): 0.0457
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00467, 0.00479
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00419, 0.00408
--- total mse / var(X): 0.00444
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00178, 0.00191
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00202, 0.00187
--- total mse / var(X): 0.00189
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00212, 0.0021
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00194, 0.00196
--- total mse / var(X): 0.00203
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00735, 0.00293
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00304, 0.00487
--- total mse / var(X): 0.0039
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00554, 0.00425
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00772, 0.00952
--- total mse / var(X): 0.00689
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0214, 0.00819
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00769, 0.0124
--- total mse / var(X): 0.0103
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00979, 0.00959
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00921, 0.00939
--- total mse / var(X): 0.00949
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00388, 0.0023
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00272, 0.00382
--- total mse / var(X): 0.00306
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00411, 0.00313
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0045, 0.00557
--- total mse / var(X): 0.00435
start table evaluation...
Elapsed time: 190.55865859985352 seconds
Cosine similarity between AMM and exact (Train): 0.65167385
Cosine similarity between AMM and exact (Test): 0.6545888
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.3678489411028253 0.626589174319036 0.4635585879207221
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256],
               'cossim_layer_train': [0.997008740901947,
                                      0.9983939528465271,
                                      0.9977041482925415,
                                      0.6516739130020142],
               'cossim_layer_test': [0.9960432648658752,
                                     0.9982847571372986,
                                     0.9976058006286621,
                                     0.6545886993408203],
               'cossim_amm_train': [0.9946223497390747,
                                    0.9983022212982178,
                                    0.9960147738456726,
                                    0.9951891899108887,
                                    0.9978194236755371,
                                    0.9835749268531799,
                                    0.9906817674636841,
                                    0.8769245147705078],
               'cossim_amm_test': [0.9928695559501648,
                                   0.9983192086219788,
                                   0.9963721036911011,
                                   0.9956283569335938,
                                   0.9978488087654114,
                                   0.9834607243537903,
                                   0.9908016920089722,
                                   0.8768405914306641],
               'f1': [0.3678489411028253,
                      0.626589174319036,
                      0.4635585879207221],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 256),
                              (192, 2, 256),
                              (256, 256, 2),
                              (256, 256, 2),
                              (32, 2, 256),
                              (32, 2, 256),
                              (32, 2, 256),
                              (256, 2, 256)],
               'lut_total_size': 557056}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.0367, 0.0528
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.000108, 6.1e-05
--- total mse / var(X): 0.0264
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00352, 0.00361
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00317, 0.00309
--- total mse / var(X): 0.00335
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00154, 0.00165
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00177, 0.00164
--- total mse / var(X): 0.00165
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00188, 0.00186
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.0016, 0.00162
--- total mse / var(X): 0.00174
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00541, 0.00215
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00219, 0.00351
--- total mse / var(X): 0.00283
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00371, 0.00284
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00566, 0.00698
--- total mse / var(X): 0.00491
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.0222, 0.00862
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.0088, 0.0142
--- total mse / var(X): 0.0114
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00812, 0.00799
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00637, 0.00647
--- total mse / var(X): 0.00723
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00382, 0.00226
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00245, 0.00346
--- total mse / var(X): 0.00286
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00933, 0.00818
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 512, 23
mse / {var(X_subs), var(X)}: 0.00552, 0.0062
--- total mse / var(X): 0.00719
start table evaluation...
Elapsed time: 469.21745347976685 seconds
Cosine similarity between AMM and exact (Train): 0.7409728
Cosine similarity between AMM and exact (Test): 0.7409029
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.4463816817355224 0.6752714169475764 0.5374723986924074
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512,
                             512],
               'cossim_layer_train': [0.9981304407119751,
                                      0.998894453048706,
                                      0.9983323216438293,
                                      0.7409727573394775],
               'cossim_layer_test': [0.9966337084770203,
                                     0.9987605810165405,
                                     0.99822598695755,
                                     0.7409029006958008],
               'cossim_amm_train': [0.9966428875923157,
                                    0.9987516403198242,
                                    0.9969870448112488,
                                    0.9965595006942749,
                                    0.9984681606292725,
                                    0.9896709322929382,
                                    0.993200957775116,
                                    0.9114171266555786],
               'cossim_amm_test': [0.9939382076263428,
                                   0.998624861240387,
                                   0.9971436858177185,
                                   0.9967976808547974,
                                   0.9985193610191345,
                                   0.9896532297134399,
                                   0.9933849573135376,
                                   0.9101301431655884],
               'f1': [0.4463816817355224,
                      0.6752714169475764,
                      0.5374723986924074],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 512),
                              (192, 2, 512),
                              (512, 512, 2),
                              (512, 512, 2),
                              (32, 2, 512),
                              (32, 2, 512),
                              (32, 2, 512),
                              (256, 2, 512)],
               'lut_total_size': 1638400}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000001
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.126, 0.126
--- total mse / var(X): 0.126
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00553, 0.00553
--- total mse / var(X): 0.00553
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00139, 0.00139
--- total mse / var(X): 0.00139
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00138, 0.00138
--- total mse / var(X): 0.00138
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00941, 0.00941
--- total mse / var(X): 0.00941
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0114, 0.0114
--- total mse / var(X): 0.0114
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00415, 0.00415
--- total mse / var(X): 0.00415
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00619, 0.00619
--- total mse / var(X): 0.00619
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00239, 0.00239
--- total mse / var(X): 0.00239
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 2.67e-11, 2.67e-11
--- total mse / var(X): 2.67e-11
start table evaluation...
Elapsed time: 138.76226162910461 seconds
Cosine similarity between AMM and exact (Train): 0.57205
Cosine similarity between AMM and exact (Test): 0.57745683
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.3094767454858446 0.5149010035259018 0.38659434224084316
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9902336597442627,
                                      0.9965999126434326,
                                      0.9959441423416138,
                                      0.5720499753952026],
               'cossim_layer_test': [0.9867123365402222,
                                     0.9963148236274719,
                                     0.9957249164581299,
                                     0.5774569511413574],
               'cossim_amm_train': [0.9823788404464722,
                                    0.9969587922096252,
                                    0.9925236701965332,
                                    0.9918044209480286,
                                    0.9960869550704956,
                                    0.9659322500228882,
                                    0.9870731234550476,
                                    0.8507104516029358],
               'cossim_amm_test': [0.9759840369224548,
                                   0.9972376227378845,
                                   0.9933895468711853,
                                   0.9927376508712769,
                                   0.9961833357810974,
                                   0.9673823118209839,
                                   0.9874781966209412,
                                   0.8527644872665405],
               'f1': [0.3094767454858446,
                      0.5149010035259018,
                      0.38659434224084316],
               'lut_num': 8,
               'lut_shapes': [(32, 1, 128),
                              (192, 1, 128),
                              (128, 128, 1),
                              (128, 128, 1),
                              (32, 1, 128),
                              (32, 1, 128),
                              (32, 1, 128),
                              (256, 1, 128)],
               'lut_total_size': 106496}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999998
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 2.5e-07, 2.83e-07
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 2.44e-05, 2.92e-05
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 1.96e-05, 2.01e-05
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 1.18e-05, 1.36e-05
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 8.69e-06, 1.08e-05
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 5.05e-06, 6.26e-06
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 9.34e-06, 9.38e-06
running kmeans in subspace 8/8... X.shape:  (100000, 2)
k:  128
nnz_rows:  0
mse / {var(X_subs), var(X)}: 0, 0
--- total mse / var(X): 1.12e-05
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00297, 0.00404
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00508, 0.00431
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.005, 0.00507
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00559, 0.0049
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00324, 0.00323
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0045, 0.00418
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00377, 0.00438
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00314, 0.00254
--- total mse / var(X): 0.00408
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00254, 0.00308
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0027, 0.00281
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00284, 0.00266
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00228, 0.00251
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00287, 0.00337
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00275, 0.00284
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00337, 0.00284
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00368, 0.00244
--- total mse / var(X): 0.00282
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00319, 0.00363
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00368, 0.00293
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00366, 0.00381
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00373, 0.00371
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00283, 0.00346
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00407, 0.0042
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00288, 0.00299
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00333, 0.00246
--- total mse / var(X): 0.0034
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000233, 3.98e-05
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000685, 0.000293
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000334, 0.000182
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000254, 5.86e-05
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000112, 2.33e-05
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00266, 0.00179
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00236, 0.00203
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000714, 0.00349
--- total mse / var(X): 0.000988
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 1.16e-07, 6.51e-08
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000249, 0.000184
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000298, 0.000142
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000239, 0.000176
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000252, 0.000139
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00385, 0.00704
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0043, 0.00689
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00491, 0.00739
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999997
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00567, 0.00662
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00125, 0.00137
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00103, 0.00179
running kmeans in subspace 4/4... X.shape:  (100000, 3)
k:  128
nnz_rows:  0
mse / {var(X_subs), var(X)}: 0, 0
--- total mse / var(X): 0.00245
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00492, 0.00544
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00779, 0.00737
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00582, 0.00561
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00538, 0.00529
--- total mse / var(X): 0.00593
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00259, 0.00292
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00245, 0.0025
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00252, 0.00278
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00307, 0.00231
--- total mse / var(X): 0.00263
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00325, 0.00315
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00324, 0.00329
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0027, 0.00304
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00285, 0.00252
--- total mse / var(X): 0.003
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00449, 0.00257
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00375, 0.000822
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00481, 0.00282
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00141, 0.00369
--- total mse / var(X): 0.00248
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00502, 0.00446
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00393, 0.00253
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00678, 0.00908
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00863, 0.00974
--- total mse / var(X): 0.00645
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0337, 0.00997
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0191, 0.00892
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0121, 0.0199
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00789, 0.0126
--- total mse / var(X): 0.0128
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.013, 0.0126
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.015, 0.0151
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0114, 0.0118
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0117, 0.0116
--- total mse / var(X): 0.0128
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00942, 0.00873
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0105, 0.00259
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00383, 0.00613
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00467, 0.00573
--- total mse / var(X): 0.00579
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0181, 0.0177
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0186, 0.0129
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.013, 0.0123
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0102, 0.0141
--- total mse / var(X): 0.0142
start table evaluation...
Elapsed time: 254.0340440273285 seconds
Cosine similarity between AMM and exact (Train): 0.6326042
Cosine similarity between AMM and exact (Test): 0.635922
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.35376188784068285 0.5739734201247627 0.437732441420182
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9998159408569336,
                                      0.9989439845085144,
                                      0.9981161952018738,
                                      0.6326041221618652],
               'cossim_layer_test': [0.9987613558769226,
                                     0.998866617679596,
                                     0.9980791211128235,
                                     0.6359220743179321],
               'cossim_amm_train': [0.9996704459190369,
                                    0.998283326625824,
                                    0.9960626363754272,
                                    0.9952837824821472,
                                    0.9982252717018127,
                                    0.9868243932723999,
                                    0.9909514784812927,
                                    0.87275230884552],
               'cossim_amm_test': [0.9977715015411377,
                                   0.9984821081161499,
                                   0.9965454936027527,
                                   0.9959861636161804,
                                   0.9982936382293701,
                                   0.9872375726699829,
                                   0.9912694096565247,
                                   0.8724690675735474],
               'f1': [0.35376188784068285,
                      0.5739734201247627,
                      0.437732441420182],
               'lut_num': 8,
               'lut_shapes': [(32, 4, 128),
                              (192, 4, 128),
                              (128, 128, 4),
                              (128, 128, 4),
                              (32, 4, 128),
                              (32, 4, 128),
                              (32, 4, 128),
                              (256, 4, 128)],
               'lut_total_size': 425984}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (32). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999998
Manual and Torch results cosine similarity (Test): 1.0000006
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0212, 0.0305
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000125, 6.99e-05
--- total mse / var(X): 0.0153
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00255, 0.00261
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00218, 0.00212
--- total mse / var(X): 0.00237
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00113, 0.00121
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00131, 0.00121
--- total mse / var(X): 0.00121
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00137, 0.00136
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00129, 0.0013
--- total mse / var(X): 0.00133
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00424, 0.00169
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00181, 0.0029
--- total mse / var(X): 0.00229
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00246, 0.00189
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00389, 0.0048
--- total mse / var(X): 0.00335
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0209, 0.00817
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00859, 0.0138
--- total mse / var(X): 0.011
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00665, 0.0066
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00533, 0.00537
--- total mse / var(X): 0.00598
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00331, 0.00196
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.002, 0.00282
--- total mse / var(X): 0.00239
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00731, 0.00657
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00656, 0.00722
--- total mse / var(X): 0.00689
start table evaluation...
Elapsed time: 641.0124402046204 seconds
Cosine similarity between AMM and exact (Train): 0.793405
Cosine similarity between AMM and exact (Test): 0.7924479
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.5038614620298084 0.6876872408849626 0.5815945210866075
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024],
               'cossim_layer_train': [0.99882972240448,
                                      0.9991722106933594,
                                      0.9987541437149048,
                                      0.7934048175811768],
               'cossim_layer_test': [0.9973370432853699,
                                     0.9990250468254089,
                                     0.9986070990562439,
                                     0.7924480438232422],
               'cossim_amm_train': [0.9978984594345093,
                                    0.9991280436515808,
                                    0.9978882670402527,
                                    0.9974878430366516,
                                    0.9988163709640503,
                                    0.992568850517273,
                                    0.9950289726257324,
                                    0.9311937093734741],
               'cossim_amm_test': [0.9952049255371094,
                                   0.9988711476325989,
                                   0.9978122711181641,
                                   0.9975317120552063,
                                   0.9988436102867126,
                                   0.9924414753913879,
                                   0.9949604272842407,
                                   0.9308276772499084],
               'f1': [0.5038614620298084,
                      0.6876872408849626,
                      0.5815945210866075],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 1024),
                              (192, 2, 1024),
                              (1024, 1024, 2),
                              (1024, 1024, 2),
                              (32, 2, 1024),
                              (32, 2, 1024),
                              (32, 2, 1024),
                              (256, 2, 1024)],
               'lut_total_size': 5373952}}
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
--- total mse / var(X): 0.00275
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0908, 0.0134
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0316, 0.0148
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0274, 0.0135
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0256, 0.0116
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.027, 0.0247
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0102, 0.0242
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00983, 0.0219
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0213, 0.0199
--- total mse / var(X): 0.018
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00679, 0.00756
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0127, 0.0104
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0108, 0.0104
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0125, 0.0134
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00853, 0.00742
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00924, 0.0109
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0104, 0.0106
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00887, 0.00857
--- total mse / var(X): 0.0099
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00626, 0.0054
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00534, 0.00528
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0122, 0.00518
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00953, 0.000846
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00329, 0.00454
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00342, 0.00616
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.006, 0.00518
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00298, 0.00476
--- total mse / var(X): 0.00467
running kmeans in subspace 1/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0597, 0.0565
running kmeans in subspace 2/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.06, 0.06
running kmeans in subspace 3/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.058, 0.0355
running kmeans in subspace 4/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0576, 0.0536
running kmeans in subspace 5/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0415, 0.052
running kmeans in subspace 6/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0562, 0.045
running kmeans in subspace 7/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.031, 0.0526
running kmeans in subspace 8/8... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0731, 0.0557
--- total mse / var(X): 0.0513
start table evaluation...
Elapsed time: 231.05119824409485 seconds
Cosine similarity between AMM and exact (Train): 0.69751847
Cosine similarity between AMM and exact (Test): 0.69672805
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.42337748130062614 0.5880289821380139 0.49230104490916576
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9999994039535522,
                                      0.9993654489517212,
                                      0.9987739324569702,
                                      0.6975184679031372],
               'cossim_layer_test': [0.9999880790710449,
                                     0.999409556388855,
                                     0.9988358616828918,
                                     0.6967280507087708],
               'cossim_amm_train': [0.9999994039535522,
                                    0.9988516569137573,
                                    0.9972536563873291,
                                    0.997507631778717,
                                    0.9989151954650879,
                                    0.9927313923835754,
                                    0.9940423965454102,
                                    0.8996621966362],
               'cossim_amm_test': [0.99997878074646,
                                   0.9990273714065552,
                                   0.9976902604103088,
                                   0.9979368448257446,
                                   0.9990001320838928,
                                   0.9928537607192993,
                                   0.9942094087600708,
                                   0.8990659117698669],
               'f1': [0.42337748130062614,
                      0.5880289821380139,
                      0.49230104490916576],
               'lut_num': 8,
               'lut_shapes': [(32, 8, 128),
                              (192, 8, 128),
                              (128, 128, 8),
                              (128, 128, 8),
                              (32, 8, 128),
                              (32, 8, 128),
                              (32, 8, 128),
                              (256, 8, 128)],
               'lut_total_size': 851968}}
Traceback (most recent call last):
  File "src/3_2_vit_finetune.py", line 3, in <module>
    import vq_amm
  File "/data/neelesh/DART_by_app/433/src/vq_amm.py", line 3, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999994
Manual and Torch results cosine similarity (Test): 1.0000005
start table training with fine tuning...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.101, 0.145
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000949, 0.000535
--- total mse / var(X): 0.0728
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00628, 0.00644
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00581, 0.00566
--- total mse / var(X): 0.00605
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00188, 0.00201
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00211, 0.00196
--- total mse / var(X): 0.00199
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00218, 0.00216
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00213, 0.00215
--- total mse / var(X): 0.00215
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0105, 0.00416
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00544, 0.00872
--- total mse / var(X): 0.00644
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00989, 0.00758
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0118, 0.0145
--- total mse / var(X): 0.0111
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0126, 0.00464
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00445, 0.00727
--- total mse / var(X): 0.00595
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0148, 0.0144
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00942, 0.00969
--- total mse / var(X): 0.012
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00355, 0.00215
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00335, 0.00467
--- total mse / var(X): 0.00341
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00379, 0.00283
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00118, 0.00148
--- total mse / var(X): 0.00215
start table evaluation...
Elapsed time: 127.75547742843628 seconds
Cosine similarity between AMM and exact (Train): 0.5689926
Cosine similarity between AMM and exact (Test): 0.56511503
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.3479119232038584 0.4525661590917897 0.39339781130560486
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9953356385231018,
                                      0.9976787567138672,
                                      0.9968869090080261,
                                      0.5689926147460938],
               'cossim_layer_test': [0.9939442276954651,
                                     0.997530996799469,
                                     0.9967758059501648,
                                     0.5651150345802307],
               'cossim_amm_train': [0.9916138052940369,
                                    0.9977182745933533,
                                    0.9950300455093384,
                                    0.9930614233016968,
                                    0.9968973994255066,
                                    0.9731460213661194,
                                    0.9881507754325867,
                                    0.8704513907432556],
               'cossim_amm_test': [0.9890914559364319,
                                   0.9978845119476318,
                                   0.9957029223442078,
                                   0.9938691258430481,
                                   0.9969257116317749,
                                   0.9731071591377258,
                                   0.9883441925048828,
                                   0.868420422077179],
               'f1': [0.3479119232038584,
                      0.4525661590917897,
                      0.39339781130560486],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Traceback (most recent call last):
  File "src/3_2_vit_finetune.py", line 110, in <module>
    train_data, train_target, test_data, test_target, all_params, best_threshold = load_data_n_model(model_save_path, res_path)
  File "src/3_2_vit_finetune.py", line 55, in load_data_n_model
    with open(model_save_path+'.tensor_dict.pkl', 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'model/433.milc-s0.vit.stu.75.1.pkl.tensor_dict.pkl'
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999994
Manual and Torch results cosine similarity (Test): 1.0000005
start table training with fine tuning...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0989, 0.142
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00101, 0.000569
--- total mse / var(X): 0.0713
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00618, 0.00634
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00594, 0.00579
--- total mse / var(X): 0.00606
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.002, 0.00214
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00251, 0.00233
--- total mse / var(X): 0.00223
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00218, 0.00216
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00206, 0.00207
--- total mse / var(X): 0.00212
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00968, 0.00384
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00463, 0.00743
--- total mse / var(X): 0.00563
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00825, 0.00632
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0097, 0.012
--- total mse / var(X): 0.00915
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0114, 0.00429
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00442, 0.00717
--- total mse / var(X): 0.00573
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0149, 0.0145
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00816, 0.00839
--- total mse / var(X): 0.0114
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0031, 0.00186
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00423, 0.00592
--- total mse / var(X): 0.00389
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00146, 0.00105
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000908, 0.00116
--- total mse / var(X): 0.00111
start table evaluation...
Elapsed time: 110.3325138092041 seconds
Cosine similarity between AMM and exact (Train): 0.5908269
Cosine similarity between AMM and exact (Test): 0.59149975
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.3081548306356263 0.5525901817195552 0.3956649911749545
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.995488703250885,
                                      0.9976725578308105,
                                      0.9968822002410889,
                                      0.5908268690109253],
               'cossim_layer_test': [0.9939030408859253,
                                     0.9974902868270874,
                                     0.9967454671859741,
                                     0.5914998054504395],
               'cossim_amm_train': [0.9918879866600037,
                                    0.9977303147315979,
                                    0.9950242042541504,
                                    0.9932576417922974,
                                    0.9968577027320862,
                                    0.9750874638557434,
                                    0.9882279634475708,
                                    0.8557791709899902],
               'cossim_amm_test': [0.989010751247406,
                                   0.9978495836257935,
                                   0.995333731174469,
                                   0.9940702319145203,
                                   0.9968757629394531,
                                   0.9750345945358276,
                                   0.9884452223777771,
                                   0.8553352355957031],
               'f1': [0.3081548306356263,
                      0.5525901817195552,
                      0.3956649911749545],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0
Manual and Torch results cosine similarity (Test): 1.0000005
start table training with fine tuning...
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
/data/neelesh/DART_by_app/433/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
Retrain for 1 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.102, 0.147
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000916, 0.000515
--- total mse / var(X): 0.0737
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<03:17,  2.00s/it]  2%|▏         | 2/100 [00:03<03:09,  1.93s/it]  3%|▎         | 3/100 [00:05<03:12,  1.98s/it]  4%|▍         | 4/100 [00:07<03:03,  1.91s/it]  5%|▌         | 5/100 [00:09<02:56,  1.86s/it]  6%|▌         | 6/100 [00:11<02:57,  1.89s/it]  7%|▋         | 7/100 [00:13<02:55,  1.89s/it]  8%|▊         | 8/100 [00:15<02:51,  1.86s/it]  9%|▉         | 9/100 [00:16<02:49,  1.86s/it] 10%|█         | 10/100 [00:19<02:57,  1.98s/it] 11%|█         | 11/100 [00:21<03:07,  2.11s/it] 12%|█▏        | 12/100 [00:24<03:17,  2.25s/it] 13%|█▎        | 13/100 [00:26<03:10,  2.19s/it] 14%|█▍        | 14/100 [00:28<02:58,  2.08s/it] 15%|█▌        | 15/100 [00:29<02:50,  2.01s/it] 16%|█▌        | 16/100 [00:31<02:44,  1.96s/it] 17%|█▋        | 17/100 [00:33<02:37,  1.90s/it] 18%|█▊        | 18/100 [00:35<02:33,  1.87s/it] 19%|█▉        | 19/100 [00:37<02:30,  1.86s/it] 20%|██        | 20/100 [00:39<02:46,  2.08s/it] 21%|██        | 21/100 [00:42<02:51,  2.17s/it] 22%|██▏       | 22/100 [00:44<02:57,  2.27s/it] 23%|██▎       | 23/100 [00:46<02:49,  2.21s/it] 24%|██▍       | 24/100 [00:48<02:40,  2.11s/it] 25%|██▌       | 25/100 [00:50<02:36,  2.09s/it] 26%|██▌       | 26/100 [00:52<02:28,  2.00s/it] 27%|██▋       | 27/100 [00:54<02:24,  1.98s/it] 28%|██▊       | 28/100 [00:56<02:18,  1.93s/it] 29%|██▉       | 29/100 [00:58<02:25,  2.05s/it] 30%|███       | 30/100 [01:00<02:17,  1.97s/it] 31%|███       | 31/100 [01:02<02:12,  1.92s/it] 32%|███▏      | 32/100 [01:03<02:04,  1.83s/it] 33%|███▎      | 33/100 [01:05<01:58,  1.77s/it] 34%|███▍      | 34/100 [01:07<01:55,  1.75s/it] 35%|███▌      | 35/100 [01:08<01:52,  1.73s/it] 36%|███▌      | 36/100 [01:10<01:48,  1.69s/it] 37%|███▋      | 37/100 [01:12<01:49,  1.73s/it] 38%|███▊      | 38/100 [01:13<01:49,  1.77s/it] 39%|███▉      | 39/100 [01:16<02:05,  2.05s/it] 40%|████      | 40/100 [01:18<02:05,  2.09s/it] 41%|████      | 41/100 [01:20<01:59,  2.02s/it] 42%|████▏     | 42/100 [01:22<01:57,  2.02s/it] 43%|████▎     | 43/100 [01:24<01:58,  2.07s/it] 44%|████▍     | 44/100 [01:27<01:56,  2.08s/it] 45%|████▌     | 45/100 [01:28<01:50,  2.02s/it] 46%|████▌     | 46/100 [01:30<01:47,  1.99s/it] 47%|████▋     | 47/100 [01:32<01:41,  1.91s/it] 48%|████▊     | 48/100 [01:34<01:45,  2.04s/it] 49%|████▉     | 49/100 [01:36<01:39,  1.96s/it] 50%|█████     | 50/100 [01:38<01:34,  1.88s/it] 51%|█████     | 51/100 [01:40<01:29,  1.83s/it] 52%|█████▏    | 52/100 [01:41<01:25,  1.78s/it] 53%|█████▎    | 53/100 [01:44<01:32,  1.96s/it] 54%|█████▍    | 54/100 [01:46<01:31,  1.99s/it] 55%|█████▌    | 55/100 [01:48<01:33,  2.09s/it] 56%|█████▌    | 56/100 [01:50<01:34,  2.15s/it] 57%|█████▋    | 57/100 [01:53<01:37,  2.26s/it] 58%|█████▊    | 58/100 [01:56<01:41,  2.41s/it] 59%|█████▉    | 59/100 [01:58<01:37,  2.37s/it] 60%|██████    | 60/100 [02:00<01:35,  2.38s/it] 61%|██████    | 61/100 [02:03<01:32,  2.37s/it] 62%|██████▏   | 62/100 [02:05<01:30,  2.38s/it] 63%|██████▎   | 63/100 [02:08<01:31,  2.47s/it] 64%|██████▍   | 64/100 [02:10<01:30,  2.52s/it] 65%|██████▌   | 65/100 [02:13<01:25,  2.44s/it] 66%|██████▌   | 66/100 [02:15<01:22,  2.44s/it] 67%|██████▋   | 67/100 [02:17<01:18,  2.38s/it] 68%|██████▊   | 68/100 [02:19<01:14,  2.32s/it] 69%|██████▉   | 69/100 [02:22<01:10,  2.29s/it] 70%|███████   | 70/100 [02:25<01:14,  2.49s/it] 71%|███████   | 71/100 [02:27<01:11,  2.46s/it] 72%|███████▏  | 72/100 [02:30<01:13,  2.63s/it] 73%|███████▎  | 73/100 [02:33<01:11,  2.63s/it] 74%|███████▍  | 74/100 [02:35<01:09,  2.67s/it] 75%|███████▌  | 75/100 [02:38<01:09,  2.77s/it] 76%|███████▌  | 76/100 [02:42<01:09,  2.90s/it] 77%|███████▋  | 77/100 [02:45<01:06,  2.90s/it] 78%|███████▊  | 78/100 [02:48<01:05,  2.99s/it] 79%|███████▉  | 79/100 [02:50<01:00,  2.87s/it] 80%|████████  | 80/100 [02:53<00:55,  2.77s/it] 81%|████████  | 81/100 [02:56<00:54,  2.87s/it] 82%|████████▏ | 82/100 [02:59<00:51,  2.85s/it] 83%|████████▎ | 83/100 [03:01<00:47,  2.80s/it] 84%|████████▍ | 84/100 [03:04<00:43,  2.74s/it] 85%|████████▌ | 85/100 [03:07<00:39,  2.65s/it] 86%|████████▌ | 86/100 [03:09<00:36,  2.62s/it] 87%|████████▋ | 87/100 [03:11<00:33,  2.54s/it] 88%|████████▊ | 88/100 [03:14<00:29,  2.50s/it] 89%|████████▉ | 89/100 [03:16<00:27,  2.46s/it] 90%|█████████ | 90/100 [03:19<00:25,  2.50s/it] 91%|█████████ | 91/100 [03:21<00:22,  2.50s/it] 92%|█████████▏| 92/100 [03:24<00:19,  2.49s/it] 93%|█████████▎| 93/100 [03:26<00:17,  2.53s/it] 94%|█████████▍| 94/100 [03:29<00:15,  2.64s/it] 95%|█████████▌| 95/100 [03:32<00:13,  2.69s/it] 96%|█████████▌| 96/100 [03:35<00:10,  2.64s/it] 97%|█████████▋| 97/100 [03:37<00:08,  2.68s/it] 98%|█████████▊| 98/100 [03:40<00:05,  2.73s/it] 99%|█████████▉| 99/100 [03:43<00:02,  2.79s/it]100%|██████████| 100/100 [03:48<00:00,  3.30s/it]100%|██████████| 100/100 [03:48<00:00,  2.28s/it]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00605, 0.0062
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00555, 0.00541
--- total mse / var(X): 0.00581
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00174, 0.00186
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00212, 0.00197
--- total mse / var(X): 0.00192
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00219, 0.00217
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00192, 0.00194
--- total mse / var(X): 0.00205
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00948, 0.00376
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00528, 0.00847
--- total mse / var(X): 0.00612
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00835, 0.0064
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0105, 0.013
--- total mse / var(X): 0.00969
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:11,  8.74it/s]  2%|▏         | 2/100 [00:00<00:12,  8.09it/s]  3%|▎         | 3/100 [00:00<00:11,  8.24it/s]  4%|▍         | 4/100 [00:00<00:11,  8.43it/s]  5%|▌         | 5/100 [00:00<00:10,  8.82it/s]  7%|▋         | 7/100 [00:00<00:09,  9.34it/s]  8%|▊         | 8/100 [00:00<00:09,  9.43it/s]  9%|▉         | 9/100 [00:00<00:09,  9.57it/s] 10%|█         | 10/100 [00:01<00:09,  9.50it/s] 11%|█         | 11/100 [00:01<00:09,  9.44it/s] 12%|█▏        | 12/100 [00:01<00:09,  9.47it/s] 13%|█▎        | 13/100 [00:01<00:09,  9.25it/s] 14%|█▍        | 14/100 [00:01<00:09,  9.11it/s] 15%|█▌        | 15/100 [00:01<00:09,  9.22it/s] 17%|█▋        | 17/100 [00:01<00:08,  9.73it/s] 19%|█▉        | 19/100 [00:02<00:08, 10.09it/s] 21%|██        | 21/100 [00:02<00:07, 10.41it/s] 23%|██▎       | 23/100 [00:02<00:07, 10.13it/s] 25%|██▌       | 25/100 [00:02<00:07, 10.14it/s] 27%|██▋       | 27/100 [00:02<00:07,  9.85it/s] 29%|██▉       | 29/100 [00:03<00:07, 10.04it/s] 31%|███       | 31/100 [00:03<00:06,  9.97it/s] 32%|███▏      | 32/100 [00:03<00:06,  9.82it/s] 33%|███▎      | 33/100 [00:03<00:06,  9.75it/s] 35%|███▌      | 35/100 [00:03<00:06,  9.88it/s] 37%|███▋      | 37/100 [00:03<00:06, 10.14it/s] 39%|███▉      | 39/100 [00:04<00:05, 10.24it/s] 41%|████      | 41/100 [00:04<00:05, 10.36it/s] 43%|████▎     | 43/100 [00:04<00:05, 10.35it/s] 45%|████▌     | 45/100 [00:04<00:05, 10.45it/s] 47%|████▋     | 47/100 [00:04<00:05, 10.49it/s] 49%|████▉     | 49/100 [00:04<00:05, 10.00it/s] 51%|█████     | 51/100 [00:05<00:06,  7.87it/s] 53%|█████▎    | 53/100 [00:05<00:05,  8.54it/s] 54%|█████▍    | 54/100 [00:05<00:05,  8.73it/s] 56%|█████▌    | 56/100 [00:05<00:04,  9.50it/s] 58%|█████▊    | 58/100 [00:06<00:04,  9.35it/s] 59%|█████▉    | 59/100 [00:06<00:04,  9.45it/s] 60%|██████    | 60/100 [00:06<00:04,  9.52it/s] 61%|██████    | 61/100 [00:06<00:04,  9.53it/s] 62%|██████▏   | 62/100 [00:06<00:03,  9.55it/s] 64%|██████▍   | 64/100 [00:06<00:03,  9.76it/s] 65%|██████▌   | 65/100 [00:06<00:04,  7.74it/s] 66%|██████▌   | 66/100 [00:07<00:05,  6.14it/s] 68%|██████▊   | 68/100 [00:07<00:04,  7.37it/s] 70%|███████   | 70/100 [00:07<00:03,  8.32it/s] 72%|███████▏  | 72/100 [00:07<00:03,  9.09it/s] 74%|███████▍  | 74/100 [00:07<00:02,  9.56it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.77it/s] 78%|███████▊  | 78/100 [00:08<00:02,  9.87it/s] 80%|████████  | 80/100 [00:08<00:01, 10.19it/s] 82%|████████▏ | 82/100 [00:08<00:01, 10.09it/s] 84%|████████▍ | 84/100 [00:08<00:01, 10.23it/s] 86%|████████▌ | 86/100 [00:09<00:01, 10.38it/s] 88%|████████▊ | 88/100 [00:09<00:01, 10.56it/s] 90%|█████████ | 90/100 [00:09<00:00, 10.63it/s] 92%|█████████▏| 92/100 [00:09<00:00, 10.67it/s] 94%|█████████▍| 94/100 [00:09<00:00, 10.47it/s] 96%|█████████▌| 96/100 [00:09<00:00, 10.43it/s] 98%|█████████▊| 98/100 [00:10<00:00, 10.65it/s]100%|██████████| 100/100 [00:10<00:00, 10.40it/s]100%|██████████| 100/100 [00:10<00:00,  9.64it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0118, 0.00443
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00442, 0.00718
--- total mse / var(X): 0.0058
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:08, 11.51it/s]  4%|▍         | 4/100 [00:00<00:08, 11.62it/s]  6%|▌         | 6/100 [00:00<00:07, 11.89it/s]  8%|▊         | 8/100 [00:00<00:07, 12.18it/s] 10%|█         | 10/100 [00:00<00:07, 12.50it/s] 12%|█▏        | 12/100 [00:00<00:07, 12.28it/s] 14%|█▍        | 14/100 [00:01<00:08,  9.64it/s] 16%|█▌        | 16/100 [00:01<00:08, 10.33it/s] 18%|█▊        | 18/100 [00:01<00:07, 11.16it/s] 20%|██        | 20/100 [00:01<00:06, 11.60it/s] 22%|██▏       | 22/100 [00:01<00:06, 12.05it/s] 24%|██▍       | 24/100 [00:02<00:06, 12.24it/s] 26%|██▌       | 26/100 [00:02<00:05, 12.65it/s] 28%|██▊       | 28/100 [00:02<00:05, 12.78it/s] 30%|███       | 30/100 [00:02<00:05, 13.06it/s] 32%|███▏      | 32/100 [00:02<00:05, 13.44it/s] 34%|███▍      | 34/100 [00:02<00:04, 13.62it/s] 36%|███▌      | 36/100 [00:02<00:04, 13.41it/s] 38%|███▊      | 38/100 [00:03<00:04, 13.56it/s] 40%|████      | 40/100 [00:03<00:05, 11.38it/s] 42%|████▏     | 42/100 [00:03<00:04, 11.99it/s] 44%|████▍     | 44/100 [00:03<00:04, 12.48it/s] 46%|████▌     | 46/100 [00:03<00:04, 12.77it/s] 48%|████▊     | 48/100 [00:03<00:03, 13.11it/s] 50%|█████     | 50/100 [00:04<00:03, 13.21it/s] 52%|█████▏    | 52/100 [00:04<00:03, 13.28it/s] 54%|█████▍    | 54/100 [00:04<00:03, 13.32it/s] 56%|█████▌    | 56/100 [00:04<00:03, 13.65it/s] 58%|█████▊    | 58/100 [00:04<00:03, 13.95it/s] 60%|██████    | 60/100 [00:04<00:02, 14.02it/s] 62%|██████▏   | 62/100 [00:04<00:02, 14.11it/s] 64%|██████▍   | 64/100 [00:05<00:02, 14.25it/s] 66%|██████▌   | 66/100 [00:05<00:02, 14.42it/s] 68%|██████▊   | 68/100 [00:05<00:02, 14.36it/s] 70%|███████   | 70/100 [00:05<00:02, 14.37it/s] 72%|███████▏  | 72/100 [00:05<00:01, 14.36it/s] 74%|███████▍  | 74/100 [00:05<00:01, 14.40it/s] 76%|███████▌  | 76/100 [00:05<00:01, 14.30it/s] 78%|███████▊  | 78/100 [00:06<00:01, 13.83it/s] 80%|████████  | 80/100 [00:06<00:01, 13.56it/s] 82%|████████▏ | 82/100 [00:06<00:01, 13.80it/s] 84%|████████▍ | 84/100 [00:06<00:01, 13.90it/s] 86%|████████▌ | 86/100 [00:06<00:01, 13.92it/s] 88%|████████▊ | 88/100 [00:06<00:00, 14.13it/s] 90%|█████████ | 90/100 [00:06<00:00, 13.82it/s] 92%|█████████▏| 92/100 [00:07<00:00, 14.06it/s] 94%|█████████▍| 94/100 [00:07<00:00, 14.17it/s] 96%|█████████▌| 96/100 [00:07<00:00, 14.48it/s] 98%|█████████▊| 98/100 [00:07<00:00, 14.82it/s]100%|██████████| 100/100 [00:07<00:00, 14.98it/s]100%|██████████| 100/100 [00:07<00:00, 13.21it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00977, 0.00969
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00769, 0.00775
--- total mse / var(X): 0.00872
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:05, 17.61it/s]  4%|▍         | 4/100 [00:00<00:05, 17.32it/s]  6%|▌         | 6/100 [00:00<00:05, 16.03it/s]  8%|▊         | 8/100 [00:00<00:05, 15.79it/s] 10%|█         | 10/100 [00:00<00:05, 16.78it/s] 12%|█▏        | 12/100 [00:00<00:05, 17.08it/s] 14%|█▍        | 14/100 [00:00<00:04, 17.87it/s] 17%|█▋        | 17/100 [00:00<00:04, 18.97it/s] 20%|██        | 20/100 [00:01<00:04, 19.49it/s] 23%|██▎       | 23/100 [00:01<00:03, 20.04it/s] 26%|██▌       | 26/100 [00:01<00:03, 20.12it/s] 29%|██▉       | 29/100 [00:01<00:03, 19.92it/s] 32%|███▏      | 32/100 [00:01<00:03, 20.11it/s] 35%|███▌      | 35/100 [00:01<00:03, 20.37it/s] 38%|███▊      | 38/100 [00:01<00:02, 20.95it/s] 41%|████      | 41/100 [00:02<00:02, 21.48it/s] 44%|████▍     | 44/100 [00:02<00:02, 21.85it/s] 47%|████▋     | 47/100 [00:02<00:02, 22.16it/s] 50%|█████     | 50/100 [00:02<00:02, 22.68it/s] 53%|█████▎    | 53/100 [00:02<00:02, 22.71it/s] 56%|█████▌    | 56/100 [00:02<00:01, 22.94it/s] 59%|█████▉    | 59/100 [00:02<00:01, 23.09it/s] 62%|██████▏   | 62/100 [00:03<00:01, 23.38it/s] 65%|██████▌   | 65/100 [00:03<00:01, 22.80it/s] 68%|██████▊   | 68/100 [00:03<00:01, 22.07it/s] 71%|███████   | 71/100 [00:03<00:01, 22.11it/s] 74%|███████▍  | 74/100 [00:03<00:01, 21.92it/s] 77%|███████▋  | 77/100 [00:03<00:01, 21.15it/s] 80%|████████  | 80/100 [00:03<00:00, 21.22it/s] 83%|████████▎ | 83/100 [00:04<00:00, 21.03it/s] 86%|████████▌ | 86/100 [00:04<00:00, 21.35it/s] 89%|████████▉ | 89/100 [00:04<00:00, 21.52it/s] 92%|█████████▏| 92/100 [00:04<00:00, 22.05it/s] 95%|█████████▌| 95/100 [00:04<00:00, 21.72it/s] 98%|█████████▊| 98/100 [00:04<00:00, 21.78it/s]100%|██████████| 100/100 [00:04<00:00, 20.95it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00564, 0.00328
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00542, 0.00769
--- total mse / var(X): 0.00548
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  4%|▍         | 4/100 [00:00<00:03, 30.45it/s]  8%|▊         | 8/100 [00:00<00:02, 30.95it/s] 12%|█▏        | 12/100 [00:00<00:02, 33.53it/s] 16%|█▌        | 16/100 [00:00<00:02, 29.11it/s] 20%|██        | 20/100 [00:00<00:02, 26.91it/s] 23%|██▎       | 23/100 [00:00<00:02, 27.60it/s] 27%|██▋       | 27/100 [00:00<00:02, 29.21it/s] 30%|███       | 30/100 [00:01<00:02, 28.27it/s] 33%|███▎      | 33/100 [00:01<00:02, 28.35it/s] 36%|███▌      | 36/100 [00:01<00:02, 28.00it/s] 39%|███▉      | 39/100 [00:01<00:02, 27.51it/s] 43%|████▎     | 43/100 [00:01<00:01, 29.84it/s] 46%|████▌     | 46/100 [00:01<00:01, 28.87it/s] 49%|████▉     | 49/100 [00:01<00:01, 29.14it/s] 53%|█████▎    | 53/100 [00:01<00:01, 31.43it/s] 57%|█████▋    | 57/100 [00:01<00:01, 33.09it/s] 61%|██████    | 61/100 [00:02<00:01, 32.32it/s] 65%|██████▌   | 65/100 [00:02<00:01, 34.16it/s] 69%|██████▉   | 69/100 [00:02<00:00, 33.89it/s] 74%|███████▍  | 74/100 [00:02<00:00, 35.32it/s] 78%|███████▊  | 78/100 [00:02<00:00, 34.83it/s] 82%|████████▏ | 82/100 [00:02<00:00, 34.44it/s] 86%|████████▌ | 86/100 [00:02<00:00, 34.88it/s] 90%|█████████ | 90/100 [00:02<00:00, 25.87it/s] 93%|█████████▎| 93/100 [00:03<00:00, 26.73it/s] 97%|█████████▋| 97/100 [00:03<00:00, 29.16it/s]100%|██████████| 100/100 [00:03<00:00, 30.52it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00306, 0.00305
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00171, 0.00171
--- total mse / var(X): 0.00238
start table evaluation...
Elapsed time: 107.43298888206482 seconds
Cosine similarity between AMM and exact (Train): 0.72432995
Cosine similarity between AMM and exact (Test): 0.72412115
p,r,f1: 0.7386025342866525 0.8471060482777326 0.7891420667669624
p,r,f1: 0.5273564926620843 0.4399643535200899 0.47971272258718656
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.7386025342866525, 0.8471060482777326, 0.7891420667669624],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9950621724128723,
                                      0.9980561137199402,
                                      0.9975453615188599,
                                      0.7243298888206482],
               'cossim_layer_test': [0.9934624433517456,
                                     0.9978984594345093,
                                     0.997361958026886,
                                     0.7241211533546448],
               'cossim_amm_train': [0.9911162257194519,
                                    0.9977176785469055,
                                    0.9949645400047302,
                                    0.9935084581375122,
                                    0.9975875020027161,
                                    0.9842091798782349,
                                    0.9918542504310608,
                                    0.9284278750419617],
               'cossim_amm_test': [0.9882204532623291,
                                   0.9978299736976624,
                                   0.9952959418296814,
                                   0.9943065047264099,
                                   0.9976423978805542,
                                   0.9839295148849487,
                                   0.9915249943733215,
                                   0.928338885307312],
               'f1': [0.5273564926620843,
                      0.4399643535200899,
                      0.47971272258718656],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        1,376
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 6,128
Trainable params: 6,128
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        1,376
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 6,128
Trainable params: 6,128
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6606002773 - test_loss: 0.7149462606
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.6343802432 - test_loss: 0.6859078501
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.6082052324 - test_loss: 0.6566674251
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.5820014626 - test_loss: 0.6272929355
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.5557350443 - test_loss: 0.5977001811
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.5294048613 - test_loss: 0.5680277949
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.5033552574 - test_loss: 0.5390790149
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.4784829315 - test_loss: 0.5119666132
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.4556396527 - test_loss: 0.4873667145
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.4352134172 - test_loss: 0.4654271987
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.4171135187 - test_loss: 0.4458677276
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.4009573381 - test_loss: 0.4281534301
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.3863148964 - test_loss: 0.4118179851
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.3726862208 - test_loss: 0.3965606428
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.3600770607 - test_loss: 0.3821955949
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.3482664258 - test_loss: 0.3686420349
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.3371816419 - test_loss: 0.3558347166
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.3268194475 - test_loss: 0.3437444018
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.3170166662 - test_loss: 0.3323525547
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.3079883179 - test_loss: 0.3216202360
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2994964601 - test_loss: 0.3115430708
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2916207166 - test_loss: 0.3020836617
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2843549782 - test_loss: 0.2932327175
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2776462427 - test_loss: 0.2849638617
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2713275162 - test_loss: 0.2772520662
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2656879366 - test_loss: 0.2700773438
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2604605086 - test_loss: 0.2634160371
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2556887763 - test_loss: 0.2572386405
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2513498036 - test_loss: 0.2515270268
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2474549791 - test_loss: 0.2462514572
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2438647035 - test_loss: 0.2413945687
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2407098132 - test_loss: 0.2369300968
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2377898080 - test_loss: 0.2328271106
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2352932554 - test_loss: 0.2290797359
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2330446870 - test_loss: 0.2256542292
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2310103487 - test_loss: 0.2225431642
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2292056594 - test_loss: 0.2197063233
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2277174132 - test_loss: 0.2171392172
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2263242910 - test_loss: 0.2148346370
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2251811162 - test_loss: 0.2127600337
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2242211212 - test_loss: 0.2109089082
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2233302870 - test_loss: 0.2092611215
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2225392035 - test_loss: 0.2078023426
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2219806854 - test_loss: 0.2065171431
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2213941739 - test_loss: 0.2053852153
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2209429871 - test_loss: 0.2044087836
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2206334715 - test_loss: 0.2035618224
-------- Save Best Model! --------
------- START EPOCH 48 -------
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6161564264 - test_loss: 0.6470678877
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5558914576 - test_loss: 0.5805449397
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.5018557352 - test_loss: 0.5269169734
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4605396143 - test_loss: 0.4858225747
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.4278540411 - test_loss: 0.4515290014
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3998495892 - test_loss: 0.4214496386
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3753506610 - test_loss: 0.3946666713
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3536857757 - test_loss: 0.3707338936
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3345248173 - test_loss: 0.3493323003
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.3176264191 - test_loss: 0.3302216540
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.3028251520 - test_loss: 0.3132244959
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2897516362 - test_loss: 0.2981321745
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2784248006 - test_loss: 0.2847544571
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2686375393 - test_loss: 0.2729479267
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2601547457 - test_loss: 0.2625284361
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2528706447 - test_loss: 0.2533702318
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2467187605 - test_loss: 0.2453325568
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2414716492 - test_loss: 0.2382970748
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2371102083 - test_loss: 0.2321635103
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2334173340 - test_loss: 0.2267863248
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2303471538 - test_loss: 0.2221349619
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2278448503 - test_loss: 0.2181039743
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2257455321 - test_loss: 0.2146224299
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2241224772 - test_loss: 0.2116389975
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2227729123 - test_loss: 0.2090274277
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2215112494 - test_loss: 0.2067362965
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2203971227 - test_loss: 0.2045725615
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2192240051 - test_loss: 0.2025991641
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2179516214 - test_loss: 0.2005672444
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2167147414 - test_loss: 0.1986024986
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2155919388 - test_loss: 0.1966300668
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2143736711 - test_loss: 0.1948122754
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2132214119 - test_loss: 0.1931123912
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2119281977 - test_loss: 0.1914283078
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2105806505 - test_loss: 0.1898933831
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2093666650 - test_loss: 0.1882692470
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2081665982 - test_loss: 0.1865094175
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2069650412 - test_loss: 0.1850216599
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2058493073 - test_loss: 0.1836266491
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2045555603 - test_loss: 0.1820962423
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2035175069 - test_loss: 0.1806348848
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2025118748 - test_loss: 0.1793822433
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2014942539 - test_loss: 0.1780178079
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2006449550 - test_loss: 0.1771321653
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.1998018151 - test_loss: 0.1757370309
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.1990603584 - test_loss: 0.1746113060
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 48 - loss: 0.2203268223 - test_loss: 0.2028385446
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2200602001 - test_loss: 0.2022353216
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2199219260 - test_loss: 0.2017034790
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2197544855 - test_loss: 0.2012849103
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2196232183 - test_loss: 0.2009313006
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2195293904 - test_loss: 0.2006428920
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2194791561 - test_loss: 0.2004018827
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2193757999 - test_loss: 0.2002240478
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2194084562 - test_loss: 0.2000756088
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2193529434 - test_loss: 0.1999294347
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2193028050 - test_loss: 0.1998648926
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2193059118 - test_loss: 0.1997547113
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2192397640 - test_loss: 0.1997323218
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2192019582 - test_loss: 0.1996346653
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2191927977 - test_loss: 0.1995993048
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2190956054 - test_loss: 0.1995908614
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2191760062 - test_loss: 0.1995275631
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2191029011 - test_loss: 0.1995015832
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2190438454 - test_loss: 0.1994358328
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2190676194 - test_loss: 0.1994355557
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2189508365 - test_loss: 0.1993849025
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2190345877 - test_loss: 0.1993496225
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2190228053 - test_loss: 0.1992878081
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2189685846 - test_loss: 0.1992850656
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2188907422 - test_loss: 0.1991906379
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2188636494 - test_loss: 0.1991434140
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2188429183 - test_loss: 0.1991277699
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2188049269 - test_loss: 0.1990505102
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2187986290 - test_loss: 0.1990289364
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2187603088 - test_loss: 0.1990067321
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2187741747 - test_loss: 0.1988828172
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2186948382 - test_loss: 0.1988334331
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2186191121 - test_loss: 0.1987861641
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2185738215 - test_loss: 0.1987269789
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2185439681 - test_loss: 0.1986633106
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.2184557268 - test_loss: 0.1986048811
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.2184121134 - test_loss: 0.1985001587
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.2183424284 - test_loss: 0.1984689483
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.2183196572 - test_loss: 0.1983188197
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.2182804944 - test_loss: 0.1983507965
Early Stop Left: 4
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.2181831128 - test_loss: 0.1981701878
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.2181611813 - test_loss: 0.1981779530
Early Stop Left: 4
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.2180797946 - test_loss: 0.1981018854
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.2179987970 - test_loss: 0.1980342131
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.2179847872 - test_loss: 0.1979100165
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.2179202218 - test_loss: 0.1978862137
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.2178365189 - test_loss: 0.1978792542
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.2177679596 - test_loss: 0.1977694403
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.2177344400 - test_loss: 0.1976381737
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.2176835714 - test_loss: 0.1976269331
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.2176355012 - test_loss: 0.1975415083
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.2175584025 - test_loss: 0.1974249285
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.2174396106 - test_loss: 0.1972943855
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.2174843388 - test_loss: 0.1972924876
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.2173794757 - test_loss: 0.1972448890
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.2173388245 - test_loss: 0.1971515578
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.2172671814 - test_loss: 0.1970757994
-------- Save Best Model! --------
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.2172745179 - test_loss: 0.1969389590
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.2171373645 - test_loss: 0.1969256226
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.2171978669 - test_loss: 0.1968873814
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.2170318282 - test_loss: 0.1968199339
-------- Save Best Model! --------
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.2170621170 - test_loss: 0.1968140347
-------- Save Best Model! --------
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.2170847667 - test_loss: 0.1966682065
-------- Save Best Model! --------
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.2169671793 - test_loss: 0.1966468793
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.2168454463 - test_loss: 0.1965959637
-------- Save Best Model! --------
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.2168590178 - test_loss: 0.1965611499
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.2168412966 - test_loss: 0.1964837486
Epoch: 47 - loss: 0.1982811913 - test_loss: 0.1735553521
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.1976392223 - test_loss: 0.1727769217
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.1968095591 - test_loss: 0.1716984706
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.1960606261 - test_loss: 0.1708510246
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.1952969525 - test_loss: 0.1697334994
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.1945274439 - test_loss: 0.1686220474
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.1937112969 - test_loss: 0.1675381088
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.1928647167 - test_loss: 0.1672097926
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.1920220243 - test_loss: 0.1652695857
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.1910817316 - test_loss: 0.1643788441
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.1902922654 - test_loss: 0.1633338486
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.1893307722 - test_loss: 0.1622800008
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.1885485767 - test_loss: 0.1607145060
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.1875937607 - test_loss: 0.1598483760
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.1868838463 - test_loss: 0.1589993525
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.1860217720 - test_loss: 0.1578245628
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.1852271660 - test_loss: 0.1568738477
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.1843597659 - test_loss: 0.1556834362
-------- Save Best Model! --------
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.1835290167 - test_loss: 0.1546973762
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.1827844551 - test_loss: 0.1540917797
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.1818387669 - test_loss: 0.1528340195
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.1811222198 - test_loss: 0.1519134405
-------- Save Best Model! --------
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.1802232471 - test_loss: 0.1508505293
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.1794079173 - test_loss: 0.1500021798
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.1785494201 - test_loss: 0.1486460669
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.1776907089 - test_loss: 0.1475668889
-------- Save Best Model! --------
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.1768039055 - test_loss: 0.1467951500
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.1759893766 - test_loss: 0.1455017845
-------- Save Best Model! --------
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.1750733269 - test_loss: 0.1444616598
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.1743198923 - test_loss: 0.1434752726
-------- Save Best Model! --------
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.1735174804 - test_loss: 0.1422188364
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.1726661784 - test_loss: 0.1412659364
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.1719145848 - test_loss: 0.1401054853
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.1711442227 - test_loss: 0.1394729652
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.1703552833 - test_loss: 0.1385529037
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.1695900802 - test_loss: 0.1375549563
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.1689159004 - test_loss: 0.1364972648
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.1682704267 - test_loss: 0.1358317012
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.1675731136 - test_loss: 0.1348881169
-------- Save Best Model! --------
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.1669679223 - test_loss: 0.1340864629
-------- Save Best Model! --------
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.1662758196 - test_loss: 0.1331441755
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.1657364294 - test_loss: 0.1324603752
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.1650928654 - test_loss: 0.1317306000
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.1646333311 - test_loss: 0.1310025854
-------- Save Best Model! --------
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.1641188630 - test_loss: 0.1304013778
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.1635339003 - test_loss: 0.1298965466
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.1630464101 - test_loss: 0.1291745728
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.1626144421 - test_loss: 0.1282870334
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.1621132827 - test_loss: 0.1279845944
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.1615468278 - test_loss: 0.1270071046
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.1610901927 - test_loss: 0.1267914135
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.1605788180 - test_loss: 0.1258212172
-------- Save Best Model! --------
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.1601060015 - test_loss: 0.1254816541
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.1596373498 - test_loss: 0.1252033360
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.1592703024 - test_loss: 0.1240627186
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.1586838598 - test_loss: 0.1233577359
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.1582584265 - test_loss: 0.1227131340
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.1578330909 - test_loss: 0.1221130227
-------- Save Best Model! --------
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.1572915426 - test_loss: 0.1215293980
-------- Save Best Model! --------
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.1568935590 - test_loss: 0.1210577071
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.1564101100 - test_loss: 0.1205448863
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.1559480147 - test_loss: 0.1202068967
-------- Save Best Model! --------
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.1555493966 - test_loss: 0.1194469349
-------- Save Best Model! --------
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.1551042786 - test_loss: 0.1187160449
-------- Save Best Model! --------
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.1546522278 - test_loss: 0.1184278842
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.1543196827 - test_loss: 0.1181651225
-------- Save Best Model! --------
------- START EPOCH 113 -------
-------- Save Best Model! --------
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.2167701351 - test_loss: 0.1964618461
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.2167024624 - test_loss: 0.1963611574
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.2166763029 - test_loss: 0.1963467747
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.2166553447 - test_loss: 0.1962857153
-------- Save Best Model! --------
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.2166213786 - test_loss: 0.1962945082
Early Stop Left: 4
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.2165914763 - test_loss: 0.1962461758
-------- Save Best Model! --------
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.2165419013 - test_loss: 0.1961832877
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.2165225949 - test_loss: 0.1961285920
-------- Save Best Model! --------
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.2164733498 - test_loss: 0.1960553687
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.2164917962 - test_loss: 0.1960317361
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.2163722319 - test_loss: 0.1959730174
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.2163880285 - test_loss: 0.1959068171
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.2163367783 - test_loss: 0.1958734308
-------- Save Best Model! --------
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.2163175160 - test_loss: 0.1957968663
-------- Save Best Model! --------
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.2162030690 - test_loss: 0.1956537070
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.2160904684 - test_loss: 0.1955345635
-------- Save Best Model! --------
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.2159514195 - test_loss: 0.1953459052
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.2157741336 - test_loss: 0.1951411081
-------- Save Best Model! --------
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.2155545946 - test_loss: 0.1948806901
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.2153622983 - test_loss: 0.1945852616
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.2151430889 - test_loss: 0.1942558030
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.2148511461 - test_loss: 0.1939180455
-------- Save Best Model! --------
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.2145422477 - test_loss: 0.1934901856
-------- Save Best Model! --------
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.2142035467 - test_loss: 0.1931130847
-------- Save Best Model! --------
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.2139557062 - test_loss: 0.1927392244
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.2137053325 - test_loss: 0.1923912049
-------- Save Best Model! --------
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.2133402427 - test_loss: 0.1919665881
-------- Save Best Model! --------
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.2130896859 - test_loss: 0.1916259405
-------- Save Best Model! --------
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.2129334424 - test_loss: 0.1912527263
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.2126161144 - test_loss: 0.1909336892
-------- Save Best Model! --------
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.2123649850 - test_loss: 0.1906673003
-------- Save Best Model! --------
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.2121130492 - test_loss: 0.1903319867
-------- Save Best Model! --------
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.2118812699 - test_loss: 0.1900540700
-------- Save Best Model! --------
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.2116871600 - test_loss: 0.1897560530
-------- Save Best Model! --------
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.2114735072 - test_loss: 0.1895597231
-------- Save Best Model! --------
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.2112452788 - test_loss: 0.1892423509
-------- Save Best Model! --------
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.2111497569 - test_loss: 0.1890491208
-------- Save Best Model! --------
------- START EPOCH 152 -------
Epoch: 152 - loss: 0.2109305060 - test_loss: 0.1887643178
-------- Save Best Model! --------
------- START EPOCH 153 -------
Epoch: 153 - loss: 0.2108112606 - test_loss: 0.1885625197
-------- Save Best Model! --------
------- START EPOCH 154 -------
Epoch: 154 - loss: 0.2106974863 - test_loss: 0.1883919498
-------- Save Best Model! --------
------- START EPOCH 155 -------
Epoch: 155 - loss: 0.2104925449 - test_loss: 0.1882440645
-------- Save Best Model! --------
------- START EPOCH 156 -------
Epoch: 156 - loss: 0.2104230414 - test_loss: 0.1879463164
-------- Save Best Model! --------
------- START EPOCH 157 -------
Epoch: 157 - loss: 0.2102326645 - test_loss: 0.1878899264
-------- Save Best Model! --------
------- START EPOCH 158 -------
Epoch: 158 - loss: 0.2101166877 - test_loss: 0.1877151954
-------- Save Best Model! --------
------- START EPOCH 159 -------
Epoch: 159 - loss: 0.2099854623 - test_loss: 0.1875781525
-------- Save Best Model! --------
------- START EPOCH 160 -------
Epoch: 160 - loss: 0.2098837354 - test_loss: 0.1874152630
-------- Save Best Model! --------
------- START EPOCH 161 -------
Epoch: 161 - loss: 0.2097710809 - test_loss: 0.1872829143
-------- Save Best Model! --------
------- START EPOCH 162 -------
Epoch: 162 - loss: 0.2096812564 - test_loss: 0.1871409272
-------- Save Best Model! --------
------- START EPOCH 163 -------
Epoch: 163 - loss: 0.2096697630 - test_loss: 0.1870176189
-------- Save Best Model! --------
------- START EPOCH 164 -------
Epoch: 164 - loss: 0.2094409316 - test_loss: 0.1868928226
-------- Save Best Model! --------
------- START EPOCH 165 -------
Epoch: 165 - loss: 0.2093268909 - test_loss: 0.1867726588
-------- Save Best Model! --------
------- START EPOCH 166 -------
Epoch: 166 - loss: 0.2092159385 - test_loss: 0.1866464754
-------- Save Best Model! --------
------- START EPOCH 167 -------
Epoch: 167 - loss: 0.2091521282 - test_loss: 0.1865419385
-------- Save Best Model! --------
------- START EPOCH 168 -------
Epoch: 168 - loss: 0.2090293324 - test_loss: 0.1865163512
-------- Save Best Model! --------
------- START EPOCH 169 -------
Epoch: 169 - loss: 0.2089257962 - test_loss: 0.1862978646
-------- Save Best Model! --------
------- START EPOCH 170 -------
Epoch: 170 - loss: 0.2088668345 - test_loss: 0.1861680883
-------- Save Best Model! --------
------- START EPOCH 171 -------
Epoch: 171 - loss: 0.2087866355 - test_loss: 0.1860851091
-------- Save Best Model! --------
------- START EPOCH 172 -------
Epoch: 172 - loss: 0.2086972126 - test_loss: 0.1859616817
-------- Save Best Model! --------
------- START EPOCH 173 -------
Epoch: 173 - loss: 0.2086100772 - test_loss: 0.1859209111
-------- Save Best Model! --------
------- START EPOCH 174 -------
Epoch: 174 - loss: 0.2084420770 - test_loss: 0.1858249934
-------- Save Best Model! --------
------- START EPOCH 175 -------
Epoch: 175 - loss: 0.2084020735 - test_loss: 0.1857154364
-------- Save Best Model! --------
------- START EPOCH 176 -------
Epoch: 176 - loss: 0.2083364210 - test_loss: 0.1855553052
-------- Save Best Model! --------
------- START EPOCH 177 -------
Epoch: 177 - loss: 0.2082118231 - test_loss: 0.1854660594
-------- Save Best Model! --------
------- START EPOCH 178 -------
Epoch: 178 - loss: 0.2081570203 - test_loss: 0.1853901926
-------- Save Best Model! --------
------- START EPOCH 179 -------
Epoch: 179 - loss: 0.2080122073 - test_loss: 0.1853504011
-------- Save Best Model! --------
------- START EPOCH 180 -------
Epoch: 113 - loss: 0.1538711237 - test_loss: 0.1168593756
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.1535040345 - test_loss: 0.1163340914
-------- Save Best Model! --------
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.1530529553 - test_loss: 0.1158193385
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.1526153694 - test_loss: 0.1158164820
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.1522159643 - test_loss: 0.1154004292
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.1519074323 - test_loss: 0.1143638617
-------- Save Best Model! --------
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.1515337138 - test_loss: 0.1141060121
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.1511668088 - test_loss: 0.1135973193
-------- Save Best Model! --------
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.1507131606 - test_loss: 0.1131658305
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.1503502027 - test_loss: 0.1127307569
-------- Save Best Model! --------
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.1499955403 - test_loss: 0.1124336271
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.1496378406 - test_loss: 0.1116920296
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.1493762882 - test_loss: 0.1108993900
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.1490456678 - test_loss: 0.1109867773
Early Stop Left: 4
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.1487180119 - test_loss: 0.1106123518
-------- Save Best Model! --------
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.1483438323 - test_loss: 0.1099653697
-------- Save Best Model! --------
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.1480679556 - test_loss: 0.1091213031
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.1476647563 - test_loss: 0.1087005167
-------- Save Best Model! --------
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.1474262143 - test_loss: 0.1086729490
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.1471435098 - test_loss: 0.1082927437
-------- Save Best Model! --------
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.1467410778 - test_loss: 0.1075575215
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.1465349816 - test_loss: 0.1073487283
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.1462647384 - test_loss: 0.1065976601
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.1459813683 - test_loss: 0.1069939922
Early Stop Left: 4
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.1457015890 - test_loss: 0.1064466031
-------- Save Best Model! --------
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.1454173747 - test_loss: 0.1063381562
-------- Save Best Model! --------
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.1451542773 - test_loss: 0.1053654294
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.1449186161 - test_loss: 0.1056623068
Early Stop Left: 4
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.1446671936 - test_loss: 0.1051580128
-------- Save Best Model! --------
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.1444154977 - test_loss: 0.1045472548
-------- Save Best Model! --------
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.1441070645 - test_loss: 0.1039952835
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.1439505703 - test_loss: 0.1038255411
-------- Save Best Model! --------
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.1436739365 - test_loss: 0.1043634655
Early Stop Left: 4
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.1434656624 - test_loss: 0.1032637221
-------- Save Best Model! --------
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.1432621400 - test_loss: 0.1029659818
-------- Save Best Model! --------
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.1429729327 - test_loss: 0.1029426636
-------- Save Best Model! --------
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.1427766999 - test_loss: 0.1030980330
Early Stop Left: 4
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.1426010714 - test_loss: 0.1021756955
-------- Save Best Model! --------
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.1423304226 - test_loss: 0.1014731982
-------- Save Best Model! --------
------- START EPOCH 152 -------
Epoch: 152 - loss: 0.1422007074 - test_loss: 0.1021045050
Early Stop Left: 4
------- START EPOCH 153 -------
Epoch: 153 - loss: 0.1419667365 - test_loss: 0.1012957893
-------- Save Best Model! --------
------- START EPOCH 154 -------
Epoch: 154 - loss: 0.1417653907 - test_loss: 0.1009559381
-------- Save Best Model! --------
------- START EPOCH 155 -------
Epoch: 155 - loss: 0.1415408509 - test_loss: 0.1013569983
Early Stop Left: 4
------- START EPOCH 156 -------
Epoch: 156 - loss: 0.1413371620 - test_loss: 0.1007056714
-------- Save Best Model! --------
------- START EPOCH 157 -------
Epoch: 157 - loss: 0.1411627899 - test_loss: 0.0999337381
-------- Save Best Model! --------
------- START EPOCH 158 -------
Epoch: 158 - loss: 0.1410048968 - test_loss: 0.1002487109
Early Stop Left: 4
------- START EPOCH 159 -------
Epoch: 159 - loss: 0.1408168372 - test_loss: 0.1000950165
Early Stop Left: 3
------- START EPOCH 160 -------
Epoch: 160 - loss: 0.1406118775 - test_loss: 0.0994113787
-------- Save Best Model! --------
------- START EPOCH 161 -------
Epoch: 161 - loss: 0.1404305888 - test_loss: 0.0992036174
-------- Save Best Model! --------
------- START EPOCH 162 -------
Epoch: 162 - loss: 0.1402316259 - test_loss: 0.0995349014
Early Stop Left: 4
------- START EPOCH 163 -------
Epoch: 163 - loss: 0.1400722719 - test_loss: 0.0989785071
-------- Save Best Model! --------
------- START EPOCH 164 -------
Epoch: 164 - loss: 0.1398980504 - test_loss: 0.0987427570
-------- Save Best Model! --------
------- START EPOCH 165 -------
Epoch: 165 - loss: 0.1397136759 - test_loss: 0.0984669990
-------- Save Best Model! --------
------- START EPOCH 166 -------
Epoch: 166 - loss: 0.1395390974 - test_loss: 0.0982165745
-------- Save Best Model! --------
------- START EPOCH 167 -------
Epoch: 167 - loss: 0.1393720431 - test_loss: 0.0977656497
-------- Save Best Model! --------
------- START EPOCH 168 -------
Epoch: 168 - loss: 0.1391992980 - test_loss: 0.0979233330
Early Stop Left: 4
------- START EPOCH 169 -------
Epoch: 169 - loss: 0.1390728645 - test_loss: 0.0975141616
-------- Save Best Model! --------
------- START EPOCH 170 -------
Epoch: 170 - loss: 0.1388963227 - test_loss: 0.0975445043
Early Stop Left: 4
------- START EPOCH 171 -------
Epoch: 171 - loss: 0.1387795910 - test_loss: 0.0968720203
-------- Save Best Model! --------
------- START EPOCH 172 -------
Epoch: 172 - loss: 0.1385820612 - test_loss: 0.0971278417
Early Stop Left: 4
------- START EPOCH 173 -------
Epoch: 173 - loss: 0.1384467891 - test_loss: 0.0968122269
-------- Save Best Model! --------
------- START EPOCH 174 -------
Epoch: 174 - loss: 0.1382395548 - test_loss: 0.0970342115
Early Stop Left: 4
------- START EPOCH 175 -------
Epoch: 175 - loss: 0.1380765393 - test_loss: 0.0963190782
-------- Save Best Model! --------
------- START EPOCH 176 -------
Epoch: 176 - loss: 0.1379844652 - test_loss: 0.0959463221
-------- Save Best Model! --------
------- START EPOCH 177 -------
Epoch: 177 - loss: 0.1378745104 - test_loss: 0.0963820103
Early Stop Left: 4
------- START EPOCH 178 -------
Epoch: 178 - loss: 0.1376801148 - test_loss: 0.0959289036
-------- Save Best Model! --------
------- START EPOCH 179 -------
Epoch: 179 - loss: 0.1375666556 - test_loss: 0.0959154285
-------- Save Best Model! --------
------- START EPOCH 180 -------
Epoch: 180 - loss: 0.1374022963 - test_loss: 0.0958710028Epoch: 180 - loss: 0.2079430150 - test_loss: 0.1852110724
-------- Save Best Model! --------
------- START EPOCH 181 -------
Epoch: 181 - loss: 0.2078699868 - test_loss: 0.1850978619
-------- Save Best Model! --------
------- START EPOCH 182 -------
Epoch: 182 - loss: 0.2077552531 - test_loss: 0.1850518677
-------- Save Best Model! --------
------- START EPOCH 183 -------
Epoch: 183 - loss: 0.2076911990 - test_loss: 0.1848950121
-------- Save Best Model! --------
------- START EPOCH 184 -------
Epoch: 184 - loss: 0.2076441751 - test_loss: 0.1848498892
-------- Save Best Model! --------
------- START EPOCH 185 -------
Epoch: 185 - loss: 0.2075291506 - test_loss: 0.1847532421
-------- Save Best Model! --------
------- START EPOCH 186 -------
Epoch: 186 - loss: 0.2074527516 - test_loss: 0.1846414913
-------- Save Best Model! --------
------- START EPOCH 187 -------
Epoch: 187 - loss: 0.2073624032 - test_loss: 0.1845688625
-------- Save Best Model! --------
------- START EPOCH 188 -------
Epoch: 188 - loss: 0.2073113219 - test_loss: 0.1845229506
-------- Save Best Model! --------
------- START EPOCH 189 -------
Epoch: 189 - loss: 0.2072804449 - test_loss: 0.1844622171
-------- Save Best Model! --------
------- START EPOCH 190 -------
Epoch: 190 - loss: 0.2071393891 - test_loss: 0.1843418989
-------- Save Best Model! --------
------- START EPOCH 191 -------
Epoch: 191 - loss: 0.2070406559 - test_loss: 0.1842696492
-------- Save Best Model! --------
------- START EPOCH 192 -------
Epoch: 192 - loss: 0.2070308278 - test_loss: 0.1841371361
-------- Save Best Model! --------
------- START EPOCH 193 -------
Epoch: 193 - loss: 0.2069078422 - test_loss: 0.1840450426
-------- Save Best Model! --------
------- START EPOCH 194 -------
Epoch: 194 - loss: 0.2068572068 - test_loss: 0.1839858319
-------- Save Best Model! --------
------- START EPOCH 195 -------
Epoch: 195 - loss: 0.2067895584 - test_loss: 0.1839446847
-------- Save Best Model! --------
------- START EPOCH 196 -------
Epoch: 196 - loss: 0.2067482686 - test_loss: 0.1838432254
-------- Save Best Model! --------
------- START EPOCH 197 -------
Epoch: 197 - loss: 0.2066661511 - test_loss: 0.1837521693
-------- Save Best Model! --------
------- START EPOCH 198 -------
Epoch: 198 - loss: 0.2065657627 - test_loss: 0.1836839170
-------- Save Best Model! --------
------- START EPOCH 199 -------
Epoch: 199 - loss: 0.2065024859 - test_loss: 0.1835861984
-------- Save Best Model! --------
------- START EPOCH 200 -------
Epoch: 200 - loss: 0.2064748681 - test_loss: 0.1835289490
-------- Save Best Model! --------
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 14%|█▍        | 17/121 [00:00<00:00, 167.56it/s] 28%|██▊       | 34/121 [00:00<00:00, 163.60it/s] 42%|████▏     | 51/121 [00:00<00:00, 160.78it/s] 56%|█████▌    | 68/121 [00:00<00:00, 160.19it/s] 70%|███████   | 85/121 [00:00<00:00, 160.78it/s] 84%|████████▍ | 102/121 [00:00<00:00, 161.01it/s] 98%|█████████▊| 119/121 [00:00<00:00, 160.19it/s]100%|██████████| 121/121 [00:00<00:00, 161.83it/s]
Best micro threshold=0.310170, fscore=0.514
p,r,f1: 0.45952495772722 0.5833468944941687 0.514085132246587
throttleing by fixed threshold: 0.5
p,r,f1: 0.6629424572317263 0.24774613506916193 0.360697241496023
{'model': 'vit_min',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.31017038226127625,
                 'p': 0.45952495772722,
                 'r': 0.5833468944941687,
                 'f1': 0.514085132246587},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6629424572317263,
                 'r': 0.24774613506916193,
                 'f1': 0.360697241496023}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm

-------- Save Best Model! --------
------- START EPOCH 181 -------
Epoch: 181 - loss: 0.1372427706 - test_loss: 0.0950020898
-------- Save Best Model! --------
------- START EPOCH 182 -------
Epoch: 182 - loss: 0.1370760375 - test_loss: 0.0949786505
-------- Save Best Model! --------
------- START EPOCH 183 -------
Epoch: 183 - loss: 0.1370285258 - test_loss: 0.0947618226
-------- Save Best Model! --------
------- START EPOCH 184 -------
Epoch: 184 - loss: 0.1368642086 - test_loss: 0.0946325707
-------- Save Best Model! --------
------- START EPOCH 185 -------
Epoch: 185 - loss: 0.1367220536 - test_loss: 0.0943561288
-------- Save Best Model! --------
------- START EPOCH 186 -------
Epoch: 186 - loss: 0.1366158458 - test_loss: 0.0948091072
Early Stop Left: 4
------- START EPOCH 187 -------
Epoch: 187 - loss: 0.1364627738 - test_loss: 0.0939402910
-------- Save Best Model! --------
------- START EPOCH 188 -------
Epoch: 188 - loss: 0.1363592292 - test_loss: 0.0944614833
Early Stop Left: 4
------- START EPOCH 189 -------
Epoch: 189 - loss: 0.1362167084 - test_loss: 0.0937926601
-------- Save Best Model! --------
------- START EPOCH 190 -------
Epoch: 190 - loss: 0.1361103933 - test_loss: 0.0937907918
-------- Save Best Model! --------
------- START EPOCH 191 -------
Epoch: 191 - loss: 0.1359602720 - test_loss: 0.0932551760
-------- Save Best Model! --------
------- START EPOCH 192 -------
Epoch: 192 - loss: 0.1358560516 - test_loss: 0.0930823454
-------- Save Best Model! --------
------- START EPOCH 193 -------
Epoch: 193 - loss: 0.1357499437 - test_loss: 0.0933549341
Early Stop Left: 4
------- START EPOCH 194 -------
Epoch: 194 - loss: 0.1356456594 - test_loss: 0.0930698512
-------- Save Best Model! --------
------- START EPOCH 195 -------
Epoch: 195 - loss: 0.1355659014 - test_loss: 0.0929713285
-------- Save Best Model! --------
------- START EPOCH 196 -------
Epoch: 196 - loss: 0.1354392666 - test_loss: 0.0929747497
Early Stop Left: 4
------- START EPOCH 197 -------
Epoch: 197 - loss: 0.1353463798 - test_loss: 0.0922148552
-------- Save Best Model! --------
------- START EPOCH 198 -------
Epoch: 198 - loss: 0.1352082105 - test_loss: 0.0924927516
Early Stop Left: 4
------- START EPOCH 199 -------
Epoch: 199 - loss: 0.1351119979 - test_loss: 0.0921232888
-------- Save Best Model! --------
------- START EPOCH 200 -------
Epoch: 200 - loss: 0.1350101723 - test_loss: 0.0923307408
Early Stop Left: 4
Validation start
  0%|          | 0/121 [00:00<?, ?it/s] 13%|█▎        | 16/121 [00:00<00:00, 158.92it/s] 27%|██▋       | 33/121 [00:00<00:00, 161.63it/s] 41%|████▏     | 50/121 [00:00<00:00, 161.68it/s] 55%|█████▌    | 67/121 [00:00<00:00, 162.98it/s] 69%|██████▉   | 84/121 [00:00<00:00, 162.45it/s] 83%|████████▎ | 101/121 [00:00<00:00, 162.43it/s] 98%|█████████▊| 118/121 [00:00<00:00, 162.01it/s]100%|██████████| 121/121 [00:00<00:00, 162.69it/s]
Best micro threshold=0.353022, fscore=0.803
p,r,f1: 0.7479306863614089 0.8657507071176721 0.8025394891569589
throttleing by fixed threshold: 0.5
p,r,f1: 0.8254226589555931 0.7498748498585764 0.7858372008981684
{'model': 'vit_large',
 'app': '433.milc-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.35302218794822693,
                 'p': 0.7479306863614089,
                 'r': 0.8657507071176721,
                 'f1': 0.8025394891569589},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.8254226589555931,
                 'r': 0.7498748498585764,
                 'f1': 0.7858372008981684}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000004
Manual and Torch results cosine similarity (Test): 1.0000005
start table training with fine tuning...
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Retrain for 1 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.198, 0.198
--- total mse / var(X): 0.198
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:07<11:50,  7.17s/it]  2%|▏         | 2/100 [00:15<12:21,  7.57s/it]  3%|▎         | 3/100 [00:21<11:20,  7.02s/it]  4%|▍         | 4/100 [00:28<11:33,  7.23s/it]  5%|▌         | 5/100 [00:36<11:41,  7.39s/it]  6%|▌         | 6/100 [00:44<11:59,  7.65s/it]  7%|▋         | 7/100 [00:53<12:36,  8.14s/it]  8%|▊         | 8/100 [01:02<12:38,  8.24s/it]  9%|▉         | 9/100 [01:10<12:40,  8.35s/it] 10%|█         | 10/100 [01:18<12:12,  8.14s/it] 11%|█         | 11/100 [01:26<11:51,  8.00s/it] 12%|█▏        | 12/100 [01:34<11:40,  7.96s/it] 13%|█▎        | 13/100 [01:42<11:32,  7.96s/it] 14%|█▍        | 14/100 [01:50<11:23,  7.94s/it] 15%|█▌        | 15/100 [01:57<11:01,  7.79s/it] 16%|█▌        | 16/100 [02:04<10:29,  7.49s/it] 17%|█▋        | 17/100 [02:12<10:29,  7.58s/it] 18%|█▊        | 18/100 [02:19<10:19,  7.56s/it] 19%|█▉        | 19/100 [02:26<10:02,  7.44s/it] 20%|██        | 20/100 [02:34<09:52,  7.40s/it] 21%|██        | 21/100 [02:41<09:51,  7.48s/it] 22%|██▏       | 22/100 [02:50<10:11,  7.85s/it] 23%|██▎       | 23/100 [02:57<09:37,  7.50s/it] 24%|██▍       | 24/100 [03:04<09:34,  7.56s/it] 25%|██▌       | 25/100 [03:12<09:27,  7.57s/it] 26%|██▌       | 26/100 [03:20<09:23,  7.62s/it] 27%|██▋       | 27/100 [03:28<09:28,  7.78s/it] 28%|██▊       | 28/100 [03:36<09:22,  7.81s/it] 29%|██▉       | 29/100 [03:42<08:44,  7.38s/it] 30%|███       | 30/100 [03:48<08:04,  6.93s/it] 31%|███       | 31/100 [03:53<07:23,  6.43s/it] 32%|███▏      | 32/100 [03:59<07:13,  6.37s/it] 33%|███▎      | 33/100 [04:06<07:06,  6.37s/it] 34%|███▍      | 34/100 [04:12<06:56,  6.31s/it] 35%|███▌      | 35/100 [04:19<07:12,  6.66s/it] 36%|███▌      | 36/100 [04:26<07:00,  6.57s/it] 37%|███▋      | 37/100 [04:32<06:40,  6.35s/it] 38%|███▊      | 38/100 [04:38<06:32,  6.34s/it] 39%|███▉      | 39/100 [04:45<06:45,  6.64s/it] 40%|████      | 40/100 [04:53<06:55,  6.92s/it] 41%|████      | 41/100 [05:01<07:01,  7.15s/it] 42%|████▏     | 42/100 [05:08<06:59,  7.24s/it] 43%|████▎     | 43/100 [05:16<07:01,  7.40s/it] 44%|████▍     | 44/100 [05:23<06:59,  7.49s/it] 45%|████▌     | 45/100 [05:31<06:55,  7.56s/it] 46%|████▌     | 46/100 [05:39<06:48,  7.56s/it] 47%|████▋     | 47/100 [05:47<06:44,  7.63s/it] 48%|████▊     | 48/100 [05:55<06:51,  7.91s/it] 49%|████▉     | 49/100 [06:03<06:36,  7.78s/it] 50%|█████     | 50/100 [06:09<06:11,  7.43s/it] 51%|█████     | 51/100 [06:17<06:10,  7.56s/it] 52%|█████▏    | 52/100 [06:23<05:39,  7.07s/it] 53%|█████▎    | 53/100 [06:29<05:16,  6.74s/it] 54%|█████▍    | 54/100 [06:36<05:12,  6.79s/it] 55%|█████▌    | 55/100 [06:43<05:14,  6.98s/it] 56%|█████▌    | 56/100 [06:51<05:17,  7.23s/it] 57%|█████▋    | 57/100 [07:00<05:37,  7.85s/it] 58%|█████▊    | 58/100 [07:08<05:23,  7.71s/it] 59%|█████▉    | 59/100 [07:15<05:05,  7.45s/it] 60%|██████    | 60/100 [07:21<04:47,  7.18s/it] 61%|██████    | 61/100 [07:29<04:47,  7.37s/it] 62%|██████▏   | 62/100 [07:36<04:36,  7.27s/it] 63%|██████▎   | 63/100 [07:43<04:26,  7.20s/it] 64%|██████▍   | 64/100 [07:50<04:11,  6.98s/it] 65%|██████▌   | 65/100 [07:56<04:00,  6.86s/it] 66%|██████▌   | 66/100 [08:04<04:08,  7.31s/it] 67%|██████▋   | 67/100 [08:12<04:04,  7.40s/it] 68%|██████▊   | 68/100 [08:18<03:39,  6.87s/it] 69%|██████▉   | 69/100 [08:24<03:25,  6.61s/it] 70%|███████   | 70/100 [08:30<03:18,  6.63s/it] 71%|███████   | 71/100 [08:38<03:20,  6.91s/it] 72%|███████▏  | 72/100 [08:45<03:12,  6.87s/it] 73%|███████▎  | 73/100 [08:52<03:06,  6.92s/it] 74%|███████▍  | 74/100 [08:59<03:05,  7.13s/it] 75%|███████▌  | 75/100 [09:07<02:58,  7.15s/it] 76%|███████▌  | 76/100 [09:14<02:52,  7.19s/it] 77%|███████▋  | 77/100 [09:22<02:50,  7.42s/it] 78%|███████▊  | 78/100 [09:29<02:43,  7.43s/it] 79%|███████▉  | 79/100 [09:36<02:30,  7.19s/it] 80%|████████  | 80/100 [09:41<02:14,  6.70s/it]
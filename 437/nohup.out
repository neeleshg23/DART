===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5919307130 - test_loss: 0.5395173253
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4192135360 - test_loss: 0.3950863999
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3299194943 - test_loss: 0.3180839425
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2792854073 - test_loss: 0.2699688971
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2486420079 - test_loss: 0.2390679802
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2299698705 - test_loss: 0.2188503023
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2186113735 - test_loss: 0.2054239619
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2117333584 - test_loss: 0.1964236327
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2076008956 - test_loss: 0.1902693864
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2051421126 - test_loss: 0.1860894188
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2036994673 - test_loss: 0.1831464399
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2028706672 - test_loss: 0.1811904356
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2023963946 - test_loss: 0.1798253386
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2021249479 - test_loss: 0.1790062655
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2019475721 - test_loss: 0.1783456364
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2018254700 - test_loss: 0.1779362349
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2017238812 - test_loss: 0.1776397061
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2016088922 - test_loss: 0.1773771522
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2015023011 - test_loss: 0.1771960120
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2013783225 - test_loss: 0.1770870133
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2012676636 - test_loss: 0.1766349241
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2011250752 - test_loss: 0.1766978706
Early Stop Left: 4
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2009918511 - test_loss: 0.1764691207
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2008243480 - test_loss: 0.1764308057
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2006218869 - test_loss: 0.1764184053
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2003288017 - test_loss: 0.1766593558
Early Stop Left: 4
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.1999372270 - test_loss: 0.1764556152
Early Stop Left: 3
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.1994894575 - test_loss: 0.1762029507
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.1991055631 - test_loss: 0.1762133633
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.1988102321 - test_loss: 0.1759388309
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.1985469484 - test_loss: 0.1758976923
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.1983383280 - test_loss: 0.1761530270
Early Stop Left: 4
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.1981288308 - test_loss: 0.1768097155
Early Stop Left: 3
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.1979657884 - test_loss: 0.1762432950
Early Stop Left: 2
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.1977983022 - test_loss: 0.1766874522
Early Stop Left: 1
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.1976546892 - test_loss: 0.1764015414
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 194.53it/s] 38%|███▊      | 40/104 [00:00<00:00, 193.67it/s] 58%|█████▊    | 60/104 [00:00<00:00, 192.44it/s] 77%|███████▋  | 80/104 [00:00<00:00, 191.89it/s] 96%|█████████▌| 100/104 [00:00<00:00, 192.01it/s]100%|██████████| 104/104 [00:00<00:00, 193.74it/s]
Best micro threshold=0.294705, fscore=0.551
p,r,f1: 0.4669115142112904 0.6716482613424106 0.5508719233792644
throttleing by fixed threshold: 0.5
p,r,f1: 0.6331077097542196 0.3703984333327679 0.4673655571304903
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.29470497369766235,
                 'p': 0.4669115142112904,
                 'r': 0.6716482613424106,
                 'f1': 0.5508719233792644},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6331077097542196,
                 'r': 0.3703984333327679,
                 'f1': 0.4673655571304903}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5308656697 - test_loss: 0.5406942591
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.3810310483 - test_loss: 0.3977579397
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3068945437 - test_loss: 0.3228166063
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2672921809 - test_loss: 0.2767075211
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2449105349 - test_loss: 0.2476038840
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2323630185 - test_loss: 0.2289947899
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2254574985 - test_loss: 0.2170270494
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2217382055 - test_loss: 0.2093943710
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2197828800 - test_loss: 0.2044750802
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2187767944 - test_loss: 0.2014165837
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2182677685 - test_loss: 0.1994451320
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2180145656 - test_loss: 0.1983419026
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2178823680 - test_loss: 0.1976055711
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2178082715 - test_loss: 0.1972683071
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2177472234 - test_loss: 0.1969696826
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2176970930 - test_loss: 0.1968053664
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2176479300 - test_loss: 0.1965999802
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2175826711 - test_loss: 0.1964008946
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2175193317 - test_loss: 0.1963193740
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2174390558 - test_loss: 0.1962976357
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2173668892 - test_loss: 0.1955513993
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2172724217 - test_loss: 0.1957718620
Early Stop Left: 4
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2171900587 - test_loss: 0.1954867129
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2170901730 - test_loss: 0.1953331330
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2169819196 - test_loss: 0.1953726973
Early Stop Left: 4
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2168412049 - test_loss: 0.1953865098
Early Stop Left: 3
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2166780530 - test_loss: 0.1954117606
Early Stop Left: 2
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2164479083 - test_loss: 0.1952531942
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2161140391 - test_loss: 0.1955172659
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2157770215 - test_loss: 0.1949976031
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2155043720 - test_loss: 0.1946738438
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2153101539 - test_loss: 0.1948654874
Early Stop Left: 4
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2151348383 - test_loss: 0.1955148784
Early Stop Left: 3
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2150043650 - test_loss: 0.1947666309
Early Stop Left: 2
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2148750011 - test_loss: 0.1952912842
Early Stop Left: 1
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2147638879 - test_loss: 0.1947756255
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 16%|█▋        | 17/104 [00:00<00:00, 162.92it/s] 35%|███▍      | 36/104 [00:00<00:00, 173.47it/s] 53%|█████▎    | 55/104 [00:00<00:00, 178.03it/s] 71%|███████   | 74/104 [00:00<00:00, 182.05it/s] 90%|█████████ | 94/104 [00:00<00:00, 186.85it/s]100%|██████████| 104/104 [00:00<00:00, 184.55it/s]===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5614035226 - test_loss: 0.5400614830
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4001539626 - test_loss: 0.3963029158
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3185065797 - test_loss: 0.3202375993
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2735037346 - test_loss: 0.2730140116
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2471425551 - test_loss: 0.2428998304
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2317112646 - test_loss: 0.2233730605
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2227729604 - test_loss: 0.2105685988
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2176674976 - test_loss: 0.2021519453
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2148017523 - test_loss: 0.1965409142
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2132220605 - test_loss: 0.1928753904
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2123682528 - test_loss: 0.1904029332
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2119178941 - test_loss: 0.1888823221
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2116771832 - test_loss: 0.1878632241
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2115445409 - test_loss: 0.1873310856
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2114494453 - test_loss: 0.1868868204
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2113771194 - test_loss: 0.1866374102
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2113098815 - test_loss: 0.1864118857
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2112255163 - test_loss: 0.1861968048
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2111449149 - test_loss: 0.1860785822
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2110464234 - test_loss: 0.1860111293
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2109580463 - test_loss: 0.1854078883
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2108431994 - test_loss: 0.1855482930
Early Stop Left: 4
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2107399859 - test_loss: 0.1852766466
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2106130888 - test_loss: 0.1851732979
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2104687202 - test_loss: 0.1851663061
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2102706773 - test_loss: 0.1852904532
Early Stop Left: 4
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2100122401 - test_loss: 0.1852893235
Early Stop Left: 3
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2096522426 - test_loss: 0.1851556025
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2092609006 - test_loss: 0.1851349639
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2089508171 - test_loss: 0.1846816904
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2086962245 - test_loss: 0.1844988010
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2085030672 - test_loss: 0.1847369258
Early Stop Left: 4
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2083160402 - test_loss: 0.1854040531
Early Stop Left: 3
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2081730367 - test_loss: 0.1847275516
Early Stop Left: 2
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2080273464 - test_loss: 0.1852295682
Early Stop Left: 1
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2079022259 - test_loss: 0.1848334917
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 194.05it/s] 38%|███▊      | 40/104 [00:00<00:00, 192.85it/s] 58%|█████▊    | 60/104 [00:00<00:00, 193.11it/s] 77%|███████▋  | 80/104 [00:00<00:00, 192.59it/s] 96%|█████████▌| 100/104 [00:00<00:00, 186.10it/s]100%|██████████| 104/104 [00:00<00:00, 190.66it/s]
Best micro threshold=0.311020, fscore=0.551
p,r,f1: 0.4716634311491245 0.6635548192169303 0.5513909641365873
throttleing by fixed threshold: 0.5
p,r,f1: 0.642464600959353 0.3569615099716711 0.45893364512591855
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3110196590423584,
                 'p': 0.4716634311491245,
                 'r': 0.6635548192169303,
                 'f1': 0.5513909641365873},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.642464600959353,
                 'r': 0.3569615099716711,
                 'f1': 0.45893364512591855}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm

Best micro threshold=0.310871, fscore=0.551
p,r,f1: 0.4762741001005643 0.6543172878110198 0.5512767579241568
throttleing by fixed threshold: 0.5
p,r,f1: 0.6388292060877564 0.36258394543052247 0.4626046974685035
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3108711838722229,
                 'p': 0.4762741001005643,
                 'r': 0.6543172878110198,
                 'f1': 0.5512767579241568},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6388292060877564,
                 'r': 0.36258394543052247,
                 'f1': 0.4626046974685035}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5003160110 - test_loss: 0.5414381847
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.3618279707 - test_loss: 0.3995265138
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.2950345456 - test_loss: 0.3259539833
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2605682007 - test_loss: 0.2812154826
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2418361352 - test_loss: 0.2533764355
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2318033899 - test_loss: 0.2359275239
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2265632388 - test_loss: 0.2250070155
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2238990158 - test_loss: 0.2182966221
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2225804272 - test_loss: 0.2141374937
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2219397336 - test_loss: 0.2116853388
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2216299250 - test_loss: 0.2101656945
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2214805358 - test_loss: 0.2094002522
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2214016601 - test_loss: 0.2088642504
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2213554244 - test_loss: 0.2086360020
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2213119503 - test_loss: 0.2084411655
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2212746799 - test_loss: 0.2083195158
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2212372358 - test_loss: 0.2081159585
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2211856358 - test_loss: 0.2079216281
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2211351467 - test_loss: 0.2078722141
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2210691604 - test_loss: 0.2078930922
Early Stop Left: 4
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2210096947 - test_loss: 0.2070225901
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2209309184 - test_loss: 0.2073225544
Early Stop Left: 4
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2208633858 - test_loss: 0.2070442385
Early Stop Left: 3
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2207820336 - test_loss: 0.2068415010
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2206963148 - test_loss: 0.2069391011
Early Stop Left: 4
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2205886381 - test_loss: 0.2068837115
Early Stop Left: 3
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2204751986 - test_loss: 0.2069323593
Early Stop Left: 2
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2203299099 - test_loss: 0.2065058259
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2201103345 - test_loss: 0.2068881418
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2198084255 - test_loss: 0.2066628699
Early Stop Left: 3
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2195117187 - test_loss: 0.2062717332
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2193018697 - test_loss: 0.2063746199
Early Stop Left: 4
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2191299705 - test_loss: 0.2069618136
Early Stop Left: 3
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2190081771 - test_loss: 0.2062121224
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2188934353 - test_loss: 0.2067009785
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2187969510 - test_loss: 0.2060751677
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2187070972 - test_loss: 0.2060506960
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2186099723 - test_loss: 0.2065324298
Early Stop Left: 4
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2185410685 - test_loss: 0.2064898795
Early Stop Left: 3
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2184640278 - test_loss: 0.2071212085
Early Stop Left: 2
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2183893479 - test_loss: 0.2067631893
Early Stop Left: 1
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2183227207 - test_loss: 0.2068233411
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 18%|█▊        | 19/104 [00:00<00:00, 186.97it/s] 37%|███▋      | 38/104 [00:00<00:00, 185.75it/s] 55%|█████▍    | 57/104 [00:00<00:00, 185.70it/s] 73%|███████▎  | 76/104 [00:00<00:00, 185.75it/s] 91%|█████████▏| 95/104 [00:00<00:00, 184.49it/s]100%|██████████| 104/104 [00:00<00:00, 186.26it/s]
Best micro threshold=0.321964, fscore=0.551
p,r,f1: 0.4677979828790582 0.6706248009152714 0.551143096576519
throttleing by fixed threshold: 0.5
p,r,f1: 0.6330295823722506 0.36674941052072085 0.46442909448499403
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.321964293718338,
                 'p': 0.4677979828790582,
                 'r': 0.6706248009152714,
                 'f1': 0.551143096576519},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6330295823722506,
                 'r': 0.36674941052072085,
                 'f1': 0.46442909448499403}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5413693022 - test_loss: 0.6543835218
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4590461490 - test_loss: 0.5401152808
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3857980143 - test_loss: 0.4563142382
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3393564762 - test_loss: 0.4014813224
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3081399201 - test_loss: 0.3601884011
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2851083456 - test_loss: 0.3275943350
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2677483395 - test_loss: 0.3015478732
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2546674738 - test_loss: 0.2806954438
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2448962784 - test_loss: 0.2639966192
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2376931654 - test_loss: 0.2507015189
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2324677863 - test_loss: 0.2401354913
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2287528772 - test_loss: 0.2318669960
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2261682726 - test_loss: 0.2254311931
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2244102199 - test_loss: 0.2205632396
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2232387489 - test_loss: 0.2168414154
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2224853270 - test_loss: 0.2140964781
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2220150925 - test_loss: 0.2121663963
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2217204619 - test_loss: 0.2108219167
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2215472777 - test_loss: 0.2098758363
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2214327113 - test_loss: 0.2092408443
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2213696479 - test_loss: 0.2086613920
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2213143589 - test_loss: 0.2086028063
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2212903360 - test_loss: 0.2082049885
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2212643919 - test_loss: 0.2080937753
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2212444875 - test_loss: 0.2081340832
Early Stop Left: 4
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2212113400 - test_loss: 0.2080660733
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2211879752 - test_loss: 0.2079423626
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2211632496 - test_loss: 0.2077982431
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2211331084 - test_loss: 0.2078182829
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2211035469 - test_loss: 0.2077303446
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2210699893 - test_loss: 0.2075355393
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2210400846 - test_loss: 0.2075145770
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2209986241 - test_loss: 0.2076313905
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2209641181 - test_loss: 0.2070971822
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2209191396 - test_loss: 0.2073171902
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2208826746 - test_loss: 0.2068924387
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2208373518 - test_loss: 0.2070106514
Early Stop Left: 4
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2207856860 - test_loss: 0.2070835915
Early Stop Left: 3
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2207416979 - test_loss: 0.2067559563
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2206989877 - test_loss: 0.2068849104
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2206528725 - test_loss: 0.2068795517
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2206047092 - test_loss: 0.2067071376
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2205549940 - test_loss: 0.2066226156
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2205141139 - test_loss: 0.2066713208
Early Stop Left: 4
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2204551803 - test_loss: 0.2066138831
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2204094260 - test_loss: 0.2066156805
Early Stop Left: 4
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2203510559 - test_loss: 0.2063779971
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2202821523 - test_loss: 0.2064360731
Early Stop Left: 4
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2202160517 - test_loss: 0.2062481756
-------- Save Best Model! --------
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6083502428 - test_loss: 0.6539965874
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5139924782 - test_loss: 0.5387251371
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4286577442 - test_loss: 0.4541802552
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3733484647 - test_loss: 0.3983616843
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3350377015 - test_loss: 0.3559022113
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3058950978 - test_loss: 0.3220906653
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2832152684 - test_loss: 0.2948235288
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2655075184 - test_loss: 0.2727656836
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2517339605 - test_loss: 0.2548949999
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2410994020 - test_loss: 0.2404363949
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2329653546 - test_loss: 0.2287191236
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2268203938 - test_loss: 0.2192939660
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2222397662 - test_loss: 0.2117194171
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2188728854 - test_loss: 0.2057056450
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2164330576 - test_loss: 0.2008951868
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2147087504 - test_loss: 0.1971132297
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2135185189 - test_loss: 0.1942170888
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2127058917 - test_loss: 0.1919997019
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2121762289 - test_loss: 0.1903000706
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2118203876 - test_loss: 0.1890527490
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2116038783 - test_loss: 0.1880880965
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2114473968 - test_loss: 0.1875639736
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2113645940 - test_loss: 0.1869892037
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2112993247 - test_loss: 0.1867080534
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2112536315 - test_loss: 0.1865668790
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2111967936 - test_loss: 0.1864437137
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2111558810 - test_loss: 0.1862540308
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2111147074 - test_loss: 0.1861329210
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2110671148 - test_loss: 0.1860995284
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2110206689 - test_loss: 0.1859771631
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2109703453 - test_loss: 0.1858056332
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2109256828 - test_loss: 0.1857647861
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2108660819 - test_loss: 0.1858214800
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2108166012 - test_loss: 0.1854269944
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2107535819 - test_loss: 0.1855470118
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2107018118 - test_loss: 0.1852194130
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2106378305 - test_loss: 0.1852926841
Early Stop Left: 4
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2105652983 - test_loss: 0.1853393573
Early Stop Left: 3
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2105016274 - test_loss: 0.1851029135
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2104384945 - test_loss: 0.1851550255
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2103706116 - test_loss: 0.1851808409
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2102976005 - test_loss: 0.1850404956
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2102207619 - test_loss: 0.1850266726
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2101534908 - test_loss: 0.1850575469
Early Stop Left: 4
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2100605311 - test_loss: 0.1850529290
Early Stop Left: 3
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2099812220 - test_loss: 0.1850419181
Early Stop Left: 2
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2098803125 - test_loss: 0.1849861744
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2097597732 - test_loss: 0.1850201413
Early Stop Left: 4
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2096346436 - test_loss: 0.1849170234
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6418358917 - test_loss: 0.6538420022
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5414348495 - test_loss: 0.5381738188
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4500228378 - test_loss: 0.4533733912
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3902154165 - test_loss: 0.3971817333
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3482494130 - test_loss: 0.3542891165
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3159089929 - test_loss: 0.3200343137
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2903951496 - test_loss: 0.2923407572
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2701689702 - test_loss: 0.2698765192
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2541577994 - test_loss: 0.2516093852
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2415397912 - test_loss: 0.2367727327
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2316546415 - test_loss: 0.2246954481
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2239747641 - test_loss: 0.2149083407
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2180613598 - test_loss: 0.2069772717
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2135497437 - test_loss: 0.2005933466
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2101395274 - test_loss: 0.1954107329
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2076078570 - test_loss: 0.1912470867
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2057593000 - test_loss: 0.1879619191
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2044210347 - test_loss: 0.1853535928
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2034850589 - test_loss: 0.1832732379
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2028218290 - test_loss: 0.1816689755
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2023836518 - test_loss: 0.1804110146
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2020693081 - test_loss: 0.1795557699
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2018823621 - test_loss: 0.1787817865
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2017472757 - test_loss: 0.1782946528
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2016566114 - test_loss: 0.1779775865
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2015673330 - test_loss: 0.1777450772
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2015046290 - test_loss: 0.1774883442
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2014459657 - test_loss: 0.1773386682
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2013821961 - test_loss: 0.1772616550
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2013209602 - test_loss: 0.1771180072
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2012568393 - test_loss: 0.1769585314
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2012004131 - test_loss: 0.1769015292
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2011273789 - test_loss: 0.1769201178
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2010667429 - test_loss: 0.1766182416
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2009905811 - test_loss: 0.1766865600
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2009269131 - test_loss: 0.1764267257
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2008483166 - test_loss: 0.1764743779
Early Stop Left: 4
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2007589816 - test_loss: 0.1765054608
Early Stop Left: 3
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2006783318 - test_loss: 0.1763380644
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2005969667 - test_loss: 0.1763588384
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2005094904 - test_loss: 0.1764082585
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2004140015 - test_loss: 0.1763105772
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2003120465 - test_loss: 0.1763315910
Early Stop Left: 4
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2002189319 - test_loss: 0.1763769553
Early Stop Left: 3
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2000943250 - test_loss: 0.1763982118
Early Stop Left: 2
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.1999809947 - test_loss: 0.1763909330
Early Stop Left: 1
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.1998380220 - test_loss: 0.1764256950
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 192.35it/s] 38%|███▊      | 40/104 [00:00<00:00, 181.09it/s] 57%|█████▋    | 59/104 [00:00<00:00, 183.48it/s] 75%|███████▌  | 78/104 [00:00<00:00, 184.85it/s] 94%|█████████▍| 98/104 [00:00<00:00, 186.90it/s]100%|██████████| 104/104 [00:00<00:00, 187.21it/s]
Best micro threshold=0.284776, fscore=0.548
p,r,f1: 0.4748812620347209 0.6465876170712446 0.5475895931336008
throttleing by fixed threshold: 0.5
p,r,f1: 0.6577001016877236 0.33890299366887 0.4473125448572591
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.2847760319709778,
                 'p': 0.4748812620347209,
                 'r': 0.6465876170712446,
                 'f1': 0.5475895931336008},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6577001016877236,
                 'r': 0.33890299366887,
                 'f1': 0.4473125448572591}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5748614606 - test_loss: 0.6541725437
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4865303467 - test_loss: 0.5393566879
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4072530115 - test_loss: 0.4551463497
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3564034605 - test_loss: 0.3997673149
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3216810882 - test_loss: 0.3578335537
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2956475923 - test_loss: 0.3245659187
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2756915849 - test_loss: 0.2978428745
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2603709278 - test_loss: 0.2763179628
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2486827410 - test_loss: 0.2589595713
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2398581818 - test_loss: 0.2450072347
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2332803802 - test_loss: 0.2337880920
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2284567734 - test_loss: 0.2248646266
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2249811786 - test_loss: 0.2177937177
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2225231465 - test_loss: 0.2122996694
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2208161807 - test_loss: 0.2079912860
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2196673500 - test_loss: 0.2047033142
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2189159043 - test_loss: 0.2022828941
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2184278912 - test_loss: 0.2005125767
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2181281160 - test_loss: 0.1992144706
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2179313944 - test_loss: 0.1983088229
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2178185394 - test_loss: 0.1975788145
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2177306646 - test_loss: 0.1973254132
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2176885589 - test_loss: 0.1968699490
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2176498716 - test_loss: 0.1967026315
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2176212642 - test_loss: 0.1966696704
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2175789931 - test_loss: 0.1965895226
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2175486729 - test_loss: 0.1964348547
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2175172327 - test_loss: 0.1963085638
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2174797055 - test_loss: 0.1963031706
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2174429402 - test_loss: 0.1961977843
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2174020884 - test_loss: 0.1960142652
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2173657303 - test_loss: 0.1959826564
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2173161903 - test_loss: 0.1960734999
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2172750666 - test_loss: 0.1956026660
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2172220726 - test_loss: 0.1957751795
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2171789509 - test_loss: 0.1953901636
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2171255589 - test_loss: 0.1954880576
Early Stop Left: 4
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2170648707 - test_loss: 0.1955510038
Early Stop Left: 3
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2170125654 - test_loss: 0.1952602618
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2169612895 - test_loss: 0.1953491210
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2169060399 - test_loss: 0.1953575537
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2168474056 - test_loss: 0.1951957983
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2167863094 - test_loss: 0.1951454555
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2167344955 - test_loss: 0.1951784943
Early Stop Left: 4
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2166610705 - test_loss: 0.1951472147
Early Stop Left: 3
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2166013448 - test_loss: 0.1951395564
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2165250097 - test_loss: 0.1949873165
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2164343064 - test_loss: 0.1950301082
Early Stop Left: 4
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2163429959 - test_loss: 0.1948924692
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2201400612 - test_loss: 0.2063011854
Early Stop Left: 4
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2200374625 - test_loss: 0.2062294227
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2199166510 - test_loss: 0.2063096634
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2197749610 - test_loss: 0.2063700448
Early Stop Left: 3
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2196305206 - test_loss: 0.2062810323
Early Stop Left: 2
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2195048966 - test_loss: 0.2061852704
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2193933303 - test_loss: 0.2059724639
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2193008300 - test_loss: 0.2059430120
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2192126688 - test_loss: 0.2058215624
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2191345231 - test_loss: 0.2054733425
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2190712357 - test_loss: 0.2059979588
Early Stop Left: 4
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2190016518 - test_loss: 0.2058799939
Early Stop Left: 3
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2189503245 - test_loss: 0.2059383796
Early Stop Left: 2
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2189048506 - test_loss: 0.2060618595
Early Stop Left: 1
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2188542030 - test_loss: 0.2059507324
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 16%|█▋        | 17/104 [00:00<00:00, 169.08it/s] 34%|███▎      | 35/104 [00:00<00:00, 174.20it/s] 52%|█████▏    | 54/104 [00:00<00:00, 179.37it/s] 70%|███████   | 73/104 [00:00<00:00, 181.96it/s] 88%|████████▊ | 92/104 [00:00<00:00, 183.63it/s]100%|██████████| 104/104 [00:00<00:00, 182.97it/s]-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2094890338 - test_loss: 0.1848638182
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2093131745 - test_loss: 0.1848652187
Early Stop Left: 4
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2091374805 - test_loss: 0.1848762619
Early Stop Left: 3
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2089706721 - test_loss: 0.1848030888
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2088258996 - test_loss: 0.1846660695
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2087070055 - test_loss: 0.1845445003
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2085967122 - test_loss: 0.1844123639
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2085017645 - test_loss: 0.1844178623
Early Stop Left: 4
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2084042565 - test_loss: 0.1843794570
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2083140181 - test_loss: 0.1841174386
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2082373344 - test_loss: 0.1845631168
Early Stop Left: 4
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2081509308 - test_loss: 0.1845249605
Early Stop Left: 3
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2080858143 - test_loss: 0.1846510107
Early Stop Left: 2
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2080262510 - test_loss: 0.1847552568
Early Stop Left: 1
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2079594188 - test_loss: 0.1847010443
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 193.97it/s] 38%|███▊      | 40/104 [00:00<00:00, 194.11it/s] 58%|█████▊    | 60/104 [00:00<00:00, 193.59it/s] 77%|███████▋  | 80/104 [00:00<00:00, 194.14it/s] 96%|█████████▌| 100/104 [00:00<00:00, 193.12it/s]100%|██████████| 104/104 [00:00<00:00, 194.89it/s]
Best micro threshold=0.300802, fscore=0.551
p,r,f1: 0.47247455917629305 0.6604392021155285 0.5508640313918829
throttleing by fixed threshold: 0.5
p,r,f1: 0.6420024095893488 0.35554789059716935 0.4576463010338691
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.30080220103263855,
                 'p': 0.47247455917629305,
                 'r': 0.6604392021155285,
                 'f1': 0.5508640313918829},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6420024095893488,
                 'r': 0.35554789059716935,
                 'f1': 0.4576463010338691}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm

Best micro threshold=0.314323, fscore=0.552
p,r,f1: 0.47233231041961843 0.6628951301754962 0.5516194855853157
throttleing by fixed threshold: 0.5
p,r,f1: 0.6486681040234284 0.3481819912430992 0.4531364408494668
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.31432297825813293,
                 'p': 0.47233231041961843,
                 'r': 0.6628951301754962,
                 'f1': 0.5516194855853157},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6486681040234284,
                 'r': 0.3481819912430992,
                 'f1': 0.4531364408494668}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2162355676 - test_loss: 0.1949075697
Early Stop Left: 4
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2160952822 - test_loss: 0.1948839816
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2159399495 - test_loss: 0.1949594878
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2157778472 - test_loss: 0.1949434662
Early Stop Left: 3
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2156302533 - test_loss: 0.1947959385
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2155086621 - test_loss: 0.1946676157
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2153992769 - test_loss: 0.1944804451
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2153073195 - test_loss: 0.1944742164
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2152157376 - test_loss: 0.1943950483
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2151326039 - test_loss: 0.1940775571
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2150634839 - test_loss: 0.1945766070
Early Stop Left: 4
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2149862815 - test_loss: 0.1945016169
Early Stop Left: 3
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2149288075 - test_loss: 0.1945998927
Early Stop Left: 2
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2148767352 - test_loss: 0.1947166133
Early Stop Left: 1
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2148185753 - test_loss: 0.1946198447
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 14%|█▍        | 15/104 [00:00<00:00, 148.25it/s] 30%|██▉       | 31/104 [00:00<00:00, 149.67it/s] 44%|████▍     | 46/104 [00:00<00:00, 149.46it/s] 59%|█████▊    | 61/104 [00:00<00:00, 149.32it/s] 73%|███████▎  | 76/104 [00:00<00:00, 149.04it/s] 88%|████████▊ | 91/104 [00:00<00:00, 149.19it/s]100%|██████████| 104/104 [00:00<00:00, 148.94it/s]
Best micro threshold=0.307087, fscore=0.551
p,r,f1: 0.4725253128945675 0.661649260300102 0.5513190493578493
throttleing by fixed threshold: 0.5
p,r,f1: 0.6468426432649315 0.35065488273556084 0.45477512537171505
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.307087242603302,
                 'p': 0.4725253128945675,
                 'r': 0.661649260300102,
                 'f1': 0.5513190493578493},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6468426432649315,
                 'r': 0.35065488273556084,
                 'f1': 0.45477512537171505}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
Generation start
preprocessing_gen with context
b4 model prediction col0
 Index(['id', 'cycle', 'addr', 'ip', 'hit', 'raw', 'block_address',
       'page_address', 'page_offset', 'block_index', 'block_addr_delta',
       'patch', 'past', 'past_ip', 'past_page'],
      dtype='object')
predicting
  0%|          | 0/106 [00:00<?, ?it/s]  1%|          | 1/106 [00:01<02:51,  1.63s/it]  9%|▉         | 10/106 [00:01<00:12,  7.77it/s] 18%|█▊        | 19/106 [00:01<00:05, 16.07it/s] 26%|██▋       | 28/106 [00:01<00:03, 25.16it/s] 35%|███▍      | 37/106 [00:02<00:01, 34.57it/s] 43%|████▎     | 46/106 [00:02<00:01, 43.67it/s] 52%|█████▏    | 55/106 [00:02<00:00, 51.95it/s] 60%|██████    | 64/106 [00:02<00:00, 59.12it/s] 69%|██████▉   | 73/106 [00:02<00:00, 65.01it/s] 77%|███████▋  | 82/106 [00:02<00:00, 69.79it/s] 86%|████████▌ | 91/106 [00:02<00:00, 73.40it/s] 94%|█████████▍| 100/106 [00:02<00:00, 76.11it/s]100%|██████████| 106/106 [00:02<00:00, 36.63it/s]
after model prediction col1
 Index(['id', 'cycle', 'addr', 'ip', 'block_address', 'y_score'], dtype='object')
post_processing, opt_threshold<0.9
after delta filter
 Index(['id', 'pred_hex'], dtype='object')
               app  mean  max  min  median
0  437.leslie3d-s0   1.0  1.0  1.0     1.0
Done: results saved at: res/437.leslie3d-s0.vitt.pkl.degree_stats.csv
Generation start
preprocessing_gen with context
b4 model prediction col0
 Index(['id', 'cycle', 'addr', 'ip', 'hit', 'raw', 'block_address',
       'page_address', 'page_offset', 'block_index', 'block_addr_delta',
       'patch', 'past', 'past_ip', 'past_page'],
      dtype='object')
predicting
  0%|          | 0/106 [00:00<?, ?it/s]  1%|          | 1/106 [00:01<02:56,  1.68s/it] 24%|██▎       | 25/106 [00:01<00:04, 19.39it/s] 47%|████▋     | 50/106 [00:01<00:01, 42.49it/s] 71%|███████   | 75/106 [00:01<00:00, 68.42it/s] 93%|█████████▎| 99/106 [00:02<00:00, 93.86it/s]100%|██████████| 106/106 [00:02<00:00, 50.14it/s]
after model prediction col1
 Index(['id', 'cycle', 'addr', 'ip', 'block_address', 'y_score'], dtype='object')
post_processing, opt_threshold<0.9
after delta filter
 Index(['id', 'pred_hex'], dtype='object')
               app  mean  max  min  median
0  437.leslie3d-s0   1.0  1.0  1.0     1.0
Done: results saved at: res/437.leslie3d-s0.vit.stu.75.1.pkl.degree_stats.csv
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/437/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/437/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.184, 0.271
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0165, 0.00859
--- total mse / var(X): 0.14
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0159, 0.0161
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0161, 0.0159
--- total mse / var(X): 0.016
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.189, 0.182
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.135, 0.139
--- total mse / var(X): 0.161
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.132, 0.123
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.216, 0.23
--- total mse / var(X): 0.176
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.038, 0.00661
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0131, 0.0239
--- total mse / var(X): 0.0152
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.204, 0.168
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.234, 0.275
--- total mse / var(X): 0.222
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 7.94e-07, 6.05e-07
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 8.28e-07, 1.02e-06
--- total mse / var(X): 8.15e-07
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.165, 0.167
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0783, 0.0774
--- total mse / var(X): 0.122
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00191, 0.00171
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0734, 0.0812
--- total mse / var(X): 0.0414
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00076, 0.000457
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.000128, 0.00018
--- total mse / var(X): 0.000318
start table evaluation...
Elapsed time: 143.30267024040222 seconds
Cosine similarity between AMM and exact (Train): 0.9791228
Cosine similarity between AMM and exact (Test): 0.9793372
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.481230420393108 0.5744364841966778 0.523718816185726
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16],
               'cossim_layer_train': [0.991419792175293,
                                      0.97850102186203,
                                      0.9730565547943115,
                                      0.9791228771209717],
               'cossim_layer_test': [0.9873868823051453,
                                     0.9784481525421143,
                                     0.9727845191955566,
                                     0.9793371558189392],
               'cossim_amm_train': [0.9850297570228577,
                                    0.9924594163894653,
                                    0.9535022377967834,
                                    0.908700168132782,
                                    0.9515961408615112,
                                    0.9396005272865295,
                                    0.9596190452575684,
                                    0.9914041757583618],
               'cossim_amm_test': [0.9778163433074951,
                                   0.991725742816925,
                                   0.952557384967804,
                                   0.906773030757904,
                                   0.9500485062599182,
                                   0.9388520121574402,
                                   0.9589248895645142,
                                   0.9915526509284973],
               'f1': [0.481230420393108, 0.5744364841966778, 0.523718816185726],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 16),
                              (192, 2, 16),
                              (16, 16, 2),
                              (16, 16, 2),
                              (32, 2, 16),
                              (32, 2, 16),
                              (32, 2, 16),
                              (256, 2, 16)],
               'lut_total_size': 19456}}
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000002
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0661, 0.0973
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.000346, 0.000182
--- total mse / var(X): 0.0487
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0103, 0.0104
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00799, 0.00786
--- total mse / var(X): 0.00915
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00386, 0.00372
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00384, 0.00398
--- total mse / var(X): 0.00385
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00556, 0.00521
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00946, 0.0101
--- total mse / var(X): 0.00763
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00275, 0.000701
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.000792, 0.00138
--- total mse / var(X): 0.00104
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0314, 0.0259
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0418, 0.049
--- total mse / var(X): 0.0375
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00733, 0.00536
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00179, 0.00228
--- total mse / var(X): 0.00382
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00953, 0.00968
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00785, 0.00772
--- total mse / var(X): 0.0087
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0036, 0.00314
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00398, 0.00449
--- total mse / var(X): 0.00381
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00391, 0.00271
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00147, 0.00192
--- total mse / var(X): 0.00231
start table evaluation...
Elapsed time: 155.5989968776703 seconds
Cosine similarity between AMM and exact (Train): 0.98821485
Cosine similarity between AMM and exact (Test): 0.9882458
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.456559436736662 0.667450754213057 0.542220956440786
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64],
               'cossim_layer_train': [0.9968982338905334,
                                      0.9891536831855774,
                                      0.9857347011566162,
                                      0.9882148504257202],
               'cossim_layer_test': [0.9948887228965759,
                                     0.9893563389778137,
                                     0.985577404499054,
                                     0.9882457852363586],
               'cossim_amm_train': [0.994602620601654,
                                    0.9961360096931458,
                                    0.9946829676628113,
                                    0.9747483134269714,
                                    0.9726804494857788,
                                    0.9761015176773071,
                                    0.9813834428787231,
                                    0.9955530166625977],
               'cossim_amm_test': [0.9909282326698303,
                                   0.9950069785118103,
                                   0.9930561780929565,
                                   0.9744016528129578,
                                   0.9730263948440552,
                                   0.9746174216270447,
                                   0.9797830581665039,
                                   0.9954137206077576],
               'f1': [0.456559436736662, 0.667450754213057, 0.542220956440786],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 64),
                              (192, 2, 64),
                              (64, 64, 2),
                              (64, 64, 2),
                              (32, 2, 64),
                              (32, 2, 64),
                              (32, 2, 64),
                              (256, 2, 64)],
               'lut_total_size': 90112}}
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999994
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0189, 0.0278
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.000773, 0.000406
--- total mse / var(X): 0.0141
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00327, 0.00332
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00292, 0.00287
--- total mse / var(X): 0.00309
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0021, 0.00202
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00247, 0.00256
--- total mse / var(X): 0.00229
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00283, 0.00265
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00252, 0.00268
--- total mse / var(X): 0.00266
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00191, 0.000491
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.000488, 0.000851
--- total mse / var(X): 0.000671
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00398, 0.00328
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00565, 0.00664
--- total mse / var(X): 0.00496
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0159, 0.0123
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00873, 0.0107
--- total mse / var(X): 0.0115
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00497, 0.00498
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00514, 0.00514
--- total mse / var(X): 0.00506
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00261, 0.0023
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00261, 0.00292
--- total mse / var(X): 0.00261
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00343, 0.00251
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00155, 0.00196
--- total mse / var(X): 0.00223
start table evaluation...
Elapsed time: 209.74683332443237 seconds
Cosine similarity between AMM and exact (Train): 0.9935402
Cosine similarity between AMM and exact (Test): 0.9933111
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4572175007143123 0.6725906742587451 0.5443759992189312
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256],
               'cossim_layer_train': [0.9990865588188171,
                                      0.9978952407836914,
                                      0.997070848941803,
                                      0.9935402274131775],
               'cossim_layer_test': [0.9971669316291809,
                                     0.997300386428833,
                                     0.9963747262954712,
                                     0.9933111071586609],
               'cossim_amm_train': [0.9984104633331299,
                                    0.9987253546714783,
                                    0.998102068901062,
                                    0.9954813718795776,
                                    0.9948530197143555,
                                    0.9945569634437561,
                                    0.9953650236129761,
                                    0.9976763725280762],
               'cossim_amm_test': [0.9949666857719421,
                                   0.9975481629371643,
                                   0.9964181184768677,
                                   0.9936527609825134,
                                   0.9937084913253784,
                                   0.9933972358703613,
                                   0.9942874908447266,
                                   0.9973754286766052],
               'f1': [0.4572175007143123,
                      0.6725906742587451,
                      0.5443759992189312],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 256),
                              (192, 2, 256),
                              (256, 256, 2),
                              (256, 256, 2),
                              (32, 2, 256),
                              (32, 2, 256),
                              (32, 2, 256),
                              (256, 2, 256)],
               'lut_total_size': 557056}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00782, 0.0113
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 1.1e-05, 1.16e-05
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00296, 0.00443
running kmeans in subspace 4/4... X.shape:  (100000, 3)
k:  128
nnz_rows:  0
mse / {var(X_subs), var(X)}: 0, 0
--- total mse / var(X): 0.00394
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00464, 0.005
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00607, 0.00581
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00417, 0.00403
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00453, 0.00453
--- total mse / var(X): 0.00484
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00356, 0.00385
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0034, 0.00287
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00375, 0.00416
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00326, 0.00314
--- total mse / var(X): 0.00351
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00454, 0.00396
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00423, 0.00424
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00351, 0.0041
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00317, 0.00303
--- total mse / var(X): 0.00383
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00155, 0.000524
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0016, 0.000281
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0024, 0.000389
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000309, 0.00103
--- total mse / var(X): 0.000555
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00592, 0.00525
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00372, 0.00283
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00642, 0.00753
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00735, 0.00867
--- total mse / var(X): 0.00607
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0169, 0.0156
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0204, 0.0121
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0107, 0.013
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0104, 0.0132
--- total mse / var(X): 0.0135
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00677, 0.00663
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00727, 0.00746
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00638, 0.00706
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00736, 0.00654
--- total mse / var(X): 0.00692
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00244, 0.00241
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0036, 0.00277
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00314, 0.0047
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00422, 0.00314
--- total mse / var(X): 0.00325
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0338, 0.0215
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.028, 0.0231
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0151, 0.0253
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0251, 0.0217
--- total mse / var(X): 0.0229
start table evaluation...
Elapsed time: 243.36092066764832 seconds
Cosine similarity between AMM and exact (Train): 0.9918608
Cosine similarity between AMM and exact (Test): 0.99142426
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.45041956317607706 0.6816981526822014 0.5424350839987971
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.999758243560791,
                                      0.9971522092819214,
                                      0.9959498047828674,
                                      0.9918608069419861],
               'cossim_layer_test': [0.9991098642349243,
                                     0.9969096779823303,
                                     0.9956226944923401,
                                     0.9914242625236511],
               'cossim_amm_train': [0.9995793104171753,
                                    0.9983547925949097,
                                    0.9974685907363892,
                                    0.9937498569488525,
                                    0.9926177859306335,
                                    0.9922909140586853,
                                    0.9934727549552917,
                                    0.9970407485961914],
               'cossim_amm_test': [0.9984195232391357,
                                   0.9973575472831726,
                                   0.995969295501709,
                                   0.99221271276474,
                                   0.9918105006217957,
                                   0.9915139675140381,
                                   0.992766261100769,
                                   0.9967265725135803],
               'f1': [0.45041956317607706,
                      0.6816981526822014,
                      0.5424350839987971],
               'lut_num': 8,
               'lut_shapes': [(32, 4, 128),
                              (192, 4, 128),
                              (128, 128, 4),
                              (128, 128, 4),
                              (32, 4, 128),
                              (32, 4, 128),
                              (32, 4, 128),
                              (256, 4, 128)],
               'lut_total_size': 425984}}
/data/neelesh/DART_by_app/437/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (32). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (32). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999994
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00613, 0.00904
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 1.69e-05, 8.85e-06
--- total mse / var(X): 0.00453
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00107, 0.00108
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00104, 0.00102
--- total mse / var(X): 0.00105
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000823, 0.000793
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000796, 0.000824
--- total mse / var(X): 0.000809
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.001, 0.000938
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000933, 0.000992
--- total mse / var(X): 0.000965
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00133, 0.000343
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000309, 0.000538
--- total mse / var(X): 0.000441
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00188, 0.00155
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00299, 0.00352
--- total mse / var(X): 0.00253
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0113, 0.00888
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00682, 0.00827
--- total mse / var(X): 0.00858
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00242, 0.00242
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00269, 0.00269
--- total mse / var(X): 0.00255
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00133, 0.00118
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00147, 0.00164
--- total mse / var(X): 0.00141
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0024, 0.00196
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00184, 0.00218
--- total mse / var(X): 0.00207
start table evaluation...
Elapsed time: 404.9725503921509 seconds
Cosine similarity between AMM and exact (Train): 0.9971841
Cosine similarity between AMM and exact (Test): 0.99605554
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4637515348425625 0.6677372477396226 0.5473570366364743
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024],
               'cossim_layer_train': [0.9996858835220337,
                                      0.9993245601654053,
                                      0.9990249872207642,
                                      0.9971840977668762],
               'cossim_layer_test': [0.997848391532898,
                                     0.9985202550888062,
                                     0.9979953169822693,
                                     0.9960556626319885],
               'cossim_amm_train': [0.9994527101516724,
                                    0.99956214427948,
                                    0.9993343949317932,
                                    0.9986280798912048,
                                    0.9983522295951843,
                                    0.9980485439300537,
                                    0.9982872009277344,
                                    0.9988349080085754],
               'cossim_amm_test': [0.996178388595581,
                                   0.9987626671791077,
                                   0.9981913566589355,
                                   0.9971542954444885,
                                   0.9968301057815552,
                                   0.9963769912719727,
                                   0.9968456029891968,
                                   0.9983238577842712],
               'f1': [0.4637515348425625,
                      0.6677372477396226,
                      0.5473570366364743],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 1024),
                              (192, 2, 1024),
                              (1024, 1024, 2),
                              (1024, 1024, 2),
                              (32, 2, 1024),
                              (32, 2, 1024),
                              (32, 2, 1024),
                              (256, 2, 1024)],
               'lut_total_size': 5373952}}
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999964
Manual and Torch results cosine similarity (Test): 1.0000004
start table training with fine tuning...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0347, 0.0513
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000557, 0.000289
--- total mse / var(X): 0.0258
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00592, 0.00601
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00545, 0.00536
--- total mse / var(X): 0.00569
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00313, 0.00301
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00338, 0.00351
--- total mse / var(X): 0.00326
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00402, 0.00377
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00349, 0.00371
--- total mse / var(X): 0.00374
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00228, 0.000585
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00076, 0.00132
--- total mse / var(X): 0.000955
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00769, 0.00634
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0102, 0.0119
--- total mse / var(X): 0.00913
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0127, 0.00959
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00681, 0.00848
--- total mse / var(X): 0.00903
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00707, 0.00709
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00635, 0.00633
--- total mse / var(X): 0.00671
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00299, 0.00263
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00301, 0.00337
--- total mse / var(X): 0.003
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0107, 0.00547
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00357, 0.00532
--- total mse / var(X): 0.00539
start table evaluation...
Elapsed time: 121.47670936584473 seconds
Cosine similarity between AMM and exact (Train): 0.990152
Cosine similarity between AMM and exact (Test): 0.98983854
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4431926075098121 0.6906473057357135 0.5399171024461114
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9982587695121765,
                                      0.9946882724761963,
                                      0.9927372932434082,
                                      0.9901520013809204],
               'cossim_layer_test': [0.996036946773529,
                                     0.9943792223930359,
                                     0.9923412799835205,
                                     0.9898385405540466],
               'cossim_amm_train': [0.9969629049301147,
                                    0.9976957440376282,
                                    0.9967861175537109,
                                    0.988722026348114,
                                    0.98676598072052,
                                    0.9870343208312988,
                                    0.989870548248291,
                                    0.9964269399642944],
               'cossim_amm_test': [0.9929720759391785,
                                   0.9963288307189941,
                                   0.9950604438781738,
                                   0.9878477454185486,
                                   0.9863370060920715,
                                   0.98602694272995,
                                   0.9891649484634399,
                                   0.9961568117141724],
               'f1': [0.4431926075098121,
                      0.6906473057357135,
                      0.5399171024461114],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Traceback (most recent call last):
  File "src/3_2_vit_finetune.py", line 110, in <module>
    train_data, train_target, test_data, test_target, all_params, best_threshold = load_data_n_model(model_save_path, res_path)
  File "src/3_2_vit_finetune.py", line 55, in load_data_n_model
    with open(model_save_path+'.tensor_dict.pkl', 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'model/410.bwaves-s0.vit.stu.75.1.pkl.tensor_dict.pkl'
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999946
Manual and Torch results cosine similarity (Test): 1.0000004
start table training with fine tuning...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0355, 0.0525
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000775, 0.000405
--- total mse / var(X): 0.0265
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00591, 0.00601
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00508, 0.00499
--- total mse / var(X): 0.0055
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00334, 0.00322
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00285, 0.00295
--- total mse / var(X): 0.00308
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00387, 0.00363
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00311, 0.0033
--- total mse / var(X): 0.00347
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00255, 0.000653
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000688, 0.0012
--- total mse / var(X): 0.000926
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00776, 0.0064
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0126, 0.0148
--- total mse / var(X): 0.0106
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0163, 0.0126
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00726, 0.00892
--- total mse / var(X): 0.0108
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00651, 0.00654
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00655, 0.00652
--- total mse / var(X): 0.00653
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00267, 0.00233
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00354, 0.00398
--- total mse / var(X): 0.00316
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00369, 0.00258
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0021, 0.00273
--- total mse / var(X): 0.00266
start table evaluation...
Elapsed time: 102.98760724067688 seconds
Cosine similarity between AMM and exact (Train): 0.99030364
Cosine similarity between AMM and exact (Test): 0.9899327
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4496114573399829 0.6807123887717156 0.5415369943717664
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.998221755027771,
                                      0.9946091175079346,
                                      0.992769181728363,
                                      0.990303635597229],
               'cossim_layer_test': [0.9959918260574341,
                                     0.9943427443504333,
                                     0.992315948009491,
                                     0.9899327158927917],
               'cossim_amm_train': [0.9969034194946289,
                                    0.9977518320083618,
                                    0.9971110224723816,
                                    0.9883835911750793,
                                    0.9866308569908142,
                                    0.9871513843536377,
                                    0.9901368618011475,
                                    0.9965478181838989],
               'cossim_amm_test': [0.9928815364837646,
                                   0.9962530136108398,
                                   0.9953227639198303,
                                   0.9869545698165894,
                                   0.9862242341041565,
                                   0.9859871864318848,
                                   0.9892231225967407,
                                   0.9961971044540405],
               'f1': [0.4496114573399829,
                      0.6807123887717156,
                      0.5415369943717664],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999995
Manual and Torch results cosine similarity (Test): 1.0000004
start table training with fine tuning...
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
Retrain for 1 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0345, 0.051
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000288, 0.00015
--- total mse / var(X): 0.0256
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:03<05:49,  3.53s/it]  2%|▏         | 2/100 [00:07<05:54,  3.62s/it]  3%|▎         | 3/100 [00:11<06:02,  3.74s/it]  4%|▍         | 4/100 [00:15<06:28,  4.05s/it]  5%|▌         | 5/100 [00:19<06:28,  4.09s/it]  6%|▌         | 6/100 [00:24<06:29,  4.14s/it]  7%|▋         | 7/100 [00:28<06:28,  4.17s/it]  8%|▊         | 8/100 [00:32<06:14,  4.07s/it]  9%|▉         | 9/100 [00:35<05:53,  3.89s/it] 10%|█         | 10/100 [00:39<05:41,  3.80s/it] 11%|█         | 11/100 [00:43<05:41,  3.84s/it] 12%|█▏        | 12/100 [00:47<05:41,  3.88s/it] 13%|█▎        | 13/100 [00:51<05:42,  3.94s/it] 14%|█▍        | 14/100 [00:55<05:40,  3.95s/it] 15%|█▌        | 15/100 [00:59<05:40,  4.01s/it] 16%|█▌        | 16/100 [01:03<05:42,  4.08s/it] 17%|█▋        | 17/100 [01:07<05:37,  4.07s/it] 18%|█▊        | 18/100 [01:11<05:33,  4.07s/it] 19%|█▉        | 19/100 [01:15<05:29,  4.07s/it] 20%|██        | 20/100 [01:19<05:21,  4.02s/it] 21%|██        | 21/100 [01:23<05:07,  3.89s/it] 22%|██▏       | 22/100 [01:26<04:50,  3.72s/it] 23%|██▎       | 23/100 [01:29<04:34,  3.56s/it] 24%|██▍       | 24/100 [01:32<04:23,  3.47s/it] 25%|██▌       | 25/100 [01:36<04:16,  3.42s/it] 26%|██▌       | 26/100 [01:39<04:11,  3.39s/it] 27%|██▋       | 27/100 [01:42<04:06,  3.38s/it] 28%|██▊       | 28/100 [01:46<04:00,  3.34s/it] 29%|██▉       | 29/100 [01:49<03:55,  3.32s/it] 30%|███       | 30/100 [01:52<03:52,  3.33s/it] 31%|███       | 31/100 [01:56<03:56,  3.42s/it] 32%|███▏      | 32/100 [02:00<04:03,  3.58s/it] 33%|███▎      | 33/100 [02:04<04:06,  3.68s/it] 34%|███▍      | 34/100 [02:07<03:59,  3.64s/it] 35%|███▌      | 35/100 [02:11<03:53,  3.60s/it] 36%|███▌      | 36/100 [02:14<03:48,  3.58s/it] 37%|███▋      | 37/100 [02:18<03:45,  3.57s/it] 38%|███▊      | 38/100 [02:21<03:39,  3.53s/it] 39%|███▉      | 39/100 [02:25<03:36,  3.55s/it] 40%|████      | 40/100 [02:28<03:30,  3.51s/it] 41%|████      | 41/100 [02:32<03:25,  3.49s/it] 42%|████▏     | 42/100 [02:36<03:29,  3.61s/it] 43%|████▎     | 43/100 [02:39<03:27,  3.64s/it] 44%|████▍     | 44/100 [02:43<03:21,  3.60s/it] 45%|████▌     | 45/100 [02:47<03:18,  3.61s/it] 46%|████▌     | 46/100 [02:50<03:17,  3.66s/it] 47%|████▋     | 47/100 [02:54<03:21,  3.81s/it] 48%|████▊     | 48/100 [02:58<03:17,  3.80s/it] 49%|████▉     | 49/100 [03:02<03:18,  3.89s/it] 50%|█████     | 50/100 [03:06<03:13,  3.86s/it] 51%|█████     | 51/100 [03:10<03:10,  3.88s/it] 52%|█████▏    | 52/100 [03:14<03:04,  3.84s/it] 53%|█████▎    | 53/100 [03:18<03:03,  3.90s/it] 54%|█████▍    | 54/100 [03:22<03:00,  3.93s/it] 55%|█████▌    | 55/100 [03:25<02:51,  3.82s/it] 56%|█████▌    | 56/100 [03:29<02:39,  3.62s/it] 57%|█████▋    | 57/100 [03:33<02:40,  3.72s/it] 58%|█████▊    | 58/100 [03:37<02:39,  3.80s/it] 59%|█████▉    | 59/100 [03:40<02:31,  3.71s/it] 60%|██████    | 60/100 [03:44<02:28,  3.71s/it] 61%|██████    | 61/100 [03:48<02:29,  3.84s/it] 62%|██████▏   | 62/100 [03:52<02:24,  3.81s/it] 63%|██████▎   | 63/100 [03:55<02:19,  3.76s/it] 64%|██████▍   | 64/100 [03:59<02:15,  3.77s/it] 65%|██████▌   | 65/100 [04:03<02:09,  3.70s/it] 66%|██████▌   | 66/100 [04:07<02:08,  3.77s/it] 67%|██████▋   | 67/100 [04:10<02:00,  3.66s/it] 68%|██████▊   | 68/100 [04:13<01:55,  3.62s/it] 69%|██████▉   | 69/100 [04:17<01:50,  3.56s/it] 70%|███████   | 70/100 [04:21<01:49,  3.66s/it] 71%|███████   | 71/100 [04:24<01:45,  3.63s/it] 72%|███████▏  | 72/100 [04:28<01:43,  3.71s/it] 73%|███████▎  | 73/100 [04:32<01:41,  3.76s/it] 74%|███████▍  | 74/100 [04:36<01:38,  3.79s/it] 75%|███████▌  | 75/100 [04:40<01:35,  3.82s/it] 76%|███████▌  | 76/100 [04:44<01:30,  3.78s/it] 77%|███████▋  | 77/100 [04:47<01:24,  3.67s/it] 78%|███████▊  | 78/100 [04:51<01:21,  3.69s/it] 79%|███████▉  | 79/100 [04:55<01:19,  3.79s/it] 80%|████████  | 80/100 [04:59<01:18,  3.91s/it] 81%|████████  | 81/100 [05:02<01:11,  3.79s/it] 82%|████████▏ | 82/100 [05:06<01:07,  3.72s/it] 83%|████████▎ | 83/100 [05:10<01:04,  3.79s/it] 84%|████████▍ | 84/100 [05:13<00:59,  3.72s/it] 85%|████████▌ | 85/100 [05:17<00:54,  3.66s/it] 86%|████████▌ | 86/100 [05:21<00:52,  3.75s/it] 87%|████████▋ | 87/100 [05:25<00:48,  3.70s/it] 88%|████████▊ | 88/100 [05:28<00:43,  3.66s/it] 89%|████████▉ | 89/100 [05:31<00:39,  3.57s/it] 90%|█████████ | 90/100 [05:35<00:35,  3.54s/it] 91%|█████████ | 91/100 [05:38<00:31,  3.51s/it] 92%|█████████▏| 92/100 [05:42<00:27,  3.45s/it] 93%|█████████▎| 93/100 [05:45<00:23,  3.41s/it] 94%|█████████▍| 94/100 [05:49<00:20,  3.44s/it] 95%|█████████▌| 95/100 [05:52<00:17,  3.40s/it] 96%|█████████▌| 96/100 [05:56<00:13,  3.48s/it] 97%|█████████▋| 97/100 [05:59<00:10,  3.53s/it] 98%|█████████▊| 98/100 [06:03<00:07,  3.59s/it] 99%|█████████▉| 99/100 [06:06<00:03,  3.57s/it]100%|██████████| 100/100 [06:10<00:00,  3.56s/it]100%|██████████| 100/100 [06:10<00:00,  3.70s/it]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0058, 0.00589
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00517, 0.00508
--- total mse / var(X): 0.00549
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00317, 0.00306
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00322, 0.00333
--- total mse / var(X): 0.0032
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00381, 0.00357
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0033, 0.00351
--- total mse / var(X): 0.00354
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00233, 0.000597
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00077, 0.00134
--- total mse / var(X): 0.00097
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00631, 0.0052
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.018, 0.0211
--- total mse / var(X): 0.0132
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:22,  4.40it/s]  2%|▏         | 2/100 [00:00<00:20,  4.82it/s]  3%|▎         | 3/100 [00:00<00:22,  4.33it/s]  4%|▍         | 4/100 [00:00<00:22,  4.28it/s]  5%|▌         | 5/100 [00:01<00:22,  4.28it/s]  6%|▌         | 6/100 [00:01<00:21,  4.44it/s]  7%|▋         | 7/100 [00:01<00:21,  4.24it/s]  8%|▊         | 8/100 [00:01<00:20,  4.53it/s]  9%|▉         | 9/100 [00:01<00:19,  4.72it/s] 10%|█         | 10/100 [00:02<00:19,  4.60it/s] 11%|█         | 11/100 [00:02<00:18,  4.69it/s] 12%|█▏        | 12/100 [00:02<00:18,  4.83it/s] 13%|█▎        | 13/100 [00:02<00:18,  4.80it/s] 14%|█▍        | 14/100 [00:03<00:17,  5.00it/s] 15%|█▌        | 15/100 [00:03<00:16,  5.10it/s] 16%|█▌        | 16/100 [00:03<00:16,  5.13it/s] 17%|█▋        | 17/100 [00:03<00:16,  4.94it/s] 18%|█▊        | 18/100 [00:03<00:16,  5.10it/s] 19%|█▉        | 19/100 [00:03<00:15,  5.20it/s] 20%|██        | 20/100 [00:04<00:15,  5.30it/s] 21%|██        | 21/100 [00:04<00:14,  5.29it/s] 22%|██▏       | 22/100 [00:04<00:14,  5.51it/s] 23%|██▎       | 23/100 [00:04<00:14,  5.46it/s] 24%|██▍       | 24/100 [00:04<00:13,  5.67it/s] 25%|██▌       | 25/100 [00:05<00:13,  5.45it/s] 26%|██▌       | 26/100 [00:05<00:13,  5.43it/s] 27%|██▋       | 27/100 [00:05<00:13,  5.37it/s] 28%|██▊       | 28/100 [00:05<00:13,  5.40it/s] 29%|██▉       | 29/100 [00:05<00:13,  5.36it/s] 30%|███       | 30/100 [00:05<00:13,  5.36it/s] 31%|███       | 31/100 [00:06<00:12,  5.42it/s] 32%|███▏      | 32/100 [00:06<00:12,  5.55it/s] 33%|███▎      | 33/100 [00:06<00:11,  5.66it/s] 34%|███▍      | 34/100 [00:06<00:11,  5.82it/s] 35%|███▌      | 35/100 [00:06<00:13,  4.79it/s] 36%|███▌      | 36/100 [00:07<00:17,  3.71it/s] 37%|███▋      | 37/100 [00:07<00:15,  3.95it/s] 38%|███▊      | 38/100 [00:07<00:13,  4.56it/s] 39%|███▉      | 39/100 [00:07<00:12,  5.00it/s] 40%|████      | 40/100 [00:08<00:10,  5.53it/s] 41%|████      | 41/100 [00:08<00:10,  5.64it/s] 42%|████▏     | 42/100 [00:08<00:09,  5.97it/s] 43%|████▎     | 43/100 [00:08<00:09,  6.14it/s] 44%|████▍     | 44/100 [00:08<00:09,  6.11it/s] 45%|████▌     | 45/100 [00:08<00:08,  6.27it/s] 46%|████▌     | 46/100 [00:08<00:08,  6.28it/s] 47%|████▋     | 47/100 [00:09<00:08,  6.52it/s] 48%|████▊     | 48/100 [00:09<00:07,  6.50it/s] 49%|████▉     | 49/100 [00:09<00:07,  6.58it/s] 50%|█████     | 50/100 [00:09<00:07,  6.88it/s] 51%|█████     | 51/100 [00:09<00:07,  6.99it/s] 52%|█████▏    | 52/100 [00:09<00:06,  6.98it/s] 53%|█████▎    | 53/100 [00:09<00:06,  7.07it/s] 54%|█████▍    | 54/100 [00:10<00:06,  7.01it/s] 55%|█████▌    | 55/100 [00:10<00:06,  7.02it/s] 56%|█████▌    | 56/100 [00:10<00:06,  6.44it/s] 57%|█████▋    | 57/100 [00:10<00:06,  6.35it/s] 58%|█████▊    | 58/100 [00:10<00:07,  5.90it/s] 59%|█████▉    | 59/100 [00:10<00:06,  5.99it/s] 60%|██████    | 60/100 [00:11<00:06,  5.86it/s] 61%|██████    | 61/100 [00:11<00:06,  5.73it/s] 62%|██████▏   | 62/100 [00:11<00:06,  5.55it/s] 63%|██████▎   | 63/100 [00:11<00:06,  5.76it/s] 64%|██████▍   | 64/100 [00:11<00:06,  5.48it/s] 65%|██████▌   | 65/100 [00:12<00:07,  4.86it/s] 66%|██████▌   | 66/100 [00:12<00:07,  4.49it/s] 67%|██████▋   | 67/100 [00:12<00:07,  4.47it/s] 68%|██████▊   | 68/100 [00:12<00:07,  4.13it/s] 69%|██████▉   | 69/100 [00:13<00:07,  4.26it/s] 70%|███████   | 70/100 [00:13<00:06,  4.59it/s] 71%|███████   | 71/100 [00:13<00:06,  4.60it/s] 72%|███████▏  | 72/100 [00:13<00:05,  4.81it/s] 73%|███████▎  | 73/100 [00:13<00:05,  4.91it/s] 74%|███████▍  | 74/100 [00:14<00:05,  5.15it/s] 75%|███████▌  | 75/100 [00:14<00:05,  4.82it/s] 76%|███████▌  | 76/100 [00:14<00:04,  5.41it/s] 77%|███████▋  | 77/100 [00:14<00:03,  5.76it/s] 78%|███████▊  | 78/100 [00:14<00:03,  5.84it/s] 79%|███████▉  | 79/100 [00:14<00:03,  5.92it/s] 80%|████████  | 80/100 [00:15<00:03,  6.39it/s] 81%|████████  | 81/100 [00:15<00:02,  7.13it/s] 82%|████████▏ | 82/100 [00:15<00:02,  7.76it/s] 84%|████████▍ | 84/100 [00:15<00:01,  8.69it/s] 86%|████████▌ | 86/100 [00:15<00:01,  8.53it/s] 87%|████████▋ | 87/100 [00:15<00:01,  8.67it/s] 88%|████████▊ | 88/100 [00:15<00:01,  8.73it/s] 89%|████████▉ | 89/100 [00:16<00:01,  8.35it/s] 90%|█████████ | 90/100 [00:16<00:01,  8.68it/s] 91%|█████████ | 91/100 [00:16<00:01,  8.77it/s] 92%|█████████▏| 92/100 [00:16<00:00,  8.98it/s] 93%|█████████▎| 93/100 [00:16<00:00,  8.93it/s] 94%|█████████▍| 94/100 [00:16<00:00,  8.82it/s] 95%|█████████▌| 95/100 [00:16<00:00,  9.08it/s] 97%|█████████▋| 97/100 [00:16<00:00,  9.29it/s] 98%|█████████▊| 98/100 [00:17<00:00,  9.41it/s] 99%|█████████▉| 99/100 [00:17<00:00,  9.50it/s]100%|██████████| 100/100 [00:17<00:00,  9.14it/s]100%|██████████| 100/100 [00:17<00:00,  5.80it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0125, 0.00946
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00781, 0.0097
--- total mse / var(X): 0.00958
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:11,  8.77it/s]  2%|▏         | 2/100 [00:00<00:12,  7.76it/s]  3%|▎         | 3/100 [00:00<00:12,  7.50it/s]  4%|▍         | 4/100 [00:00<00:12,  7.71it/s]  5%|▌         | 5/100 [00:00<00:13,  7.28it/s]  6%|▌         | 6/100 [00:00<00:12,  7.73it/s]  7%|▋         | 7/100 [00:00<00:11,  7.93it/s]  8%|▊         | 8/100 [00:01<00:11,  7.87it/s]  9%|▉         | 9/100 [00:01<00:12,  7.38it/s] 10%|█         | 10/100 [00:01<00:11,  7.51it/s] 11%|█         | 11/100 [00:01<00:11,  7.58it/s] 12%|█▏        | 12/100 [00:01<00:11,  7.58it/s] 13%|█▎        | 13/100 [00:01<00:11,  7.58it/s] 14%|█▍        | 14/100 [00:01<00:11,  7.22it/s] 15%|█▌        | 15/100 [00:01<00:11,  7.46it/s] 16%|█▌        | 16/100 [00:02<00:10,  7.79it/s] 17%|█▋        | 17/100 [00:02<00:10,  7.95it/s] 18%|█▊        | 18/100 [00:02<00:10,  7.86it/s] 19%|█▉        | 19/100 [00:02<00:10,  7.49it/s] 20%|██        | 20/100 [00:02<00:10,  7.40it/s] 21%|██        | 21/100 [00:02<00:10,  7.78it/s] 22%|██▏       | 22/100 [00:02<00:09,  8.14it/s] 23%|██▎       | 23/100 [00:02<00:09,  8.29it/s] 24%|██▍       | 24/100 [00:03<00:09,  8.38it/s] 25%|██▌       | 25/100 [00:03<00:09,  8.21it/s] 26%|██▌       | 26/100 [00:03<00:09,  7.55it/s] 27%|██▋       | 27/100 [00:03<00:09,  7.54it/s] 28%|██▊       | 28/100 [00:03<00:09,  7.44it/s] 29%|██▉       | 29/100 [00:03<00:09,  7.77it/s] 30%|███       | 30/100 [00:03<00:09,  7.54it/s] 31%|███       | 31/100 [00:04<00:08,  7.67it/s] 32%|███▏      | 32/100 [00:04<00:09,  7.49it/s] 33%|███▎      | 33/100 [00:04<00:09,  7.22it/s] 34%|███▍      | 34/100 [00:04<00:09,  7.17it/s] 35%|███▌      | 35/100 [00:04<00:09,  7.17it/s] 36%|███▌      | 36/100 [00:04<00:08,  7.22it/s] 37%|███▋      | 37/100 [00:04<00:08,  7.14it/s] 38%|███▊      | 38/100 [00:05<00:08,  7.31it/s] 39%|███▉      | 39/100 [00:05<00:08,  7.40it/s] 40%|████      | 40/100 [00:05<00:07,  7.62it/s] 41%|████      | 41/100 [00:05<00:07,  7.71it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.69it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.40it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.09it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.27it/s] 47%|████▋     | 47/100 [00:06<00:06,  8.20it/s] 48%|████▊     | 48/100 [00:06<00:06,  8.42it/s] 49%|████▉     | 49/100 [00:06<00:05,  8.53it/s] 51%|█████     | 51/100 [00:06<00:05,  9.24it/s] 52%|█████▏    | 52/100 [00:06<00:05,  9.25it/s] 53%|█████▎    | 53/100 [00:06<00:05,  9.24it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.48it/s] 57%|█████▋    | 57/100 [00:07<00:04,  9.90it/s] 58%|█████▊    | 58/100 [00:07<00:04,  9.87it/s] 60%|██████    | 60/100 [00:07<00:03, 10.28it/s] 62%|██████▏   | 62/100 [00:07<00:03, 10.45it/s] 64%|██████▍   | 64/100 [00:07<00:03, 10.63it/s] 66%|██████▌   | 66/100 [00:07<00:03, 10.29it/s] 68%|██████▊   | 68/100 [00:08<00:03,  9.96it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.81it/s] 70%|███████   | 70/100 [00:08<00:03,  9.51it/s] 71%|███████   | 71/100 [00:08<00:03,  9.30it/s] 72%|███████▏  | 72/100 [00:08<00:02,  9.44it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.26it/s] 74%|███████▍  | 74/100 [00:08<00:02,  8.84it/s] 75%|███████▌  | 75/100 [00:09<00:02,  8.45it/s] 76%|███████▌  | 76/100 [00:09<00:02,  8.73it/s] 77%|███████▋  | 77/100 [00:09<00:02,  8.88it/s] 78%|███████▊  | 78/100 [00:09<00:02,  8.23it/s] 79%|███████▉  | 79/100 [00:09<00:02,  8.41it/s] 80%|████████  | 80/100 [00:09<00:02,  8.79it/s] 81%|████████  | 81/100 [00:09<00:02,  8.94it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.12it/s] 85%|████████▌ | 85/100 [00:10<00:01,  9.62it/s] 86%|████████▌ | 86/100 [00:10<00:01,  9.69it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.10it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.01it/s] 89%|████████▉ | 89/100 [00:10<00:01,  8.55it/s] 90%|█████████ | 90/100 [00:10<00:01,  8.62it/s] 91%|█████████ | 91/100 [00:10<00:01,  8.34it/s] 92%|█████████▏| 92/100 [00:10<00:00,  8.34it/s] 93%|█████████▎| 93/100 [00:11<00:00,  8.11it/s] 94%|█████████▍| 94/100 [00:11<00:00,  8.19it/s] 95%|█████████▌| 95/100 [00:11<00:00,  8.21it/s] 96%|█████████▌| 96/100 [00:11<00:00,  7.48it/s] 97%|█████████▋| 97/100 [00:11<00:00,  6.94it/s] 98%|█████████▊| 98/100 [00:11<00:00,  7.42it/s] 99%|█████████▉| 99/100 [00:11<00:00,  7.62it/s]100%|██████████| 100/100 [00:12<00:00,  6.53it/s]100%|██████████| 100/100 [00:12<00:00,  8.28it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00806, 0.00804
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00779, 0.00781
--- total mse / var(X): 0.00793
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:08, 11.11it/s]  4%|▍         | 4/100 [00:00<00:09,  9.64it/s]  5%|▌         | 5/100 [00:00<00:10,  9.46it/s]  7%|▋         | 7/100 [00:00<00:09,  9.68it/s]  8%|▊         | 8/100 [00:00<00:09,  9.49it/s]  9%|▉         | 9/100 [00:00<00:09,  9.42it/s] 10%|█         | 10/100 [00:01<00:09,  9.48it/s] 12%|█▏        | 12/100 [00:01<00:09,  9.69it/s] 14%|█▍        | 14/100 [00:01<00:08, 10.26it/s] 16%|█▌        | 16/100 [00:01<00:08, 10.00it/s] 18%|█▊        | 18/100 [00:01<00:08,  9.97it/s] 19%|█▉        | 19/100 [00:01<00:08,  9.77it/s] 21%|██        | 21/100 [00:02<00:07, 10.28it/s] 23%|██▎       | 23/100 [00:02<00:07, 10.18it/s] 25%|██▌       | 25/100 [00:02<00:06, 10.72it/s] 27%|██▋       | 27/100 [00:02<00:06, 11.27it/s] 29%|██▉       | 29/100 [00:02<00:06, 10.74it/s] 31%|███       | 31/100 [00:03<00:06, 10.94it/s] 33%|███▎      | 33/100 [00:03<00:06, 10.75it/s] 35%|███▌      | 35/100 [00:03<00:06, 10.54it/s] 37%|███▋      | 37/100 [00:03<00:05, 10.58it/s] 39%|███▉      | 39/100 [00:03<00:05, 10.95it/s] 41%|████      | 41/100 [00:03<00:05, 11.14it/s] 43%|████▎     | 43/100 [00:04<00:05, 10.66it/s] 45%|████▌     | 45/100 [00:04<00:04, 11.69it/s] 47%|████▋     | 47/100 [00:04<00:04, 11.93it/s] 49%|████▉     | 49/100 [00:04<00:04, 11.73it/s] 51%|█████     | 51/100 [00:05<00:07,  6.98it/s] 52%|█████▏    | 52/100 [00:05<00:07,  6.62it/s] 54%|█████▍    | 54/100 [00:05<00:05,  7.97it/s] 56%|█████▌    | 56/100 [00:05<00:04,  9.13it/s] 58%|█████▊    | 58/100 [00:05<00:04,  9.94it/s] 60%|██████    | 60/100 [00:06<00:03, 10.06it/s] 62%|██████▏   | 62/100 [00:06<00:03, 10.52it/s] 64%|██████▍   | 64/100 [00:06<00:03, 10.81it/s] 66%|██████▌   | 66/100 [00:06<00:03, 10.91it/s] 68%|██████▊   | 68/100 [00:06<00:02, 11.01it/s] 70%|███████   | 70/100 [00:06<00:02, 11.05it/s] 72%|███████▏  | 72/100 [00:07<00:02, 11.37it/s] 74%|███████▍  | 74/100 [00:07<00:02, 11.41it/s] 76%|███████▌  | 76/100 [00:07<00:02, 11.95it/s] 78%|███████▊  | 78/100 [00:07<00:01, 12.52it/s] 80%|████████  | 80/100 [00:07<00:01, 13.21it/s] 82%|████████▏ | 82/100 [00:07<00:01, 13.93it/s] 84%|████████▍ | 84/100 [00:07<00:01, 14.25it/s] 86%|████████▌ | 86/100 [00:08<00:00, 14.38it/s] 88%|████████▊ | 88/100 [00:08<00:00, 15.17it/s] 90%|█████████ | 90/100 [00:08<00:00, 15.61it/s] 92%|█████████▏| 92/100 [00:08<00:00, 15.96it/s] 94%|█████████▍| 94/100 [00:08<00:00, 15.38it/s] 96%|█████████▌| 96/100 [00:08<00:00, 14.58it/s] 98%|█████████▊| 98/100 [00:08<00:00, 14.34it/s]100%|██████████| 100/100 [00:08<00:00, 14.77it/s]100%|██████████| 100/100 [00:08<00:00, 11.14it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00269, 0.00239
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00407, 0.00453
--- total mse / var(X): 0.00346
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:05, 17.66it/s]  4%|▍         | 4/100 [00:00<00:06, 15.54it/s]  6%|▌         | 6/100 [00:00<00:06, 15.07it/s]  8%|▊         | 8/100 [00:00<00:05, 16.34it/s] 10%|█         | 10/100 [00:00<00:05, 17.09it/s] 12%|█▏        | 12/100 [00:00<00:05, 16.20it/s] 14%|█▍        | 14/100 [00:00<00:05, 15.98it/s] 16%|█▌        | 16/100 [00:00<00:05, 16.38it/s] 18%|█▊        | 18/100 [00:01<00:05, 16.00it/s] 20%|██        | 20/100 [00:01<00:05, 15.86it/s] 22%|██▏       | 22/100 [00:01<00:04, 15.81it/s] 24%|██▍       | 24/100 [00:01<00:04, 15.93it/s] 27%|██▋       | 27/100 [00:01<00:04, 17.10it/s] 29%|██▉       | 29/100 [00:01<00:04, 17.39it/s] 31%|███       | 31/100 [00:01<00:03, 17.45it/s] 34%|███▍      | 34/100 [00:02<00:03, 17.94it/s] 36%|███▌      | 36/100 [00:02<00:03, 17.91it/s] 38%|███▊      | 38/100 [00:02<00:03, 18.14it/s] 40%|████      | 40/100 [00:02<00:03, 18.59it/s] 42%|████▏     | 42/100 [00:02<00:03, 17.59it/s] 44%|████▍     | 44/100 [00:02<00:03, 17.36it/s] 46%|████▌     | 46/100 [00:02<00:03, 15.61it/s] 49%|████▉     | 49/100 [00:02<00:02, 17.23it/s] 51%|█████     | 51/100 [00:03<00:02, 17.56it/s] 54%|█████▍    | 54/100 [00:03<00:02, 19.73it/s] 56%|█████▌    | 56/100 [00:03<00:02, 18.00it/s] 59%|█████▉    | 59/100 [00:03<00:02, 18.64it/s] 61%|██████    | 61/100 [00:03<00:02, 18.45it/s] 64%|██████▍   | 64/100 [00:03<00:01, 19.64it/s] 66%|██████▌   | 66/100 [00:03<00:01, 18.86it/s] 68%|██████▊   | 68/100 [00:03<00:01, 17.50it/s] 70%|███████   | 70/100 [00:04<00:01, 17.77it/s] 72%|███████▏  | 72/100 [00:04<00:01, 18.02it/s] 75%|███████▌  | 75/100 [00:04<00:01, 19.42it/s] 77%|███████▋  | 77/100 [00:04<00:01, 19.43it/s] 79%|███████▉  | 79/100 [00:04<00:01, 18.75it/s] 81%|████████  | 81/100 [00:04<00:01, 16.77it/s] 83%|████████▎ | 83/100 [00:04<00:00, 17.39it/s] 85%|████████▌ | 85/100 [00:04<00:00, 17.07it/s] 87%|████████▋ | 87/100 [00:04<00:00, 17.59it/s] 90%|█████████ | 90/100 [00:05<00:00, 18.97it/s] 92%|█████████▏| 92/100 [00:05<00:00, 18.38it/s] 94%|█████████▍| 94/100 [00:05<00:00, 18.39it/s] 96%|█████████▌| 96/100 [00:05<00:00, 18.62it/s] 98%|█████████▊| 98/100 [00:05<00:00, 18.45it/s]100%|██████████| 100/100 [00:05<00:00, 18.00it/s]100%|██████████| 100/100 [00:05<00:00, 17.62it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00101, 0.000757
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000841, 0.00105
--- total mse / var(X): 0.000905
start table evaluation...
Elapsed time: 118.22426271438599 seconds
Cosine similarity between AMM and exact (Train): 0.99458873
Cosine similarity between AMM and exact (Test): 0.9937253
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4744259503328659 0.6364076727489997 0.5436067268209082
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9983572959899902,
                                      0.9972733855247498,
                                      0.996353030204773,
                                      0.9945887327194214],
               'cossim_layer_test': [0.9963802695274353,
                                     0.9967865347862244,
                                     0.9956217408180237,
                                     0.9937252998352051],
               'cossim_amm_train': [0.9971400499343872,
                                    0.9977587461471558,
                                    0.9969255328178406,
                                    0.9867649674415588,
                                    0.9935149550437927,
                                    0.9928291440010071,
                                    0.9942933917045593,
                                    0.9980881810188293],
               'cossim_amm_test': [0.9935691356658936,
                                   0.9964739680290222,
                                   0.9952053427696228,
                                   0.9851962327957153,
                                   0.9925605058670044,
                                   0.990229606628418,
                                   0.9928097724914551,
                                   0.9977247714996338],
               'f1': [0.4744259503328659,
                      0.6364076727489997,
                      0.5436067268209082],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5004456132 - test_loss: 0.5926514595
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4179426702 - test_loss: 0.4963800956
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3636558595 - test_loss: 0.4334849967
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3264382194 - test_loss: 0.3848499017
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2984238285 - test_loss: 0.3461319864
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2772029512 - test_loss: 0.3152206213
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2612555991 - test_loss: 0.2906365311
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2494390843 - test_loss: 0.2711573682
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2408014387 - test_loss: 0.2558157169
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2346002531 - test_loss: 0.2438152756
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2302216775 - test_loss: 0.2345413024
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2271847093 - test_loss: 0.2273719454
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2251168225 - test_loss: 0.2219243265
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2237301568 - test_loss: 0.2179650282
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2228201846 - test_loss: 0.2150087457
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2222347772 - test_loss: 0.2128743185
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2218510116 - test_loss: 0.2113216857
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2216180563 - test_loss: 0.2101076434
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2214575003 - test_loss: 0.2094859017
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2213484693 - test_loss: 0.2088665285
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2212861247 - test_loss: 0.2086005593
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2212163142 - test_loss: 0.2083410043
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2211724068 - test_loss: 0.2080587483
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2211308772 - test_loss: 0.2077553671
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2210895819 - test_loss: 0.2079110757
Early Stop Left: 4
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2210532097 - test_loss: 0.2075262541
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2209997287 - test_loss: 0.2077565034
Early Stop Left: 4
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2209464884 - test_loss: 0.2074448800
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2208650577 - test_loss: 0.2072646215
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2207675956 - test_loss: 0.2072705208
Early Stop Left: 4
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2206046525 - test_loss: 0.2070918598
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2203425115 - test_loss: 0.2066687088
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2200734246 - test_loss: 0.2066980226
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2198487622 - test_loss: 0.2060573329
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2196743232 - test_loss: 0.2061829511
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2195243410 - test_loss: 0.2058819497
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2193896582 - test_loss: 0.2055707514
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2192854921 - test_loss: 0.2056352019
Early Stop Left: 4
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2191852413 - test_loss: 0.2053931719
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2190943963 - test_loss: 0.2055696824
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2190132902 - test_loss: 0.2059549629
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2189451564 - test_loss: 0.2053624233
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2188737339 - test_loss: 0.2052205834
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2188042876 - test_loss: 0.2053730065
Early Stop Left: 4
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2187263806 - test_loss: 0.2052398708
Early Stop Left: 3
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2186661869 - test_loss: 0.2052344941
Early Stop Left: 2
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2185968235 - test_loss: 0.2054723722
Early Stop Left: 1
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2185304134 - test_loss: 0.2054389215
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 15%|█▌        | 16/104 [00:00<00:00, 153.58it/s] 31%|███       | 32/104 [00:00<00:00, 152.75it/s] 46%|████▌     | 48/104 [00:00<00:00, 151.79it/s] 62%|██████▏   | 64/104 [00:00<00:00, 150.57it/s] 77%|███████▋  | 80/104 [00:00<00:00, 149.25it/s] 92%|█████████▏| 96/104 [00:00<00:00, 150.01it/s]100%|██████████| 104/104 [00:00<00:00, 151.46it/s]===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        1,376
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 6,128
Trainable params: 6,128
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        1,376
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 6,128
Trainable params: 6,128
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5494589365 - test_loss: 0.6932253843
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5115155777 - test_loss: 0.6420348585
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4729940918 - test_loss: 0.5889463488
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4341168755 - test_loss: 0.5357199821
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3981240508 - test_loss: 0.4884180057
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3685690946 - test_loss: 0.4500925558
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3451388754 - test_loss: 0.4185858375
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3258658283 - test_loss: 0.3915783989
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3094856907 - test_loss: 0.3678437640
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2953688065 - test_loss: 0.3467543509
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2831538464 - test_loss: 0.3279385790
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2726004273 - test_loss: 0.3111663429
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2635210284 - test_loss: 0.2962462326
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2557665214 - test_loss: 0.2830081860
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2491850798 - test_loss: 0.2713404325
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2436477614 - test_loss: 0.2611077806
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2390434974 - test_loss: 0.2521704997
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2352446135 - test_loss: 0.2444467324
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2321513523 - test_loss: 0.2378105659
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2296594621 - test_loss: 0.2321783917
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2276764725 - test_loss: 0.2274372449
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2261148211 - test_loss: 0.2234469486
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2249002462 - test_loss: 0.2202034977
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2239749841 - test_loss: 0.2175537719
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2232644970 - test_loss: 0.2153594207
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2227329986 - test_loss: 0.2137215511
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2223404789 - test_loss: 0.2123946232
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2220473421 - test_loss: 0.2113120552
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2218344864 - test_loss: 0.2105271335
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2216840277 - test_loss: 0.2099061641
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2215712501 - test_loss: 0.2094876607
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2214947262 - test_loss: 0.2091608853
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2214461502 - test_loss: 0.2089870912
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2214046201 - test_loss: 0.2088330749
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2213833994 - test_loss: 0.2086454630
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2213632082 - test_loss: 0.2085139022
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2213599992 - test_loss: 0.2085046656
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2213521089 - test_loss: 0.2084204337
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2213402290 - test_loss: 0.2083803268
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2213380162 - test_loss: 0.2083092246
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2213377059 - test_loss: 0.2083295559
Early Stop Left: 4
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2213317197 - test_loss: 0.2082925895
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2213320207 - test_loss: 0.2083065991
Early Stop Left: 4
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2213260327 - test_loss: 0.2082625308
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2213204049 - test_loss: 0.2082866761
Early Stop Left: 4
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2213177117 - test_loss: 0.2082794101
Early Stop Left: 3
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2213147926 - test_loss: 0.2082922033
Early Stop Left: 2
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2213074015 - test_loss: 0.2082511063
-------- Save Best Model! --------
------- START EPOCH 49 -------

Best micro threshold=0.331877, fscore=0.554
p,r,f1: 0.4834772044338266 0.6480672053498896 0.553801897687244
throttleing by fixed threshold: 0.5
p,r,f1: 0.6405026424726344 0.36891319057662475 0.4681715219258065
{'model': 'vit_large',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3318767845630646,
                 'p': 0.4834772044338266,
                 'r': 0.6480672053498896,
                 'f1': 0.553801897687244},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6405026424726344,
                 'r': 0.36891319057662475,
                 'f1': 0.4681715219258065}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
Epoch: 49 - loss: 0.2213072038 - test_loss: 0.2083841217
Early Stop Left: 4
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2212992612 - test_loss: 0.2082057065
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2212959607 - test_loss: 0.2081997367
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2212942764 - test_loss: 0.2082974011
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2212907043 - test_loss: 0.2081732756
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2212830713 - test_loss: 0.2081742760
Early Stop Left: 4
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2212714219 - test_loss: 0.2081880491
Early Stop Left: 3
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2212684333 - test_loss: 0.2082325177
Early Stop Left: 2
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2212638010 - test_loss: 0.2081805889
Early Stop Left: 1
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2212563562 - test_loss: 0.2081376767
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2212460164 - test_loss: 0.2081446612
Early Stop Left: 4
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2212329041 - test_loss: 0.2081694346
Early Stop Left: 3
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2212269486 - test_loss: 0.2079959562
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2212206823 - test_loss: 0.2080788410
Early Stop Left: 4
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2212104568 - test_loss: 0.2078619628
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2211940433 - test_loss: 0.2080182370
Early Stop Left: 4
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2211847395 - test_loss: 0.2078462316
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2211682990 - test_loss: 0.2078476760
Early Stop Left: 4
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2211545909 - test_loss: 0.2078596442
Early Stop Left: 3
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2211415228 - test_loss: 0.2078547333
Early Stop Left: 2
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2211242691 - test_loss: 0.2078242167
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2211064829 - test_loss: 0.2076066502
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2210895925 - test_loss: 0.2077015701
Early Stop Left: 4
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2210754828 - test_loss: 0.2077398959
Early Stop Left: 3
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2210477020 - test_loss: 0.2076668725
Early Stop Left: 2
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2210283381 - test_loss: 0.2076187909
Early Stop Left: 1
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2210117223 - test_loss: 0.2075141362
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2209869201 - test_loss: 0.2075398275
Early Stop Left: 4
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2209660000 - test_loss: 0.2074942701
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2209397041 - test_loss: 0.2074090720
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2209147456 - test_loss: 0.2074086010
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2208895404 - test_loss: 0.2072703515
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2208683055 - test_loss: 0.2073397579
Early Stop Left: 4
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2208416044 - test_loss: 0.2072299616
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.2208180996 - test_loss: 0.2071679977
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.2207871204 - test_loss: 0.2072749823
Early Stop Left: 4
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.2207613918 - test_loss: 0.2072129252
Early Stop Left: 3
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.2207327369 - test_loss: 0.2071907758
Early Stop Left: 2
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.2207115146 - test_loss: 0.2071497326
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.2206837042 - test_loss: 0.2070702254
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.2206569457 - test_loss: 0.2070980781
Early Stop Left: 4
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.2206258168 - test_loss: 0.2071010056
Early Stop Left: 3
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.2205928642 - test_loss: 0.2069821965
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.2205739135 - test_loss: 0.2070291611
Early Stop Left: 4
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.2205425543 - test_loss: 0.2069405290
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.2205177511 - test_loss: 0.2069210978
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.2204909774 - test_loss: 0.2068049207
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.2204590574 - test_loss: 0.2069406098
Early Stop Left: 4
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.2204387131 - test_loss: 0.2069002943
Early Stop Left: 3
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.2204130554 - test_loss: 0.2068287777
Early Stop Left: 2
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.2203761973 - test_loss: 0.2067684233
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.2203506164 - test_loss: 0.2067148565
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.2203220676 - test_loss: 0.2066843723
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.2202966739 - test_loss: 0.2068733802
Early Stop Left: 4
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.2202610596 - test_loss: 0.2066493322
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.2202284432 - test_loss: 0.2066719383
Early Stop Left: 4
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.2201982928 - test_loss: 0.2066675485
Early Stop Left: 3
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.2201689583 - test_loss: 0.2065901809
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.2201237912 - test_loss: 0.2065402544
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.2201065864 - test_loss: 0.2066578885
Early Stop Left: 4
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.2200679501 - test_loss: 0.2066047011
Early Stop Left: 3
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.2200341686 - test_loss: 0.2065712722
Early Stop Left: 2
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.2199905414 - test_loss: 0.2065157529
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.2199564906 - test_loss: 0.2065902668
Early Stop Left: 4
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.2199174409 - test_loss: 0.2064357364
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.2198731154 - test_loss: 0.2064557866
Early Stop Left: 4
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.2198329227 - test_loss: 0.2064253027
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.2197840148 - test_loss: 0.2064239960
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.2197459107 - test_loss: 0.2064214790
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.2196969238 - test_loss: 0.2064585713
Early Stop Left: 4
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.2196471836 - test_loss: 0.2063437457
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.2195934427 - test_loss: 0.2063846091
Early Stop Left: 4
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.2195500948 - test_loss: 0.2061960370
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.2195002075 - test_loss: 0.2062914985
Early Stop Left: 4
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.2194572098 - test_loss: 0.2061815633
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.2194069512 - test_loss: 0.2061584953
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.2193644702 - test_loss: 0.2061209451
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.2193233077 - test_loss: 0.2060514128
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.2192791173 - test_loss: 0.2060912805
Early Stop Left: 4
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.2192407345 - test_loss: 0.2061141887
Early Stop Left: 3
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.2192018092 - test_loss: 0.2059340054
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.2191643673 - test_loss: 0.2058447902
-------- Save Best Model! --------
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.2191267902 - test_loss: 0.2057746640
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.2190971221 - test_loss: 0.2058013976
Early Stop Left: 4
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.2190631518 - test_loss: 0.2057297723
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.2190363983 - test_loss: 0.2057188393
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.2190069950 - test_loss: 0.2057027000
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.2189794611 - test_loss: 0.2058316624
Early Stop Left: 4
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.2189473433 - test_loss: 0.2056874637
-------- Save Best Model! --------
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.2189202940 - test_loss: 0.2057336651
Early Stop Left: 4
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.2188966959 - test_loss: 0.2054453610
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.2188729626 - test_loss: 0.2055815014
Early Stop Left: 4
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.2188510249 - test_loss: 0.2054647061
Early Stop Left: 3
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.2188291295 - test_loss: 0.2054423883
-------- Save Best Model! --------
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.2188066345 - test_loss: 0.2052658195
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.2187854328 - test_loss: 0.2053353680
Early Stop Left: 4
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.2187735962 - test_loss: 0.2054102524
Early Stop Left: 3
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.2187548152 - test_loss: 0.2051801720
-------- Save Best Model! --------
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.2187381917 - test_loss: 0.2054348833
Early Stop Left: 4
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.2187194262 - test_loss: 0.2052862627
Early Stop Left: 3
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.2187002635 - test_loss: 0.2052836322
Early Stop Left: 2
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.2186847420 - test_loss: 0.2053670266
Early Stop Left: 1
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.2186762021 - test_loss: 0.2051866489
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 198.15it/s] 38%|███▊      | 40/104 [00:00<00:00, 196.33it/s] 58%|█████▊    | 60/104 [00:00<00:00, 195.46it/s] 77%|███████▋  | 80/104 [00:00<00:00, 195.20it/s] 96%|█████████▌| 100/104 [00:00<00:00, 189.59it/s]100%|██████████| 104/104 [00:00<00:00, 193.35it/s]
Best micro threshold=0.317660, fscore=0.556
p,r,f1: 0.47984307118819286 0.6613910391610264 0.5561766943511064
throttleing by fixed threshold: 0.5
p,r,f1: 0.6501412496358673 0.36176593101914417 0.4648627047319185
{'model': 'vit_min',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.31766006350517273,
                 'p': 0.47984307118819286,
                 'r': 0.6613910391610264,
                 'f1': 0.5561766943511064},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6501412496358673,
                 'r': 0.36176593101914417,
                 'f1': 0.4648627047319185}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999993
Manual and Torch results cosine similarity (Test): 1.0000004
start table training with fine tuning...
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Retrain for 1 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0678, 0.0678
--- total mse / var(X): 0.0678
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:05<09:50,  5.97s/it]  2%|▏         | 2/100 [00:11<09:45,  5.98s/it]  3%|▎         | 3/100 [00:17<09:24,  5.82s/it]  4%|▍         | 4/100 [00:23<09:09,  5.72s/it]  5%|▌         | 5/100 [00:28<08:41,  5.49s/it]  6%|▌         | 6/100 [00:35<09:30,  6.07s/it]  7%|▋         | 7/100 [00:40<09:08,  5.90s/it]  8%|▊         | 8/100 [00:46<08:58,  5.85s/it]  9%|▉         | 9/100 [00:53<09:25,  6.21s/it] 10%|█         | 10/100 [00:58<08:46,  5.85s/it] 11%|█         | 11/100 [01:04<08:24,  5.67s/it] 12%|█▏        | 12/100 [01:10<08:37,  5.88s/it] 13%|█▎        | 13/100 [01:17<09:01,  6.23s/it] 14%|█▍        | 14/100 [01:24<09:19,  6.51s/it] 15%|█▌        | 15/100 [01:29<08:27,  5.97s/it] 16%|█▌        | 16/100 [01:34<07:49,  5.59s/it] 17%|█▋        | 17/100 [01:40<08:12,  5.93s/it] 18%|█▊        | 18/100 [01:47<08:18,  6.08s/it] 19%|█▉        | 19/100 [01:51<07:36,  5.64s/it] 20%|██        | 20/100 [01:56<07:17,  5.47s/it] 21%|██        | 21/100 [02:03<07:28,  5.68s/it] 22%|██▏       | 22/100 [02:08<07:28,  5.75s/it] 23%|██▎       | 23/100 [02:14<07:12,  5.62s/it] 24%|██▍       | 24/100 [02:19<06:59,  5.52s/it] 25%|██▌       | 25/100 [02:25<07:11,  5.76s/it] 26%|██▌       | 26/100 [02:32<07:15,  5.88s/it] 27%|██▋       | 27/100 [02:37<06:59,  5.75s/it] 28%|██▊       | 28/100 [02:43<06:56,  5.79s/it] 29%|██▉       | 29/100 [02:49<07:00,  5.92s/it] 30%|███       | 30/100 [02:56<07:08,  6.12s/it] 31%|███       | 31/100 [03:02<07:01,  6.11s/it] 32%|███▏      | 32/100 [03:06<06:20,  5.60s/it] 33%|███▎      | 33/100 [03:11<06:05,  5.46s/it] 34%|███▍      | 34/100 [03:19<06:38,  6.04s/it] 35%|███▌      | 35/100 [03:24<06:13,  5.74s/it] 36%|███▌      | 36/100 [03:29<05:59,  5.61s/it] 37%|███▋      | 37/100 [03:35<05:53,  5.60s/it] 38%|███▊      | 38/100 [03:40<05:50,  5.66s/it] 39%|███▉      | 39/100 [03:46<05:47,  5.69s/it] 40%|████      | 40/100 [03:52<05:39,  5.65s/it] 41%|████      | 41/100 [03:57<05:34,  5.66s/it] 42%|████▏     | 42/100 [04:04<05:36,  5.80s/it] 43%|████▎     | 43/100 [04:10<05:46,  6.09s/it] 44%|████▍     | 44/100 [04:16<05:42,  6.11s/it] 45%|████▌     | 45/100 [04:22<05:26,  5.94s/it] 46%|████▌     | 46/100 [04:28<05:19,  5.91s/it] 47%|████▋     | 47/100 [04:33<05:03,  5.73s/it] 48%|████▊     | 48/100 [04:39<04:56,  5.69s/it] 49%|████▉     | 49/100 [04:44<04:49,  5.68s/it] 50%|█████     | 50/100 [04:49<04:33,  5.47s/it] 51%|█████     | 51/100 [04:53<04:07,  5.05s/it] 52%|█████▏    | 52/100 [05:00<04:17,  5.37s/it] 53%|█████▎    | 53/100 [05:06<04:33,  5.82s/it] 54%|█████▍    | 54/100 [05:12<04:20,  5.67s/it] 55%|█████▌    | 55/100 [05:17<04:10,  5.56s/it] 56%|█████▌    | 56/100 [05:23<04:09,  5.66s/it] 57%|█████▋    | 57/100 [05:30<04:15,  5.95s/it] 58%|█████▊    | 58/100 [05:35<04:04,  5.82s/it] 59%|█████▉    | 59/100 [05:40<03:51,  5.64s/it] 60%|██████    | 60/100 [05:47<03:53,  5.85s/it] 61%|██████    | 61/100 [05:53<03:55,  6.05s/it] 62%|██████▏   | 62/100 [06:01<04:05,  6.46s/it] 63%|██████▎   | 63/100 [06:08<04:04,  6.62s/it] 64%|██████▍   | 64/100 [06:13<03:45,  6.26s/it] 65%|██████▌   | 65/100 [06:17<03:20,  5.71s/it] 66%|██████▌   | 66/100 [06:23<03:07,  5.53s/it] 67%|██████▋   | 67/100 [06:28<03:04,  5.59s/it] 68%|██████▊   | 68/100 [06:35<03:07,  5.85s/it] 69%|██████▉   | 69/100 [06:42<03:14,  6.27s/it] 70%|███████   | 70/100 [06:47<02:57,  5.92s/it] 71%|███████   | 71/100 [06:52<02:47,  5.76s/it] 72%|███████▏  | 72/100 [06:58<02:38,  5.67s/it] 73%|███████▎  | 73/100 [07:05<02:43,  6.04s/it] 74%|███████▍  | 74/100 [07:10<02:32,  5.87s/it] 75%|███████▌  | 75/100 [07:16<02:22,  5.69s/it] 76%|███████▌  | 76/100 [07:21<02:14,  5.59s/it] 77%|███████▋  | 77/100 [07:27<02:11,  5.72s/it] 78%|███████▊  | 78/100 [07:34<02:12,  6.00s/it] 79%|███████▉  | 79/100 [07:39<01:59,  5.69s/it] 80%|████████  | 80/100 [07:44<01:53,  5.70s/it] 81%|████████  | 81/100 [07:50<01:47,  5.67s/it] 82%|████████▏ | 82/100 [07:56<01:45,  5.84s/it] 83%|████████▎ | 83/100 [08:03<01:45,  6.18s/it] 84%|████████▍ | 84/100 [08:09<01:38,  6.17s/it] 85%|████████▌ | 85/100 [08:15<01:31,  6.10s/it] 86%|████████▌ | 86/100 [08:22<01:27,  6.23s/it] 87%|████████▋ | 87/100 [08:27<01:17,  5.95s/it] 88%|████████▊ | 88/100 [08:33<01:09,  5.83s/it] 89%|████████▉ | 89/100 [08:38<01:03,  5.73s/it] 90%|█████████ | 90/100 [08:44<00:56,  5.70s/it] 91%|█████████ | 91/100 [08:50<00:53,  5.92s/it] 92%|█████████▏| 92/100 [08:56<00:46,  5.83s/it] 93%|█████████▎| 93/100 [09:01<00:39,  5.67s/it] 94%|█████████▍| 94/100 [09:06<00:32,  5.41s/it] 95%|█████████▌| 95/100 [09:13<00:29,  5.86s/it] 96%|█████████▌| 96/100 [09:20<00:25,  6.28s/it] 97%|█████████▋| 97/100 [09:26<00:18,  6.31s/it] 98%|█████████▊| 98/100 [09:32<00:11,  5.95s/it]
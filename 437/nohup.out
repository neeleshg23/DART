===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5919307130 - test_loss: 0.5395173253
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4192135360 - test_loss: 0.3950863999
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3299194943 - test_loss: 0.3180839425
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2792854073 - test_loss: 0.2699688971
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2486420079 - test_loss: 0.2390679802
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2299698705 - test_loss: 0.2188503023
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2186113735 - test_loss: 0.2054239619
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2117333584 - test_loss: 0.1964236327
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2076008956 - test_loss: 0.1902693864
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2051421126 - test_loss: 0.1860894188
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2036994673 - test_loss: 0.1831464399
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2028706672 - test_loss: 0.1811904356
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2023963946 - test_loss: 0.1798253386
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2021249479 - test_loss: 0.1790062655
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2019475721 - test_loss: 0.1783456364
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2018254700 - test_loss: 0.1779362349
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2017238812 - test_loss: 0.1776397061
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2016088922 - test_loss: 0.1773771522
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2015023011 - test_loss: 0.1771960120
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2013783225 - test_loss: 0.1770870133
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2012676636 - test_loss: 0.1766349241
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2011250752 - test_loss: 0.1766978706
Early Stop Left: 4
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2009918511 - test_loss: 0.1764691207
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2008243480 - test_loss: 0.1764308057
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2006218869 - test_loss: 0.1764184053
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2003288017 - test_loss: 0.1766593558
Early Stop Left: 4
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.1999372270 - test_loss: 0.1764556152
Early Stop Left: 3
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.1994894575 - test_loss: 0.1762029507
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.1991055631 - test_loss: 0.1762133633
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.1988102321 - test_loss: 0.1759388309
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.1985469484 - test_loss: 0.1758976923
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.1983383280 - test_loss: 0.1761530270
Early Stop Left: 4
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.1981288308 - test_loss: 0.1768097155
Early Stop Left: 3
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.1979657884 - test_loss: 0.1762432950
Early Stop Left: 2
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.1977983022 - test_loss: 0.1766874522
Early Stop Left: 1
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.1976546892 - test_loss: 0.1764015414
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 194.53it/s] 38%|███▊      | 40/104 [00:00<00:00, 193.67it/s] 58%|█████▊    | 60/104 [00:00<00:00, 192.44it/s] 77%|███████▋  | 80/104 [00:00<00:00, 191.89it/s] 96%|█████████▌| 100/104 [00:00<00:00, 192.01it/s]100%|██████████| 104/104 [00:00<00:00, 193.74it/s]
Best micro threshold=0.294705, fscore=0.551
p,r,f1: 0.4669115142112904 0.6716482613424106 0.5508719233792644
throttleing by fixed threshold: 0.5
p,r,f1: 0.6331077097542196 0.3703984333327679 0.4673655571304903
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.29470497369766235,
                 'p': 0.4669115142112904,
                 'r': 0.6716482613424106,
                 'f1': 0.5508719233792644},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6331077097542196,
                 'r': 0.3703984333327679,
                 'f1': 0.4673655571304903}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5308656697 - test_loss: 0.5406942591
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.3810310483 - test_loss: 0.3977579397
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3068945437 - test_loss: 0.3228166063
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2672921809 - test_loss: 0.2767075211
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2449105349 - test_loss: 0.2476038840
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2323630185 - test_loss: 0.2289947899
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2254574985 - test_loss: 0.2170270494
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2217382055 - test_loss: 0.2093943710
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2197828800 - test_loss: 0.2044750802
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2187767944 - test_loss: 0.2014165837
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2182677685 - test_loss: 0.1994451320
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2180145656 - test_loss: 0.1983419026
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2178823680 - test_loss: 0.1976055711
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2178082715 - test_loss: 0.1972683071
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2177472234 - test_loss: 0.1969696826
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2176970930 - test_loss: 0.1968053664
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2176479300 - test_loss: 0.1965999802
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2175826711 - test_loss: 0.1964008946
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2175193317 - test_loss: 0.1963193740
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2174390558 - test_loss: 0.1962976357
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2173668892 - test_loss: 0.1955513993
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2172724217 - test_loss: 0.1957718620
Early Stop Left: 4
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2171900587 - test_loss: 0.1954867129
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2170901730 - test_loss: 0.1953331330
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2169819196 - test_loss: 0.1953726973
Early Stop Left: 4
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2168412049 - test_loss: 0.1953865098
Early Stop Left: 3
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2166780530 - test_loss: 0.1954117606
Early Stop Left: 2
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2164479083 - test_loss: 0.1952531942
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2161140391 - test_loss: 0.1955172659
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2157770215 - test_loss: 0.1949976031
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2155043720 - test_loss: 0.1946738438
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2153101539 - test_loss: 0.1948654874
Early Stop Left: 4
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2151348383 - test_loss: 0.1955148784
Early Stop Left: 3
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2150043650 - test_loss: 0.1947666309
Early Stop Left: 2
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2148750011 - test_loss: 0.1952912842
Early Stop Left: 1
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2147638879 - test_loss: 0.1947756255
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 16%|█▋        | 17/104 [00:00<00:00, 162.92it/s] 35%|███▍      | 36/104 [00:00<00:00, 173.47it/s] 53%|█████▎    | 55/104 [00:00<00:00, 178.03it/s] 71%|███████   | 74/104 [00:00<00:00, 182.05it/s] 90%|█████████ | 94/104 [00:00<00:00, 186.85it/s]100%|██████████| 104/104 [00:00<00:00, 184.55it/s]===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5614035226 - test_loss: 0.5400614830
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4001539626 - test_loss: 0.3963029158
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3185065797 - test_loss: 0.3202375993
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2735037346 - test_loss: 0.2730140116
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2471425551 - test_loss: 0.2428998304
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2317112646 - test_loss: 0.2233730605
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2227729604 - test_loss: 0.2105685988
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2176674976 - test_loss: 0.2021519453
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2148017523 - test_loss: 0.1965409142
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2132220605 - test_loss: 0.1928753904
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2123682528 - test_loss: 0.1904029332
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2119178941 - test_loss: 0.1888823221
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2116771832 - test_loss: 0.1878632241
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2115445409 - test_loss: 0.1873310856
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2114494453 - test_loss: 0.1868868204
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2113771194 - test_loss: 0.1866374102
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2113098815 - test_loss: 0.1864118857
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2112255163 - test_loss: 0.1861968048
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2111449149 - test_loss: 0.1860785822
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2110464234 - test_loss: 0.1860111293
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2109580463 - test_loss: 0.1854078883
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2108431994 - test_loss: 0.1855482930
Early Stop Left: 4
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2107399859 - test_loss: 0.1852766466
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2106130888 - test_loss: 0.1851732979
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2104687202 - test_loss: 0.1851663061
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2102706773 - test_loss: 0.1852904532
Early Stop Left: 4
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2100122401 - test_loss: 0.1852893235
Early Stop Left: 3
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2096522426 - test_loss: 0.1851556025
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2092609006 - test_loss: 0.1851349639
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2089508171 - test_loss: 0.1846816904
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2086962245 - test_loss: 0.1844988010
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2085030672 - test_loss: 0.1847369258
Early Stop Left: 4
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2083160402 - test_loss: 0.1854040531
Early Stop Left: 3
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2081730367 - test_loss: 0.1847275516
Early Stop Left: 2
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2080273464 - test_loss: 0.1852295682
Early Stop Left: 1
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2079022259 - test_loss: 0.1848334917
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 194.05it/s] 38%|███▊      | 40/104 [00:00<00:00, 192.85it/s] 58%|█████▊    | 60/104 [00:00<00:00, 193.11it/s] 77%|███████▋  | 80/104 [00:00<00:00, 192.59it/s] 96%|█████████▌| 100/104 [00:00<00:00, 186.10it/s]100%|██████████| 104/104 [00:00<00:00, 190.66it/s]
Best micro threshold=0.311020, fscore=0.551
p,r,f1: 0.4716634311491245 0.6635548192169303 0.5513909641365873
throttleing by fixed threshold: 0.5
p,r,f1: 0.642464600959353 0.3569615099716711 0.45893364512591855
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3110196590423584,
                 'p': 0.4716634311491245,
                 'r': 0.6635548192169303,
                 'f1': 0.5513909641365873},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.642464600959353,
                 'r': 0.3569615099716711,
                 'f1': 0.45893364512591855}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm

Best micro threshold=0.310871, fscore=0.551
p,r,f1: 0.4762741001005643 0.6543172878110198 0.5512767579241568
throttleing by fixed threshold: 0.5
p,r,f1: 0.6388292060877564 0.36258394543052247 0.4626046974685035
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3108711838722229,
                 'p': 0.4762741001005643,
                 'r': 0.6543172878110198,
                 'f1': 0.5512767579241568},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6388292060877564,
                 'r': 0.36258394543052247,
                 'f1': 0.4626046974685035}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5003160110 - test_loss: 0.5414381847
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.3618279707 - test_loss: 0.3995265138
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.2950345456 - test_loss: 0.3259539833
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.2605682007 - test_loss: 0.2812154826
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2418361352 - test_loss: 0.2533764355
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2318033899 - test_loss: 0.2359275239
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2265632388 - test_loss: 0.2250070155
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2238990158 - test_loss: 0.2182966221
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2225804272 - test_loss: 0.2141374937
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2219397336 - test_loss: 0.2116853388
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2216299250 - test_loss: 0.2101656945
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2214805358 - test_loss: 0.2094002522
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2214016601 - test_loss: 0.2088642504
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2213554244 - test_loss: 0.2086360020
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2213119503 - test_loss: 0.2084411655
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2212746799 - test_loss: 0.2083195158
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2212372358 - test_loss: 0.2081159585
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2211856358 - test_loss: 0.2079216281
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2211351467 - test_loss: 0.2078722141
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2210691604 - test_loss: 0.2078930922
Early Stop Left: 4
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2210096947 - test_loss: 0.2070225901
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2209309184 - test_loss: 0.2073225544
Early Stop Left: 4
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2208633858 - test_loss: 0.2070442385
Early Stop Left: 3
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2207820336 - test_loss: 0.2068415010
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2206963148 - test_loss: 0.2069391011
Early Stop Left: 4
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2205886381 - test_loss: 0.2068837115
Early Stop Left: 3
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2204751986 - test_loss: 0.2069323593
Early Stop Left: 2
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2203299099 - test_loss: 0.2065058259
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2201103345 - test_loss: 0.2068881418
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2198084255 - test_loss: 0.2066628699
Early Stop Left: 3
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2195117187 - test_loss: 0.2062717332
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2193018697 - test_loss: 0.2063746199
Early Stop Left: 4
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2191299705 - test_loss: 0.2069618136
Early Stop Left: 3
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2190081771 - test_loss: 0.2062121224
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2188934353 - test_loss: 0.2067009785
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2187969510 - test_loss: 0.2060751677
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2187070972 - test_loss: 0.2060506960
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2186099723 - test_loss: 0.2065324298
Early Stop Left: 4
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2185410685 - test_loss: 0.2064898795
Early Stop Left: 3
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2184640278 - test_loss: 0.2071212085
Early Stop Left: 2
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2183893479 - test_loss: 0.2067631893
Early Stop Left: 1
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2183227207 - test_loss: 0.2068233411
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 18%|█▊        | 19/104 [00:00<00:00, 186.97it/s] 37%|███▋      | 38/104 [00:00<00:00, 185.75it/s] 55%|█████▍    | 57/104 [00:00<00:00, 185.70it/s] 73%|███████▎  | 76/104 [00:00<00:00, 185.75it/s] 91%|█████████▏| 95/104 [00:00<00:00, 184.49it/s]100%|██████████| 104/104 [00:00<00:00, 186.26it/s]
Best micro threshold=0.321964, fscore=0.551
p,r,f1: 0.4677979828790582 0.6706248009152714 0.551143096576519
throttleing by fixed threshold: 0.5
p,r,f1: 0.6330295823722506 0.36674941052072085 0.46442909448499403
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.321964293718338,
                 'p': 0.4677979828790582,
                 'r': 0.6706248009152714,
                 'f1': 0.551143096576519},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6330295823722506,
                 'r': 0.36674941052072085,
                 'f1': 0.46442909448499403}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5413693022 - test_loss: 0.6543835218
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4590461490 - test_loss: 0.5401152808
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3857980143 - test_loss: 0.4563142382
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3393564762 - test_loss: 0.4014813224
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3081399201 - test_loss: 0.3601884011
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2851083456 - test_loss: 0.3275943350
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2677483395 - test_loss: 0.3015478732
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2546674738 - test_loss: 0.2806954438
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2448962784 - test_loss: 0.2639966192
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2376931654 - test_loss: 0.2507015189
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2324677863 - test_loss: 0.2401354913
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2287528772 - test_loss: 0.2318669960
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2261682726 - test_loss: 0.2254311931
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2244102199 - test_loss: 0.2205632396
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2232387489 - test_loss: 0.2168414154
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2224853270 - test_loss: 0.2140964781
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2220150925 - test_loss: 0.2121663963
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2217204619 - test_loss: 0.2108219167
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2215472777 - test_loss: 0.2098758363
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2214327113 - test_loss: 0.2092408443
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2213696479 - test_loss: 0.2086613920
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2213143589 - test_loss: 0.2086028063
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2212903360 - test_loss: 0.2082049885
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2212643919 - test_loss: 0.2080937753
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2212444875 - test_loss: 0.2081340832
Early Stop Left: 4
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2212113400 - test_loss: 0.2080660733
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2211879752 - test_loss: 0.2079423626
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2211632496 - test_loss: 0.2077982431
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2211331084 - test_loss: 0.2078182829
Early Stop Left: 4
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2211035469 - test_loss: 0.2077303446
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2210699893 - test_loss: 0.2075355393
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2210400846 - test_loss: 0.2075145770
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2209986241 - test_loss: 0.2076313905
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2209641181 - test_loss: 0.2070971822
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2209191396 - test_loss: 0.2073171902
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2208826746 - test_loss: 0.2068924387
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2208373518 - test_loss: 0.2070106514
Early Stop Left: 4
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2207856860 - test_loss: 0.2070835915
Early Stop Left: 3
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2207416979 - test_loss: 0.2067559563
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2206989877 - test_loss: 0.2068849104
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2206528725 - test_loss: 0.2068795517
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2206047092 - test_loss: 0.2067071376
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2205549940 - test_loss: 0.2066226156
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2205141139 - test_loss: 0.2066713208
Early Stop Left: 4
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2204551803 - test_loss: 0.2066138831
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2204094260 - test_loss: 0.2066156805
Early Stop Left: 4
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2203510559 - test_loss: 0.2063779971
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2202821523 - test_loss: 0.2064360731
Early Stop Left: 4
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2202160517 - test_loss: 0.2062481756
-------- Save Best Model! --------
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6083502428 - test_loss: 0.6539965874
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5139924782 - test_loss: 0.5387251371
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4286577442 - test_loss: 0.4541802552
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3733484647 - test_loss: 0.3983616843
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3350377015 - test_loss: 0.3559022113
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3058950978 - test_loss: 0.3220906653
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2832152684 - test_loss: 0.2948235288
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2655075184 - test_loss: 0.2727656836
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2517339605 - test_loss: 0.2548949999
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2410994020 - test_loss: 0.2404363949
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2329653546 - test_loss: 0.2287191236
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2268203938 - test_loss: 0.2192939660
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2222397662 - test_loss: 0.2117194171
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2188728854 - test_loss: 0.2057056450
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2164330576 - test_loss: 0.2008951868
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2147087504 - test_loss: 0.1971132297
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2135185189 - test_loss: 0.1942170888
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2127058917 - test_loss: 0.1919997019
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2121762289 - test_loss: 0.1903000706
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2118203876 - test_loss: 0.1890527490
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2116038783 - test_loss: 0.1880880965
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2114473968 - test_loss: 0.1875639736
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2113645940 - test_loss: 0.1869892037
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2112993247 - test_loss: 0.1867080534
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2112536315 - test_loss: 0.1865668790
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2111967936 - test_loss: 0.1864437137
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2111558810 - test_loss: 0.1862540308
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2111147074 - test_loss: 0.1861329210
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2110671148 - test_loss: 0.1860995284
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2110206689 - test_loss: 0.1859771631
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2109703453 - test_loss: 0.1858056332
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2109256828 - test_loss: 0.1857647861
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2108660819 - test_loss: 0.1858214800
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2108166012 - test_loss: 0.1854269944
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2107535819 - test_loss: 0.1855470118
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2107018118 - test_loss: 0.1852194130
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2106378305 - test_loss: 0.1852926841
Early Stop Left: 4
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2105652983 - test_loss: 0.1853393573
Early Stop Left: 3
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2105016274 - test_loss: 0.1851029135
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2104384945 - test_loss: 0.1851550255
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2103706116 - test_loss: 0.1851808409
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2102976005 - test_loss: 0.1850404956
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2102207619 - test_loss: 0.1850266726
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2101534908 - test_loss: 0.1850575469
Early Stop Left: 4
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2100605311 - test_loss: 0.1850529290
Early Stop Left: 3
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2099812220 - test_loss: 0.1850419181
Early Stop Left: 2
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2098803125 - test_loss: 0.1849861744
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2097597732 - test_loss: 0.1850201413
Early Stop Left: 4
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2096346436 - test_loss: 0.1849170234
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.6418358917 - test_loss: 0.6538420022
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5414348495 - test_loss: 0.5381738188
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4500228378 - test_loss: 0.4533733912
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3902154165 - test_loss: 0.3971817333
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3482494130 - test_loss: 0.3542891165
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3159089929 - test_loss: 0.3200343137
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2903951496 - test_loss: 0.2923407572
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2701689702 - test_loss: 0.2698765192
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2541577994 - test_loss: 0.2516093852
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2415397912 - test_loss: 0.2367727327
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2316546415 - test_loss: 0.2246954481
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2239747641 - test_loss: 0.2149083407
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2180613598 - test_loss: 0.2069772717
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2135497437 - test_loss: 0.2005933466
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2101395274 - test_loss: 0.1954107329
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2076078570 - test_loss: 0.1912470867
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2057593000 - test_loss: 0.1879619191
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2044210347 - test_loss: 0.1853535928
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2034850589 - test_loss: 0.1832732379
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2028218290 - test_loss: 0.1816689755
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2023836518 - test_loss: 0.1804110146
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2020693081 - test_loss: 0.1795557699
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2018823621 - test_loss: 0.1787817865
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2017472757 - test_loss: 0.1782946528
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2016566114 - test_loss: 0.1779775865
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2015673330 - test_loss: 0.1777450772
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2015046290 - test_loss: 0.1774883442
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2014459657 - test_loss: 0.1773386682
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2013821961 - test_loss: 0.1772616550
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2013209602 - test_loss: 0.1771180072
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2012568393 - test_loss: 0.1769585314
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2012004131 - test_loss: 0.1769015292
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2011273789 - test_loss: 0.1769201178
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2010667429 - test_loss: 0.1766182416
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2009905811 - test_loss: 0.1766865600
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2009269131 - test_loss: 0.1764267257
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2008483166 - test_loss: 0.1764743779
Early Stop Left: 4
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2007589816 - test_loss: 0.1765054608
Early Stop Left: 3
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2006783318 - test_loss: 0.1763380644
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2005969667 - test_loss: 0.1763588384
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2005094904 - test_loss: 0.1764082585
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2004140015 - test_loss: 0.1763105772
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2003120465 - test_loss: 0.1763315910
Early Stop Left: 4
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2002189319 - test_loss: 0.1763769553
Early Stop Left: 3
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2000943250 - test_loss: 0.1763982118
Early Stop Left: 2
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.1999809947 - test_loss: 0.1763909330
Early Stop Left: 1
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.1998380220 - test_loss: 0.1764256950
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 192.35it/s] 38%|███▊      | 40/104 [00:00<00:00, 181.09it/s] 57%|█████▋    | 59/104 [00:00<00:00, 183.48it/s] 75%|███████▌  | 78/104 [00:00<00:00, 184.85it/s] 94%|█████████▍| 98/104 [00:00<00:00, 186.90it/s]100%|██████████| 104/104 [00:00<00:00, 187.21it/s]
Best micro threshold=0.284776, fscore=0.548
p,r,f1: 0.4748812620347209 0.6465876170712446 0.5475895931336008
throttleing by fixed threshold: 0.5
p,r,f1: 0.6577001016877236 0.33890299366887 0.4473125448572591
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.2847760319709778,
                 'p': 0.4748812620347209,
                 'r': 0.6465876170712446,
                 'f1': 0.5475895931336008},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6577001016877236,
                 'r': 0.33890299366887,
                 'f1': 0.4473125448572591}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5748614606 - test_loss: 0.6541725437
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4865303467 - test_loss: 0.5393566879
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4072530115 - test_loss: 0.4551463497
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3564034605 - test_loss: 0.3997673149
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3216810882 - test_loss: 0.3578335537
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2956475923 - test_loss: 0.3245659187
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2756915849 - test_loss: 0.2978428745
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2603709278 - test_loss: 0.2763179628
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2486827410 - test_loss: 0.2589595713
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2398581818 - test_loss: 0.2450072347
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2332803802 - test_loss: 0.2337880920
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2284567734 - test_loss: 0.2248646266
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2249811786 - test_loss: 0.2177937177
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2225231465 - test_loss: 0.2122996694
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2208161807 - test_loss: 0.2079912860
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2196673500 - test_loss: 0.2047033142
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2189159043 - test_loss: 0.2022828941
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2184278912 - test_loss: 0.2005125767
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2181281160 - test_loss: 0.1992144706
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2179313944 - test_loss: 0.1983088229
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2178185394 - test_loss: 0.1975788145
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2177306646 - test_loss: 0.1973254132
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2176885589 - test_loss: 0.1968699490
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2176498716 - test_loss: 0.1967026315
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2176212642 - test_loss: 0.1966696704
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2175789931 - test_loss: 0.1965895226
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2175486729 - test_loss: 0.1964348547
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2175172327 - test_loss: 0.1963085638
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2174797055 - test_loss: 0.1963031706
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2174429402 - test_loss: 0.1961977843
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2174020884 - test_loss: 0.1960142652
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2173657303 - test_loss: 0.1959826564
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2173161903 - test_loss: 0.1960734999
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2172750666 - test_loss: 0.1956026660
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2172220726 - test_loss: 0.1957751795
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2171789509 - test_loss: 0.1953901636
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2171255589 - test_loss: 0.1954880576
Early Stop Left: 4
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2170648707 - test_loss: 0.1955510038
Early Stop Left: 3
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2170125654 - test_loss: 0.1952602618
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2169612895 - test_loss: 0.1953491210
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2169060399 - test_loss: 0.1953575537
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2168474056 - test_loss: 0.1951957983
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2167863094 - test_loss: 0.1951454555
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2167344955 - test_loss: 0.1951784943
Early Stop Left: 4
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2166610705 - test_loss: 0.1951472147
Early Stop Left: 3
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2166013448 - test_loss: 0.1951395564
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2165250097 - test_loss: 0.1949873165
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2164343064 - test_loss: 0.1950301082
Early Stop Left: 4
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2163429959 - test_loss: 0.1948924692
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2201400612 - test_loss: 0.2063011854
Early Stop Left: 4
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2200374625 - test_loss: 0.2062294227
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2199166510 - test_loss: 0.2063096634
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2197749610 - test_loss: 0.2063700448
Early Stop Left: 3
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2196305206 - test_loss: 0.2062810323
Early Stop Left: 2
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2195048966 - test_loss: 0.2061852704
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2193933303 - test_loss: 0.2059724639
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2193008300 - test_loss: 0.2059430120
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2192126688 - test_loss: 0.2058215624
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2191345231 - test_loss: 0.2054733425
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2190712357 - test_loss: 0.2059979588
Early Stop Left: 4
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2190016518 - test_loss: 0.2058799939
Early Stop Left: 3
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2189503245 - test_loss: 0.2059383796
Early Stop Left: 2
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2189048506 - test_loss: 0.2060618595
Early Stop Left: 1
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2188542030 - test_loss: 0.2059507324
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 16%|█▋        | 17/104 [00:00<00:00, 169.08it/s] 34%|███▎      | 35/104 [00:00<00:00, 174.20it/s] 52%|█████▏    | 54/104 [00:00<00:00, 179.37it/s] 70%|███████   | 73/104 [00:00<00:00, 181.96it/s] 88%|████████▊ | 92/104 [00:00<00:00, 183.63it/s]100%|██████████| 104/104 [00:00<00:00, 182.97it/s]-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2094890338 - test_loss: 0.1848638182
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2093131745 - test_loss: 0.1848652187
Early Stop Left: 4
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2091374805 - test_loss: 0.1848762619
Early Stop Left: 3
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2089706721 - test_loss: 0.1848030888
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2088258996 - test_loss: 0.1846660695
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2087070055 - test_loss: 0.1845445003
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2085967122 - test_loss: 0.1844123639
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2085017645 - test_loss: 0.1844178623
Early Stop Left: 4
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2084042565 - test_loss: 0.1843794570
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2083140181 - test_loss: 0.1841174386
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2082373344 - test_loss: 0.1845631168
Early Stop Left: 4
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2081509308 - test_loss: 0.1845249605
Early Stop Left: 3
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2080858143 - test_loss: 0.1846510107
Early Stop Left: 2
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2080262510 - test_loss: 0.1847552568
Early Stop Left: 1
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2079594188 - test_loss: 0.1847010443
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 193.97it/s] 38%|███▊      | 40/104 [00:00<00:00, 194.11it/s] 58%|█████▊    | 60/104 [00:00<00:00, 193.59it/s] 77%|███████▋  | 80/104 [00:00<00:00, 194.14it/s] 96%|█████████▌| 100/104 [00:00<00:00, 193.12it/s]100%|██████████| 104/104 [00:00<00:00, 194.89it/s]
Best micro threshold=0.300802, fscore=0.551
p,r,f1: 0.47247455917629305 0.6604392021155285 0.5508640313918829
throttleing by fixed threshold: 0.5
p,r,f1: 0.6420024095893488 0.35554789059716935 0.4576463010338691
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.30080220103263855,
                 'p': 0.47247455917629305,
                 'r': 0.6604392021155285,
                 'f1': 0.5508640313918829},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6420024095893488,
                 'r': 0.35554789059716935,
                 'f1': 0.4576463010338691}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm

Best micro threshold=0.314323, fscore=0.552
p,r,f1: 0.47233231041961843 0.6628951301754962 0.5516194855853157
throttleing by fixed threshold: 0.5
p,r,f1: 0.6486681040234284 0.3481819912430992 0.4531364408494668
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.31432297825813293,
                 'p': 0.47233231041961843,
                 'r': 0.6628951301754962,
                 'f1': 0.5516194855853157},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6486681040234284,
                 'r': 0.3481819912430992,
                 'f1': 0.4531364408494668}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
-------- Save Best Model! --------
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2162355676 - test_loss: 0.1949075697
Early Stop Left: 4
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2160952822 - test_loss: 0.1948839816
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2159399495 - test_loss: 0.1949594878
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2157778472 - test_loss: 0.1949434662
Early Stop Left: 3
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2156302533 - test_loss: 0.1947959385
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2155086621 - test_loss: 0.1946676157
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2153992769 - test_loss: 0.1944804451
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2153073195 - test_loss: 0.1944742164
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2152157376 - test_loss: 0.1943950483
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2151326039 - test_loss: 0.1940775571
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2150634839 - test_loss: 0.1945766070
Early Stop Left: 4
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2149862815 - test_loss: 0.1945016169
Early Stop Left: 3
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2149288075 - test_loss: 0.1945998927
Early Stop Left: 2
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2148767352 - test_loss: 0.1947166133
Early Stop Left: 1
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2148185753 - test_loss: 0.1946198447
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 14%|█▍        | 15/104 [00:00<00:00, 148.25it/s] 30%|██▉       | 31/104 [00:00<00:00, 149.67it/s] 44%|████▍     | 46/104 [00:00<00:00, 149.46it/s] 59%|█████▊    | 61/104 [00:00<00:00, 149.32it/s] 73%|███████▎  | 76/104 [00:00<00:00, 149.04it/s] 88%|████████▊ | 91/104 [00:00<00:00, 149.19it/s]100%|██████████| 104/104 [00:00<00:00, 148.94it/s]
Best micro threshold=0.307087, fscore=0.551
p,r,f1: 0.4725253128945675 0.661649260300102 0.5513190493578493
throttleing by fixed threshold: 0.5
p,r,f1: 0.6468426432649315 0.35065488273556084 0.45477512537171505
{'model': 'vit',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.307087242603302,
                 'p': 0.4725253128945675,
                 'r': 0.661649260300102,
                 'f1': 0.5513190493578493},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6468426432649315,
                 'r': 0.35065488273556084,
                 'f1': 0.45477512537171505}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
Generation start
preprocessing_gen with context
b4 model prediction col0
 Index(['id', 'cycle', 'addr', 'ip', 'hit', 'raw', 'block_address',
       'page_address', 'page_offset', 'block_index', 'block_addr_delta',
       'patch', 'past', 'past_ip', 'past_page'],
      dtype='object')
predicting
  0%|          | 0/106 [00:00<?, ?it/s]  1%|          | 1/106 [00:01<02:51,  1.63s/it]  9%|▉         | 10/106 [00:01<00:12,  7.77it/s] 18%|█▊        | 19/106 [00:01<00:05, 16.07it/s] 26%|██▋       | 28/106 [00:01<00:03, 25.16it/s] 35%|███▍      | 37/106 [00:02<00:01, 34.57it/s] 43%|████▎     | 46/106 [00:02<00:01, 43.67it/s] 52%|█████▏    | 55/106 [00:02<00:00, 51.95it/s] 60%|██████    | 64/106 [00:02<00:00, 59.12it/s] 69%|██████▉   | 73/106 [00:02<00:00, 65.01it/s] 77%|███████▋  | 82/106 [00:02<00:00, 69.79it/s] 86%|████████▌ | 91/106 [00:02<00:00, 73.40it/s] 94%|█████████▍| 100/106 [00:02<00:00, 76.11it/s]100%|██████████| 106/106 [00:02<00:00, 36.63it/s]
after model prediction col1
 Index(['id', 'cycle', 'addr', 'ip', 'block_address', 'y_score'], dtype='object')
post_processing, opt_threshold<0.9
after delta filter
 Index(['id', 'pred_hex'], dtype='object')
               app  mean  max  min  median
0  437.leslie3d-s0   1.0  1.0  1.0     1.0
Done: results saved at: res/437.leslie3d-s0.vitt.pkl.degree_stats.csv
Generation start
preprocessing_gen with context
b4 model prediction col0
 Index(['id', 'cycle', 'addr', 'ip', 'hit', 'raw', 'block_address',
       'page_address', 'page_offset', 'block_index', 'block_addr_delta',
       'patch', 'past', 'past_ip', 'past_page'],
      dtype='object')
predicting
  0%|          | 0/106 [00:00<?, ?it/s]  1%|          | 1/106 [00:01<02:56,  1.68s/it] 24%|██▎       | 25/106 [00:01<00:04, 19.39it/s] 47%|████▋     | 50/106 [00:01<00:01, 42.49it/s] 71%|███████   | 75/106 [00:01<00:00, 68.42it/s] 93%|█████████▎| 99/106 [00:02<00:00, 93.86it/s]100%|██████████| 106/106 [00:02<00:00, 50.14it/s]
after model prediction col1
 Index(['id', 'cycle', 'addr', 'ip', 'block_address', 'y_score'], dtype='object')
post_processing, opt_threshold<0.9
after delta filter
 Index(['id', 'pred_hex'], dtype='object')
               app  mean  max  min  median
0  437.leslie3d-s0   1.0  1.0  1.0     1.0
Done: results saved at: res/437.leslie3d-s0.vit.stu.75.1.pkl.degree_stats.csv
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/437/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
/data/neelesh/DART_by_app/437/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.184, 0.271
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0165, 0.00859
--- total mse / var(X): 0.14
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0159, 0.0161
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0161, 0.0159
--- total mse / var(X): 0.016
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.189, 0.182
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.135, 0.139
--- total mse / var(X): 0.161
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.132, 0.123
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.216, 0.23
--- total mse / var(X): 0.176
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.038, 0.00661
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0131, 0.0239
--- total mse / var(X): 0.0152
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.204, 0.168
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.234, 0.275
--- total mse / var(X): 0.222
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 7.94e-07, 6.05e-07
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 8.28e-07, 1.02e-06
--- total mse / var(X): 8.15e-07
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.165, 0.167
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0783, 0.0774
--- total mse / var(X): 0.122
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00191, 0.00171
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0734, 0.0812
--- total mse / var(X): 0.0414
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.00076, 0.000457
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.000128, 0.00018
--- total mse / var(X): 0.000318
start table evaluation...
Elapsed time: 143.30267024040222 seconds
Cosine similarity between AMM and exact (Train): 0.9791228
Cosine similarity between AMM and exact (Test): 0.9793372
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.481230420393108 0.5744364841966778 0.523718816185726
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16],
               'cossim_layer_train': [0.991419792175293,
                                      0.97850102186203,
                                      0.9730565547943115,
                                      0.9791228771209717],
               'cossim_layer_test': [0.9873868823051453,
                                     0.9784481525421143,
                                     0.9727845191955566,
                                     0.9793371558189392],
               'cossim_amm_train': [0.9850297570228577,
                                    0.9924594163894653,
                                    0.9535022377967834,
                                    0.908700168132782,
                                    0.9515961408615112,
                                    0.9396005272865295,
                                    0.9596190452575684,
                                    0.9914041757583618],
               'cossim_amm_test': [0.9778163433074951,
                                   0.991725742816925,
                                   0.952557384967804,
                                   0.906773030757904,
                                   0.9500485062599182,
                                   0.9388520121574402,
                                   0.9589248895645142,
                                   0.9915526509284973],
               'f1': [0.481230420393108, 0.5744364841966778, 0.523718816185726],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 16),
                              (192, 2, 16),
                              (16, 16, 2),
                              (16, 16, 2),
                              (32, 2, 16),
                              (32, 2, 16),
                              (32, 2, 16),
                              (256, 2, 16)],
               'lut_total_size': 19456}}
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (8). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000002
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0661, 0.0973
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.000346, 0.000182
--- total mse / var(X): 0.0487
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0103, 0.0104
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00799, 0.00786
--- total mse / var(X): 0.00915
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00386, 0.00372
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00384, 0.00398
--- total mse / var(X): 0.00385
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00556, 0.00521
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00946, 0.0101
--- total mse / var(X): 0.00763
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00275, 0.000701
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.000792, 0.00138
--- total mse / var(X): 0.00104
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0314, 0.0259
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0418, 0.049
--- total mse / var(X): 0.0375
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00733, 0.00536
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00179, 0.00228
--- total mse / var(X): 0.00382
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00953, 0.00968
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00785, 0.00772
--- total mse / var(X): 0.0087
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0036, 0.00314
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00398, 0.00449
--- total mse / var(X): 0.00381
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00391, 0.00271
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00147, 0.00192
--- total mse / var(X): 0.00231
start table evaluation...
Elapsed time: 155.5989968776703 seconds
Cosine similarity between AMM and exact (Train): 0.98821485
Cosine similarity between AMM and exact (Test): 0.9882458
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.456559436736662 0.667450754213057 0.542220956440786
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64],
               'cossim_layer_train': [0.9968982338905334,
                                      0.9891536831855774,
                                      0.9857347011566162,
                                      0.9882148504257202],
               'cossim_layer_test': [0.9948887228965759,
                                     0.9893563389778137,
                                     0.985577404499054,
                                     0.9882457852363586],
               'cossim_amm_train': [0.994602620601654,
                                    0.9961360096931458,
                                    0.9946829676628113,
                                    0.9747483134269714,
                                    0.9726804494857788,
                                    0.9761015176773071,
                                    0.9813834428787231,
                                    0.9955530166625977],
               'cossim_amm_test': [0.9909282326698303,
                                   0.9950069785118103,
                                   0.9930561780929565,
                                   0.9744016528129578,
                                   0.9730263948440552,
                                   0.9746174216270447,
                                   0.9797830581665039,
                                   0.9954137206077576],
               'f1': [0.456559436736662, 0.667450754213057, 0.542220956440786],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 64),
                              (192, 2, 64),
                              (64, 64, 2),
                              (64, 64, 2),
                              (32, 2, 64),
                              (32, 2, 64),
                              (32, 2, 64),
                              (256, 2, 64)],
               'lut_total_size': 90112}}
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999994
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0189, 0.0278
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.000773, 0.000406
--- total mse / var(X): 0.0141
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00327, 0.00332
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00292, 0.00287
--- total mse / var(X): 0.00309
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0021, 0.00202
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00247, 0.00256
--- total mse / var(X): 0.00229
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00283, 0.00265
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00252, 0.00268
--- total mse / var(X): 0.00266
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00191, 0.000491
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.000488, 0.000851
--- total mse / var(X): 0.000671
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00398, 0.00328
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00565, 0.00664
--- total mse / var(X): 0.00496
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0159, 0.0123
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00873, 0.0107
--- total mse / var(X): 0.0115
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00497, 0.00498
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00514, 0.00514
--- total mse / var(X): 0.00506
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00261, 0.0023
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00261, 0.00292
--- total mse / var(X): 0.00261
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00343, 0.00251
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00155, 0.00196
--- total mse / var(X): 0.00223
start table evaluation...
Elapsed time: 209.74683332443237 seconds
Cosine similarity between AMM and exact (Train): 0.9935402
Cosine similarity between AMM and exact (Test): 0.9933111
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4572175007143123 0.6725906742587451 0.5443759992189312
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256],
               'cossim_layer_train': [0.9990865588188171,
                                      0.9978952407836914,
                                      0.997070848941803,
                                      0.9935402274131775],
               'cossim_layer_test': [0.9971669316291809,
                                     0.997300386428833,
                                     0.9963747262954712,
                                     0.9933111071586609],
               'cossim_amm_train': [0.9984104633331299,
                                    0.9987253546714783,
                                    0.998102068901062,
                                    0.9954813718795776,
                                    0.9948530197143555,
                                    0.9945569634437561,
                                    0.9953650236129761,
                                    0.9976763725280762],
               'cossim_amm_test': [0.9949666857719421,
                                   0.9975481629371643,
                                   0.9964181184768677,
                                   0.9936527609825134,
                                   0.9937084913253784,
                                   0.9933972358703613,
                                   0.9942874908447266,
                                   0.9973754286766052],
               'f1': [0.4572175007143123,
                      0.6725906742587451,
                      0.5443759992189312],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 256),
                              (192, 2, 256),
                              (256, 256, 2),
                              (256, 256, 2),
                              (32, 2, 256),
                              (32, 2, 256),
                              (32, 2, 256),
                              (256, 2, 256)],
               'lut_total_size': 557056}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00782, 0.0113
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 1.1e-05, 1.16e-05
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00296, 0.00443
running kmeans in subspace 4/4... X.shape:  (100000, 3)
k:  128
nnz_rows:  0
mse / {var(X_subs), var(X)}: 0, 0
--- total mse / var(X): 0.00394
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00464, 0.005
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00607, 0.00581
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00417, 0.00403
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00453, 0.00453
--- total mse / var(X): 0.00484
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00356, 0.00385
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0034, 0.00287
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00375, 0.00416
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00326, 0.00314
--- total mse / var(X): 0.00351
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00454, 0.00396
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00423, 0.00424
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00351, 0.0041
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00317, 0.00303
--- total mse / var(X): 0.00383
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00155, 0.000524
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0016, 0.000281
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0024, 0.000389
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000309, 0.00103
--- total mse / var(X): 0.000555
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00592, 0.00525
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00372, 0.00283
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00642, 0.00753
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00735, 0.00867
--- total mse / var(X): 0.00607
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0169, 0.0156
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0204, 0.0121
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0107, 0.013
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0104, 0.0132
--- total mse / var(X): 0.0135
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00677, 0.00663
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00727, 0.00746
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00638, 0.00706
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00736, 0.00654
--- total mse / var(X): 0.00692
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00244, 0.00241
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0036, 0.00277
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00314, 0.0047
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00422, 0.00314
--- total mse / var(X): 0.00325
running kmeans in subspace 1/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0338, 0.0215
running kmeans in subspace 2/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.028, 0.0231
running kmeans in subspace 3/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0151, 0.0253
running kmeans in subspace 4/4... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0251, 0.0217
--- total mse / var(X): 0.0229
start table evaluation...
Elapsed time: 243.36092066764832 seconds
Cosine similarity between AMM and exact (Train): 0.9918608
Cosine similarity between AMM and exact (Test): 0.99142426
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.45041956317607706 0.6816981526822014 0.5424350839987971
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.999758243560791,
                                      0.9971522092819214,
                                      0.9959498047828674,
                                      0.9918608069419861],
               'cossim_layer_test': [0.9991098642349243,
                                     0.9969096779823303,
                                     0.9956226944923401,
                                     0.9914242625236511],
               'cossim_amm_train': [0.9995793104171753,
                                    0.9983547925949097,
                                    0.9974685907363892,
                                    0.9937498569488525,
                                    0.9926177859306335,
                                    0.9922909140586853,
                                    0.9934727549552917,
                                    0.9970407485961914],
               'cossim_amm_test': [0.9984195232391357,
                                   0.9973575472831726,
                                   0.995969295501709,
                                   0.99221271276474,
                                   0.9918105006217957,
                                   0.9915139675140381,
                                   0.992766261100769,
                                   0.9967265725135803],
               'f1': [0.45041956317607706,
                      0.6816981526822014,
                      0.5424350839987971],
               'lut_num': 8,
               'lut_shapes': [(32, 4, 128),
                              (192, 4, 128),
                              (128, 128, 4),
                              (128, 128, 4),
                              (32, 4, 128),
                              (32, 4, 128),
                              (32, 4, 128),
                              (256, 4, 128)],
               'lut_total_size': 425984}}
/data/neelesh/DART_by_app/437/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (32). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (32). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999994
Manual and Torch results cosine similarity (Test): 1.0000004
start table training...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00613, 0.00904
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 1.69e-05, 8.85e-06
--- total mse / var(X): 0.00453
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00107, 0.00108
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00104, 0.00102
--- total mse / var(X): 0.00105
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000823, 0.000793
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000796, 0.000824
--- total mse / var(X): 0.000809
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.001, 0.000938
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000933, 0.000992
--- total mse / var(X): 0.000965
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00133, 0.000343
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.000309, 0.000538
--- total mse / var(X): 0.000441
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00188, 0.00155
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00299, 0.00352
--- total mse / var(X): 0.00253
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0113, 0.00888
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00682, 0.00827
--- total mse / var(X): 0.00858
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00242, 0.00242
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00269, 0.00269
--- total mse / var(X): 0.00255
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00133, 0.00118
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00147, 0.00164
--- total mse / var(X): 0.00141
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.0024, 0.00196
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 1024, 32
mse / {var(X_subs), var(X)}: 0.00184, 0.00218
--- total mse / var(X): 0.00207
start table evaluation...
Elapsed time: 404.9725503921509 seconds
Cosine similarity between AMM and exact (Train): 0.9971841
Cosine similarity between AMM and exact (Test): 0.99605554
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4637515348425625 0.6677372477396226 0.5473570366364743
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024,
                             1024],
               'cossim_layer_train': [0.9996858835220337,
                                      0.9993245601654053,
                                      0.9990249872207642,
                                      0.9971840977668762],
               'cossim_layer_test': [0.997848391532898,
                                     0.9985202550888062,
                                     0.9979953169822693,
                                     0.9960556626319885],
               'cossim_amm_train': [0.9994527101516724,
                                    0.99956214427948,
                                    0.9993343949317932,
                                    0.9986280798912048,
                                    0.9983522295951843,
                                    0.9980485439300537,
                                    0.9982872009277344,
                                    0.9988349080085754],
               'cossim_amm_test': [0.996178388595581,
                                   0.9987626671791077,
                                   0.9981913566589355,
                                   0.9971542954444885,
                                   0.9968301057815552,
                                   0.9963769912719727,
                                   0.9968456029891968,
                                   0.9983238577842712],
               'f1': [0.4637515348425625,
                      0.6677372477396226,
                      0.5473570366364743],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 1024),
                              (192, 2, 1024),
                              (1024, 1024, 2),
                              (1024, 1024, 2),
                              (32, 2, 1024),
                              (32, 2, 1024),
                              (32, 2, 1024),
                              (256, 2, 1024)],
               'lut_total_size': 5373952}}
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999964
Manual and Torch results cosine similarity (Test): 1.0000004
start table training with fine tuning...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0347, 0.0513
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000557, 0.000289
--- total mse / var(X): 0.0258
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00592, 0.00601
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00545, 0.00536
--- total mse / var(X): 0.00569
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00313, 0.00301
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00338, 0.00351
--- total mse / var(X): 0.00326
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00402, 0.00377
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00349, 0.00371
--- total mse / var(X): 0.00374
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00228, 0.000585
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00076, 0.00132
--- total mse / var(X): 0.000955
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00769, 0.00634
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0102, 0.0119
--- total mse / var(X): 0.00913
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0127, 0.00959
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00681, 0.00848
--- total mse / var(X): 0.00903
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00707, 0.00709
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00635, 0.00633
--- total mse / var(X): 0.00671
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00299, 0.00263
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00301, 0.00337
--- total mse / var(X): 0.003
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0107, 0.00547
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00357, 0.00532
--- total mse / var(X): 0.00539
start table evaluation...
Elapsed time: 121.47670936584473 seconds
Cosine similarity between AMM and exact (Train): 0.990152
Cosine similarity between AMM and exact (Test): 0.98983854
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4431926075098121 0.6906473057357135 0.5399171024461114
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9982587695121765,
                                      0.9946882724761963,
                                      0.9927372932434082,
                                      0.9901520013809204],
               'cossim_layer_test': [0.996036946773529,
                                     0.9943792223930359,
                                     0.9923412799835205,
                                     0.9898385405540466],
               'cossim_amm_train': [0.9969629049301147,
                                    0.9976957440376282,
                                    0.9967861175537109,
                                    0.988722026348114,
                                    0.98676598072052,
                                    0.9870343208312988,
                                    0.989870548248291,
                                    0.9964269399642944],
               'cossim_amm_test': [0.9929720759391785,
                                   0.9963288307189941,
                                   0.9950604438781738,
                                   0.9878477454185486,
                                   0.9863370060920715,
                                   0.98602694272995,
                                   0.9891649484634399,
                                   0.9961568117141724],
               'f1': [0.4431926075098121,
                      0.6906473057357135,
                      0.5399171024461114],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Traceback (most recent call last):
  File "src/3_2_vit_finetune.py", line 110, in <module>
    train_data, train_target, test_data, test_target, all_params, best_threshold = load_data_n_model(model_save_path, res_path)
  File "src/3_2_vit_finetune.py", line 55, in load_data_n_model
    with open(model_save_path+'.tensor_dict.pkl', 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'model/410.bwaves-s0.vit.stu.75.1.pkl.tensor_dict.pkl'
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.99999946
Manual and Torch results cosine similarity (Test): 1.0000004
start table training with fine tuning...
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0355, 0.0525
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000775, 0.000405
--- total mse / var(X): 0.0265
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00591, 0.00601
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00508, 0.00499
--- total mse / var(X): 0.0055
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00334, 0.00322
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00285, 0.00295
--- total mse / var(X): 0.00308
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00387, 0.00363
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00311, 0.0033
--- total mse / var(X): 0.00347
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00255, 0.000653
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000688, 0.0012
--- total mse / var(X): 0.000926
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00776, 0.0064
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0126, 0.0148
--- total mse / var(X): 0.0106
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0163, 0.0126
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00726, 0.00892
--- total mse / var(X): 0.0108
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00651, 0.00654
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00655, 0.00652
--- total mse / var(X): 0.00653
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00267, 0.00233
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00354, 0.00398
--- total mse / var(X): 0.00316
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00369, 0.00258
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0021, 0.00273
--- total mse / var(X): 0.00266
start table evaluation...
Elapsed time: 102.98760724067688 seconds
Cosine similarity between AMM and exact (Train): 0.99030364
Cosine similarity between AMM and exact (Test): 0.9899327
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4496114573399829 0.6807123887717156 0.5415369943717664
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.998221755027771,
                                      0.9946091175079346,
                                      0.992769181728363,
                                      0.990303635597229],
               'cossim_layer_test': [0.9959918260574341,
                                     0.9943427443504333,
                                     0.992315948009491,
                                     0.9899327158927917],
               'cossim_amm_train': [0.9969034194946289,
                                    0.9977518320083618,
                                    0.9971110224723816,
                                    0.9883835911750793,
                                    0.9866308569908142,
                                    0.9871513843536377,
                                    0.9901368618011475,
                                    0.9965478181838989],
               'cossim_amm_test': [0.9928815364837646,
                                   0.9962530136108398,
                                   0.9953227639198303,
                                   0.9869545698165894,
                                   0.9862242341041565,
                                   0.9859871864318848,
                                   0.9892231225967407,
                                   0.9961971044540405],
               'f1': [0.4496114573399829,
                      0.6807123887717156,
                      0.5415369943717664],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999995
Manual and Torch results cosine similarity (Test): 1.0000004
start table training with fine tuning...
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
Retrain for 1 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0345, 0.051
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000288, 0.00015
--- total mse / var(X): 0.0256
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:03<05:49,  3.53s/it]  2%|▏         | 2/100 [00:07<05:54,  3.62s/it]  3%|▎         | 3/100 [00:11<06:02,  3.74s/it]  4%|▍         | 4/100 [00:15<06:28,  4.05s/it]  5%|▌         | 5/100 [00:19<06:28,  4.09s/it]  6%|▌         | 6/100 [00:24<06:29,  4.14s/it]  7%|▋         | 7/100 [00:28<06:28,  4.17s/it]  8%|▊         | 8/100 [00:32<06:14,  4.07s/it]  9%|▉         | 9/100 [00:35<05:53,  3.89s/it] 10%|█         | 10/100 [00:39<05:41,  3.80s/it] 11%|█         | 11/100 [00:43<05:41,  3.84s/it] 12%|█▏        | 12/100 [00:47<05:41,  3.88s/it] 13%|█▎        | 13/100 [00:51<05:42,  3.94s/it] 14%|█▍        | 14/100 [00:55<05:40,  3.95s/it] 15%|█▌        | 15/100 [00:59<05:40,  4.01s/it] 16%|█▌        | 16/100 [01:03<05:42,  4.08s/it] 17%|█▋        | 17/100 [01:07<05:37,  4.07s/it] 18%|█▊        | 18/100 [01:11<05:33,  4.07s/it] 19%|█▉        | 19/100 [01:15<05:29,  4.07s/it] 20%|██        | 20/100 [01:19<05:21,  4.02s/it] 21%|██        | 21/100 [01:23<05:07,  3.89s/it] 22%|██▏       | 22/100 [01:26<04:50,  3.72s/it] 23%|██▎       | 23/100 [01:29<04:34,  3.56s/it] 24%|██▍       | 24/100 [01:32<04:23,  3.47s/it] 25%|██▌       | 25/100 [01:36<04:16,  3.42s/it] 26%|██▌       | 26/100 [01:39<04:11,  3.39s/it] 27%|██▋       | 27/100 [01:42<04:06,  3.38s/it] 28%|██▊       | 28/100 [01:46<04:00,  3.34s/it] 29%|██▉       | 29/100 [01:49<03:55,  3.32s/it] 30%|███       | 30/100 [01:52<03:52,  3.33s/it] 31%|███       | 31/100 [01:56<03:56,  3.42s/it] 32%|███▏      | 32/100 [02:00<04:03,  3.58s/it] 33%|███▎      | 33/100 [02:04<04:06,  3.68s/it] 34%|███▍      | 34/100 [02:07<03:59,  3.64s/it] 35%|███▌      | 35/100 [02:11<03:53,  3.60s/it] 36%|███▌      | 36/100 [02:14<03:48,  3.58s/it] 37%|███▋      | 37/100 [02:18<03:45,  3.57s/it] 38%|███▊      | 38/100 [02:21<03:39,  3.53s/it] 39%|███▉      | 39/100 [02:25<03:36,  3.55s/it] 40%|████      | 40/100 [02:28<03:30,  3.51s/it] 41%|████      | 41/100 [02:32<03:25,  3.49s/it] 42%|████▏     | 42/100 [02:36<03:29,  3.61s/it] 43%|████▎     | 43/100 [02:39<03:27,  3.64s/it] 44%|████▍     | 44/100 [02:43<03:21,  3.60s/it] 45%|████▌     | 45/100 [02:47<03:18,  3.61s/it] 46%|████▌     | 46/100 [02:50<03:17,  3.66s/it] 47%|████▋     | 47/100 [02:54<03:21,  3.81s/it] 48%|████▊     | 48/100 [02:58<03:17,  3.80s/it] 49%|████▉     | 49/100 [03:02<03:18,  3.89s/it] 50%|█████     | 50/100 [03:06<03:13,  3.86s/it] 51%|█████     | 51/100 [03:10<03:10,  3.88s/it] 52%|█████▏    | 52/100 [03:14<03:04,  3.84s/it] 53%|█████▎    | 53/100 [03:18<03:03,  3.90s/it] 54%|█████▍    | 54/100 [03:22<03:00,  3.93s/it] 55%|█████▌    | 55/100 [03:25<02:51,  3.82s/it] 56%|█████▌    | 56/100 [03:29<02:39,  3.62s/it] 57%|█████▋    | 57/100 [03:33<02:40,  3.72s/it] 58%|█████▊    | 58/100 [03:37<02:39,  3.80s/it] 59%|█████▉    | 59/100 [03:40<02:31,  3.71s/it] 60%|██████    | 60/100 [03:44<02:28,  3.71s/it] 61%|██████    | 61/100 [03:48<02:29,  3.84s/it] 62%|██████▏   | 62/100 [03:52<02:24,  3.81s/it] 63%|██████▎   | 63/100 [03:55<02:19,  3.76s/it] 64%|██████▍   | 64/100 [03:59<02:15,  3.77s/it] 65%|██████▌   | 65/100 [04:03<02:09,  3.70s/it] 66%|██████▌   | 66/100 [04:07<02:08,  3.77s/it] 67%|██████▋   | 67/100 [04:10<02:00,  3.66s/it] 68%|██████▊   | 68/100 [04:13<01:55,  3.62s/it] 69%|██████▉   | 69/100 [04:17<01:50,  3.56s/it] 70%|███████   | 70/100 [04:21<01:49,  3.66s/it] 71%|███████   | 71/100 [04:24<01:45,  3.63s/it] 72%|███████▏  | 72/100 [04:28<01:43,  3.71s/it] 73%|███████▎  | 73/100 [04:32<01:41,  3.76s/it] 74%|███████▍  | 74/100 [04:36<01:38,  3.79s/it] 75%|███████▌  | 75/100 [04:40<01:35,  3.82s/it] 76%|███████▌  | 76/100 [04:44<01:30,  3.78s/it] 77%|███████▋  | 77/100 [04:47<01:24,  3.67s/it] 78%|███████▊  | 78/100 [04:51<01:21,  3.69s/it] 79%|███████▉  | 79/100 [04:55<01:19,  3.79s/it] 80%|████████  | 80/100 [04:59<01:18,  3.91s/it] 81%|████████  | 81/100 [05:02<01:11,  3.79s/it] 82%|████████▏ | 82/100 [05:06<01:07,  3.72s/it] 83%|████████▎ | 83/100 [05:10<01:04,  3.79s/it] 84%|████████▍ | 84/100 [05:13<00:59,  3.72s/it] 85%|████████▌ | 85/100 [05:17<00:54,  3.66s/it] 86%|████████▌ | 86/100 [05:21<00:52,  3.75s/it] 87%|████████▋ | 87/100 [05:25<00:48,  3.70s/it] 88%|████████▊ | 88/100 [05:28<00:43,  3.66s/it] 89%|████████▉ | 89/100 [05:31<00:39,  3.57s/it] 90%|█████████ | 90/100 [05:35<00:35,  3.54s/it] 91%|█████████ | 91/100 [05:38<00:31,  3.51s/it] 92%|█████████▏| 92/100 [05:42<00:27,  3.45s/it] 93%|█████████▎| 93/100 [05:45<00:23,  3.41s/it] 94%|█████████▍| 94/100 [05:49<00:20,  3.44s/it] 95%|█████████▌| 95/100 [05:52<00:17,  3.40s/it] 96%|█████████▌| 96/100 [05:56<00:13,  3.48s/it] 97%|█████████▋| 97/100 [05:59<00:10,  3.53s/it] 98%|█████████▊| 98/100 [06:03<00:07,  3.59s/it] 99%|█████████▉| 99/100 [06:06<00:03,  3.57s/it]100%|██████████| 100/100 [06:10<00:00,  3.56s/it]100%|██████████| 100/100 [06:10<00:00,  3.70s/it]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0058, 0.00589
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00517, 0.00508
--- total mse / var(X): 0.00549
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00317, 0.00306
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00322, 0.00333
--- total mse / var(X): 0.0032
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00381, 0.00357
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0033, 0.00351
--- total mse / var(X): 0.00354
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00233, 0.000597
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00077, 0.00134
--- total mse / var(X): 0.00097
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00631, 0.0052
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.018, 0.0211
--- total mse / var(X): 0.0132
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:22,  4.40it/s]  2%|▏         | 2/100 [00:00<00:20,  4.82it/s]  3%|▎         | 3/100 [00:00<00:22,  4.33it/s]  4%|▍         | 4/100 [00:00<00:22,  4.28it/s]  5%|▌         | 5/100 [00:01<00:22,  4.28it/s]  6%|▌         | 6/100 [00:01<00:21,  4.44it/s]  7%|▋         | 7/100 [00:01<00:21,  4.24it/s]  8%|▊         | 8/100 [00:01<00:20,  4.53it/s]  9%|▉         | 9/100 [00:01<00:19,  4.72it/s] 10%|█         | 10/100 [00:02<00:19,  4.60it/s] 11%|█         | 11/100 [00:02<00:18,  4.69it/s] 12%|█▏        | 12/100 [00:02<00:18,  4.83it/s] 13%|█▎        | 13/100 [00:02<00:18,  4.80it/s] 14%|█▍        | 14/100 [00:03<00:17,  5.00it/s] 15%|█▌        | 15/100 [00:03<00:16,  5.10it/s] 16%|█▌        | 16/100 [00:03<00:16,  5.13it/s] 17%|█▋        | 17/100 [00:03<00:16,  4.94it/s] 18%|█▊        | 18/100 [00:03<00:16,  5.10it/s] 19%|█▉        | 19/100 [00:03<00:15,  5.20it/s] 20%|██        | 20/100 [00:04<00:15,  5.30it/s] 21%|██        | 21/100 [00:04<00:14,  5.29it/s] 22%|██▏       | 22/100 [00:04<00:14,  5.51it/s] 23%|██▎       | 23/100 [00:04<00:14,  5.46it/s] 24%|██▍       | 24/100 [00:04<00:13,  5.67it/s] 25%|██▌       | 25/100 [00:05<00:13,  5.45it/s] 26%|██▌       | 26/100 [00:05<00:13,  5.43it/s] 27%|██▋       | 27/100 [00:05<00:13,  5.37it/s] 28%|██▊       | 28/100 [00:05<00:13,  5.40it/s] 29%|██▉       | 29/100 [00:05<00:13,  5.36it/s] 30%|███       | 30/100 [00:05<00:13,  5.36it/s] 31%|███       | 31/100 [00:06<00:12,  5.42it/s] 32%|███▏      | 32/100 [00:06<00:12,  5.55it/s] 33%|███▎      | 33/100 [00:06<00:11,  5.66it/s] 34%|███▍      | 34/100 [00:06<00:11,  5.82it/s] 35%|███▌      | 35/100 [00:06<00:13,  4.79it/s] 36%|███▌      | 36/100 [00:07<00:17,  3.71it/s] 37%|███▋      | 37/100 [00:07<00:15,  3.95it/s] 38%|███▊      | 38/100 [00:07<00:13,  4.56it/s] 39%|███▉      | 39/100 [00:07<00:12,  5.00it/s] 40%|████      | 40/100 [00:08<00:10,  5.53it/s] 41%|████      | 41/100 [00:08<00:10,  5.64it/s] 42%|████▏     | 42/100 [00:08<00:09,  5.97it/s] 43%|████▎     | 43/100 [00:08<00:09,  6.14it/s] 44%|████▍     | 44/100 [00:08<00:09,  6.11it/s] 45%|████▌     | 45/100 [00:08<00:08,  6.27it/s] 46%|████▌     | 46/100 [00:08<00:08,  6.28it/s] 47%|████▋     | 47/100 [00:09<00:08,  6.52it/s] 48%|████▊     | 48/100 [00:09<00:07,  6.50it/s] 49%|████▉     | 49/100 [00:09<00:07,  6.58it/s] 50%|█████     | 50/100 [00:09<00:07,  6.88it/s] 51%|█████     | 51/100 [00:09<00:07,  6.99it/s] 52%|█████▏    | 52/100 [00:09<00:06,  6.98it/s] 53%|█████▎    | 53/100 [00:09<00:06,  7.07it/s] 54%|█████▍    | 54/100 [00:10<00:06,  7.01it/s] 55%|█████▌    | 55/100 [00:10<00:06,  7.02it/s] 56%|█████▌    | 56/100 [00:10<00:06,  6.44it/s] 57%|█████▋    | 57/100 [00:10<00:06,  6.35it/s] 58%|█████▊    | 58/100 [00:10<00:07,  5.90it/s] 59%|█████▉    | 59/100 [00:10<00:06,  5.99it/s] 60%|██████    | 60/100 [00:11<00:06,  5.86it/s] 61%|██████    | 61/100 [00:11<00:06,  5.73it/s] 62%|██████▏   | 62/100 [00:11<00:06,  5.55it/s] 63%|██████▎   | 63/100 [00:11<00:06,  5.76it/s] 64%|██████▍   | 64/100 [00:11<00:06,  5.48it/s] 65%|██████▌   | 65/100 [00:12<00:07,  4.86it/s] 66%|██████▌   | 66/100 [00:12<00:07,  4.49it/s] 67%|██████▋   | 67/100 [00:12<00:07,  4.47it/s] 68%|██████▊   | 68/100 [00:12<00:07,  4.13it/s] 69%|██████▉   | 69/100 [00:13<00:07,  4.26it/s] 70%|███████   | 70/100 [00:13<00:06,  4.59it/s] 71%|███████   | 71/100 [00:13<00:06,  4.60it/s] 72%|███████▏  | 72/100 [00:13<00:05,  4.81it/s] 73%|███████▎  | 73/100 [00:13<00:05,  4.91it/s] 74%|███████▍  | 74/100 [00:14<00:05,  5.15it/s] 75%|███████▌  | 75/100 [00:14<00:05,  4.82it/s] 76%|███████▌  | 76/100 [00:14<00:04,  5.41it/s] 77%|███████▋  | 77/100 [00:14<00:03,  5.76it/s] 78%|███████▊  | 78/100 [00:14<00:03,  5.84it/s] 79%|███████▉  | 79/100 [00:14<00:03,  5.92it/s] 80%|████████  | 80/100 [00:15<00:03,  6.39it/s] 81%|████████  | 81/100 [00:15<00:02,  7.13it/s] 82%|████████▏ | 82/100 [00:15<00:02,  7.76it/s] 84%|████████▍ | 84/100 [00:15<00:01,  8.69it/s] 86%|████████▌ | 86/100 [00:15<00:01,  8.53it/s] 87%|████████▋ | 87/100 [00:15<00:01,  8.67it/s] 88%|████████▊ | 88/100 [00:15<00:01,  8.73it/s] 89%|████████▉ | 89/100 [00:16<00:01,  8.35it/s] 90%|█████████ | 90/100 [00:16<00:01,  8.68it/s] 91%|█████████ | 91/100 [00:16<00:01,  8.77it/s] 92%|█████████▏| 92/100 [00:16<00:00,  8.98it/s] 93%|█████████▎| 93/100 [00:16<00:00,  8.93it/s] 94%|█████████▍| 94/100 [00:16<00:00,  8.82it/s] 95%|█████████▌| 95/100 [00:16<00:00,  9.08it/s] 97%|█████████▋| 97/100 [00:16<00:00,  9.29it/s] 98%|█████████▊| 98/100 [00:17<00:00,  9.41it/s] 99%|█████████▉| 99/100 [00:17<00:00,  9.50it/s]100%|██████████| 100/100 [00:17<00:00,  9.14it/s]100%|██████████| 100/100 [00:17<00:00,  5.80it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.0125, 0.00946
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00781, 0.0097
--- total mse / var(X): 0.00958
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:11,  8.77it/s]  2%|▏         | 2/100 [00:00<00:12,  7.76it/s]  3%|▎         | 3/100 [00:00<00:12,  7.50it/s]  4%|▍         | 4/100 [00:00<00:12,  7.71it/s]  5%|▌         | 5/100 [00:00<00:13,  7.28it/s]  6%|▌         | 6/100 [00:00<00:12,  7.73it/s]  7%|▋         | 7/100 [00:00<00:11,  7.93it/s]  8%|▊         | 8/100 [00:01<00:11,  7.87it/s]  9%|▉         | 9/100 [00:01<00:12,  7.38it/s] 10%|█         | 10/100 [00:01<00:11,  7.51it/s] 11%|█         | 11/100 [00:01<00:11,  7.58it/s] 12%|█▏        | 12/100 [00:01<00:11,  7.58it/s] 13%|█▎        | 13/100 [00:01<00:11,  7.58it/s] 14%|█▍        | 14/100 [00:01<00:11,  7.22it/s] 15%|█▌        | 15/100 [00:01<00:11,  7.46it/s] 16%|█▌        | 16/100 [00:02<00:10,  7.79it/s] 17%|█▋        | 17/100 [00:02<00:10,  7.95it/s] 18%|█▊        | 18/100 [00:02<00:10,  7.86it/s] 19%|█▉        | 19/100 [00:02<00:10,  7.49it/s] 20%|██        | 20/100 [00:02<00:10,  7.40it/s] 21%|██        | 21/100 [00:02<00:10,  7.78it/s] 22%|██▏       | 22/100 [00:02<00:09,  8.14it/s] 23%|██▎       | 23/100 [00:02<00:09,  8.29it/s] 24%|██▍       | 24/100 [00:03<00:09,  8.38it/s] 25%|██▌       | 25/100 [00:03<00:09,  8.21it/s] 26%|██▌       | 26/100 [00:03<00:09,  7.55it/s] 27%|██▋       | 27/100 [00:03<00:09,  7.54it/s] 28%|██▊       | 28/100 [00:03<00:09,  7.44it/s] 29%|██▉       | 29/100 [00:03<00:09,  7.77it/s] 30%|███       | 30/100 [00:03<00:09,  7.54it/s] 31%|███       | 31/100 [00:04<00:08,  7.67it/s] 32%|███▏      | 32/100 [00:04<00:09,  7.49it/s] 33%|███▎      | 33/100 [00:04<00:09,  7.22it/s] 34%|███▍      | 34/100 [00:04<00:09,  7.17it/s] 35%|███▌      | 35/100 [00:04<00:09,  7.17it/s] 36%|███▌      | 36/100 [00:04<00:08,  7.22it/s] 37%|███▋      | 37/100 [00:04<00:08,  7.14it/s] 38%|███▊      | 38/100 [00:05<00:08,  7.31it/s] 39%|███▉      | 39/100 [00:05<00:08,  7.40it/s] 40%|████      | 40/100 [00:05<00:07,  7.62it/s] 41%|████      | 41/100 [00:05<00:07,  7.71it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.69it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.40it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.09it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.27it/s] 47%|████▋     | 47/100 [00:06<00:06,  8.20it/s] 48%|████▊     | 48/100 [00:06<00:06,  8.42it/s] 49%|████▉     | 49/100 [00:06<00:05,  8.53it/s] 51%|█████     | 51/100 [00:06<00:05,  9.24it/s] 52%|█████▏    | 52/100 [00:06<00:05,  9.25it/s] 53%|█████▎    | 53/100 [00:06<00:05,  9.24it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.48it/s] 57%|█████▋    | 57/100 [00:07<00:04,  9.90it/s] 58%|█████▊    | 58/100 [00:07<00:04,  9.87it/s] 60%|██████    | 60/100 [00:07<00:03, 10.28it/s] 62%|██████▏   | 62/100 [00:07<00:03, 10.45it/s] 64%|██████▍   | 64/100 [00:07<00:03, 10.63it/s] 66%|██████▌   | 66/100 [00:07<00:03, 10.29it/s] 68%|██████▊   | 68/100 [00:08<00:03,  9.96it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.81it/s] 70%|███████   | 70/100 [00:08<00:03,  9.51it/s] 71%|███████   | 71/100 [00:08<00:03,  9.30it/s] 72%|███████▏  | 72/100 [00:08<00:02,  9.44it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.26it/s] 74%|███████▍  | 74/100 [00:08<00:02,  8.84it/s] 75%|███████▌  | 75/100 [00:09<00:02,  8.45it/s] 76%|███████▌  | 76/100 [00:09<00:02,  8.73it/s] 77%|███████▋  | 77/100 [00:09<00:02,  8.88it/s] 78%|███████▊  | 78/100 [00:09<00:02,  8.23it/s] 79%|███████▉  | 79/100 [00:09<00:02,  8.41it/s] 80%|████████  | 80/100 [00:09<00:02,  8.79it/s] 81%|████████  | 81/100 [00:09<00:02,  8.94it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.12it/s] 85%|████████▌ | 85/100 [00:10<00:01,  9.62it/s] 86%|████████▌ | 86/100 [00:10<00:01,  9.69it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.10it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.01it/s] 89%|████████▉ | 89/100 [00:10<00:01,  8.55it/s] 90%|█████████ | 90/100 [00:10<00:01,  8.62it/s] 91%|█████████ | 91/100 [00:10<00:01,  8.34it/s] 92%|█████████▏| 92/100 [00:10<00:00,  8.34it/s] 93%|█████████▎| 93/100 [00:11<00:00,  8.11it/s] 94%|█████████▍| 94/100 [00:11<00:00,  8.19it/s] 95%|█████████▌| 95/100 [00:11<00:00,  8.21it/s] 96%|█████████▌| 96/100 [00:11<00:00,  7.48it/s] 97%|█████████▋| 97/100 [00:11<00:00,  6.94it/s] 98%|█████████▊| 98/100 [00:11<00:00,  7.42it/s] 99%|█████████▉| 99/100 [00:11<00:00,  7.62it/s]100%|██████████| 100/100 [00:12<00:00,  6.53it/s]100%|██████████| 100/100 [00:12<00:00,  8.28it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00806, 0.00804
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00779, 0.00781
--- total mse / var(X): 0.00793
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:08, 11.11it/s]  4%|▍         | 4/100 [00:00<00:09,  9.64it/s]  5%|▌         | 5/100 [00:00<00:10,  9.46it/s]  7%|▋         | 7/100 [00:00<00:09,  9.68it/s]  8%|▊         | 8/100 [00:00<00:09,  9.49it/s]  9%|▉         | 9/100 [00:00<00:09,  9.42it/s] 10%|█         | 10/100 [00:01<00:09,  9.48it/s] 12%|█▏        | 12/100 [00:01<00:09,  9.69it/s] 14%|█▍        | 14/100 [00:01<00:08, 10.26it/s] 16%|█▌        | 16/100 [00:01<00:08, 10.00it/s] 18%|█▊        | 18/100 [00:01<00:08,  9.97it/s] 19%|█▉        | 19/100 [00:01<00:08,  9.77it/s] 21%|██        | 21/100 [00:02<00:07, 10.28it/s] 23%|██▎       | 23/100 [00:02<00:07, 10.18it/s] 25%|██▌       | 25/100 [00:02<00:06, 10.72it/s] 27%|██▋       | 27/100 [00:02<00:06, 11.27it/s] 29%|██▉       | 29/100 [00:02<00:06, 10.74it/s] 31%|███       | 31/100 [00:03<00:06, 10.94it/s] 33%|███▎      | 33/100 [00:03<00:06, 10.75it/s] 35%|███▌      | 35/100 [00:03<00:06, 10.54it/s] 37%|███▋      | 37/100 [00:03<00:05, 10.58it/s] 39%|███▉      | 39/100 [00:03<00:05, 10.95it/s] 41%|████      | 41/100 [00:03<00:05, 11.14it/s] 43%|████▎     | 43/100 [00:04<00:05, 10.66it/s] 45%|████▌     | 45/100 [00:04<00:04, 11.69it/s] 47%|████▋     | 47/100 [00:04<00:04, 11.93it/s] 49%|████▉     | 49/100 [00:04<00:04, 11.73it/s] 51%|█████     | 51/100 [00:05<00:07,  6.98it/s] 52%|█████▏    | 52/100 [00:05<00:07,  6.62it/s] 54%|█████▍    | 54/100 [00:05<00:05,  7.97it/s] 56%|█████▌    | 56/100 [00:05<00:04,  9.13it/s] 58%|█████▊    | 58/100 [00:05<00:04,  9.94it/s] 60%|██████    | 60/100 [00:06<00:03, 10.06it/s] 62%|██████▏   | 62/100 [00:06<00:03, 10.52it/s] 64%|██████▍   | 64/100 [00:06<00:03, 10.81it/s] 66%|██████▌   | 66/100 [00:06<00:03, 10.91it/s] 68%|██████▊   | 68/100 [00:06<00:02, 11.01it/s] 70%|███████   | 70/100 [00:06<00:02, 11.05it/s] 72%|███████▏  | 72/100 [00:07<00:02, 11.37it/s] 74%|███████▍  | 74/100 [00:07<00:02, 11.41it/s] 76%|███████▌  | 76/100 [00:07<00:02, 11.95it/s] 78%|███████▊  | 78/100 [00:07<00:01, 12.52it/s] 80%|████████  | 80/100 [00:07<00:01, 13.21it/s] 82%|████████▏ | 82/100 [00:07<00:01, 13.93it/s] 84%|████████▍ | 84/100 [00:07<00:01, 14.25it/s] 86%|████████▌ | 86/100 [00:08<00:00, 14.38it/s] 88%|████████▊ | 88/100 [00:08<00:00, 15.17it/s] 90%|█████████ | 90/100 [00:08<00:00, 15.61it/s] 92%|█████████▏| 92/100 [00:08<00:00, 15.96it/s] 94%|█████████▍| 94/100 [00:08<00:00, 15.38it/s] 96%|█████████▌| 96/100 [00:08<00:00, 14.58it/s] 98%|█████████▊| 98/100 [00:08<00:00, 14.34it/s]100%|██████████| 100/100 [00:08<00:00, 14.77it/s]100%|██████████| 100/100 [00:08<00:00, 11.14it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00269, 0.00239
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00407, 0.00453
--- total mse / var(X): 0.00346
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:05, 17.66it/s]  4%|▍         | 4/100 [00:00<00:06, 15.54it/s]  6%|▌         | 6/100 [00:00<00:06, 15.07it/s]  8%|▊         | 8/100 [00:00<00:05, 16.34it/s] 10%|█         | 10/100 [00:00<00:05, 17.09it/s] 12%|█▏        | 12/100 [00:00<00:05, 16.20it/s] 14%|█▍        | 14/100 [00:00<00:05, 15.98it/s] 16%|█▌        | 16/100 [00:00<00:05, 16.38it/s] 18%|█▊        | 18/100 [00:01<00:05, 16.00it/s] 20%|██        | 20/100 [00:01<00:05, 15.86it/s] 22%|██▏       | 22/100 [00:01<00:04, 15.81it/s] 24%|██▍       | 24/100 [00:01<00:04, 15.93it/s] 27%|██▋       | 27/100 [00:01<00:04, 17.10it/s] 29%|██▉       | 29/100 [00:01<00:04, 17.39it/s] 31%|███       | 31/100 [00:01<00:03, 17.45it/s] 34%|███▍      | 34/100 [00:02<00:03, 17.94it/s] 36%|███▌      | 36/100 [00:02<00:03, 17.91it/s] 38%|███▊      | 38/100 [00:02<00:03, 18.14it/s] 40%|████      | 40/100 [00:02<00:03, 18.59it/s] 42%|████▏     | 42/100 [00:02<00:03, 17.59it/s] 44%|████▍     | 44/100 [00:02<00:03, 17.36it/s] 46%|████▌     | 46/100 [00:02<00:03, 15.61it/s] 49%|████▉     | 49/100 [00:02<00:02, 17.23it/s] 51%|█████     | 51/100 [00:03<00:02, 17.56it/s] 54%|█████▍    | 54/100 [00:03<00:02, 19.73it/s] 56%|█████▌    | 56/100 [00:03<00:02, 18.00it/s] 59%|█████▉    | 59/100 [00:03<00:02, 18.64it/s] 61%|██████    | 61/100 [00:03<00:02, 18.45it/s] 64%|██████▍   | 64/100 [00:03<00:01, 19.64it/s] 66%|██████▌   | 66/100 [00:03<00:01, 18.86it/s] 68%|██████▊   | 68/100 [00:03<00:01, 17.50it/s] 70%|███████   | 70/100 [00:04<00:01, 17.77it/s] 72%|███████▏  | 72/100 [00:04<00:01, 18.02it/s] 75%|███████▌  | 75/100 [00:04<00:01, 19.42it/s] 77%|███████▋  | 77/100 [00:04<00:01, 19.43it/s] 79%|███████▉  | 79/100 [00:04<00:01, 18.75it/s] 81%|████████  | 81/100 [00:04<00:01, 16.77it/s] 83%|████████▎ | 83/100 [00:04<00:00, 17.39it/s] 85%|████████▌ | 85/100 [00:04<00:00, 17.07it/s] 87%|████████▋ | 87/100 [00:04<00:00, 17.59it/s] 90%|█████████ | 90/100 [00:05<00:00, 18.97it/s] 92%|█████████▏| 92/100 [00:05<00:00, 18.38it/s] 94%|█████████▍| 94/100 [00:05<00:00, 18.39it/s] 96%|█████████▌| 96/100 [00:05<00:00, 18.62it/s] 98%|█████████▊| 98/100 [00:05<00:00, 18.45it/s]100%|██████████| 100/100 [00:05<00:00, 18.00it/s]100%|██████████| 100/100 [00:05<00:00, 17.62it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.00101, 0.000757
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 128, 12
mse / {var(X_subs), var(X)}: 0.000841, 0.00105
--- total mse / var(X): 0.000905
start table evaluation...
Elapsed time: 118.22426271438599 seconds
Cosine similarity between AMM and exact (Train): 0.99458873
Cosine similarity between AMM and exact (Test): 0.9937253
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.4744259503328659 0.6364076727489997 0.5436067268209082
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128,
                             128],
               'cossim_layer_train': [0.9983572959899902,
                                      0.9972733855247498,
                                      0.996353030204773,
                                      0.9945887327194214],
               'cossim_layer_test': [0.9963802695274353,
                                     0.9967865347862244,
                                     0.9956217408180237,
                                     0.9937252998352051],
               'cossim_amm_train': [0.9971400499343872,
                                    0.9977587461471558,
                                    0.9969255328178406,
                                    0.9867649674415588,
                                    0.9935149550437927,
                                    0.9928291440010071,
                                    0.9942933917045593,
                                    0.9980881810188293],
               'cossim_amm_test': [0.9935691356658936,
                                   0.9964739680290222,
                                   0.9952053427696228,
                                   0.9851962327957153,
                                   0.9925605058670044,
                                   0.990229606628418,
                                   0.9928097724914551,
                                   0.9977247714996338],
               'f1': [0.4744259503328659,
                      0.6364076727489997,
                      0.5436067268209082],
               'lut_num': 8,
               'lut_shapes': [(32, 2, 128),
                              (192, 2, 128),
                              (128, 128, 2),
                              (128, 128, 2),
                              (32, 2, 128),
                              (32, 2, 128),
                              (32, 2, 128),
                              (256, 2, 128)],
               'lut_total_size': 212992}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5004456132 - test_loss: 0.5926514595
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.4179426702 - test_loss: 0.4963800956
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.3636558595 - test_loss: 0.4334849967
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.3264382194 - test_loss: 0.3848499017
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.2984238285 - test_loss: 0.3461319864
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.2772029512 - test_loss: 0.3152206213
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.2612555991 - test_loss: 0.2906365311
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.2494390843 - test_loss: 0.2711573682
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.2408014387 - test_loss: 0.2558157169
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2346002531 - test_loss: 0.2438152756
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2302216775 - test_loss: 0.2345413024
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2271847093 - test_loss: 0.2273719454
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2251168225 - test_loss: 0.2219243265
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2237301568 - test_loss: 0.2179650282
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2228201846 - test_loss: 0.2150087457
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2222347772 - test_loss: 0.2128743185
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2218510116 - test_loss: 0.2113216857
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2216180563 - test_loss: 0.2101076434
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2214575003 - test_loss: 0.2094859017
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2213484693 - test_loss: 0.2088665285
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2212861247 - test_loss: 0.2086005593
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2212163142 - test_loss: 0.2083410043
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2211724068 - test_loss: 0.2080587483
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2211308772 - test_loss: 0.2077553671
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2210895819 - test_loss: 0.2079110757
Early Stop Left: 4
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2210532097 - test_loss: 0.2075262541
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2209997287 - test_loss: 0.2077565034
Early Stop Left: 4
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2209464884 - test_loss: 0.2074448800
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2208650577 - test_loss: 0.2072646215
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2207675956 - test_loss: 0.2072705208
Early Stop Left: 4
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2206046525 - test_loss: 0.2070918598
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2203425115 - test_loss: 0.2066687088
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2200734246 - test_loss: 0.2066980226
Early Stop Left: 4
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2198487622 - test_loss: 0.2060573329
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2196743232 - test_loss: 0.2061829511
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2195243410 - test_loss: 0.2058819497
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2193896582 - test_loss: 0.2055707514
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2192854921 - test_loss: 0.2056352019
Early Stop Left: 4
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2191852413 - test_loss: 0.2053931719
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2190943963 - test_loss: 0.2055696824
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2190132902 - test_loss: 0.2059549629
Early Stop Left: 3
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2189451564 - test_loss: 0.2053624233
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2188737339 - test_loss: 0.2052205834
-------- Save Best Model! --------
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2188042876 - test_loss: 0.2053730065
Early Stop Left: 4
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2187263806 - test_loss: 0.2052398708
Early Stop Left: 3
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2186661869 - test_loss: 0.2052344941
Early Stop Left: 2
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2185968235 - test_loss: 0.2054723722
Early Stop Left: 1
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2185304134 - test_loss: 0.2054389215
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 15%|█▌        | 16/104 [00:00<00:00, 153.58it/s] 31%|███       | 32/104 [00:00<00:00, 152.75it/s] 46%|████▌     | 48/104 [00:00<00:00, 151.79it/s] 62%|██████▏   | 64/104 [00:00<00:00, 150.57it/s] 77%|███████▋  | 80/104 [00:00<00:00, 149.25it/s] 92%|█████████▏| 96/104 [00:00<00:00, 150.01it/s]100%|██████████| 104/104 [00:00<00:00, 151.46it/s]===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        1,376
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 6,128
Trainable params: 6,128
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        1,376
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 6,128
Trainable params: 6,128
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5494589365 - test_loss: 0.6932253843
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5115155777 - test_loss: 0.6420348585
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4729940918 - test_loss: 0.5889463488
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4341168755 - test_loss: 0.5357199821
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3981240508 - test_loss: 0.4884180057
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3685690946 - test_loss: 0.4500925558
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3451388754 - test_loss: 0.4185858375
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3258658283 - test_loss: 0.3915783989
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3094856907 - test_loss: 0.3678437640
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2953688065 - test_loss: 0.3467543509
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2831538464 - test_loss: 0.3279385790
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2726004273 - test_loss: 0.3111663429
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2635210284 - test_loss: 0.2962462326
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2557665214 - test_loss: 0.2830081860
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2491850798 - test_loss: 0.2713404325
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2436477614 - test_loss: 0.2611077806
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2390434974 - test_loss: 0.2521704997
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2352446135 - test_loss: 0.2444467324
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2321513523 - test_loss: 0.2378105659
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2296594621 - test_loss: 0.2321783917
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2276764725 - test_loss: 0.2274372449
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2261148211 - test_loss: 0.2234469486
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2249002462 - test_loss: 0.2202034977
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2239749841 - test_loss: 0.2175537719
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2232644970 - test_loss: 0.2153594207
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2227329986 - test_loss: 0.2137215511
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2223404789 - test_loss: 0.2123946232
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2220473421 - test_loss: 0.2113120552
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2218344864 - test_loss: 0.2105271335
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2216840277 - test_loss: 0.2099061641
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2215712501 - test_loss: 0.2094876607
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2214947262 - test_loss: 0.2091608853
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2214461502 - test_loss: 0.2089870912
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2214046201 - test_loss: 0.2088330749
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2213833994 - test_loss: 0.2086454630
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2213632082 - test_loss: 0.2085139022
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2213599992 - test_loss: 0.2085046656
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2213521089 - test_loss: 0.2084204337
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2213402290 - test_loss: 0.2083803268
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2213380162 - test_loss: 0.2083092246
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2213377059 - test_loss: 0.2083295559
Early Stop Left: 4
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2213317197 - test_loss: 0.2082925895
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2213320207 - test_loss: 0.2083065991
Early Stop Left: 4
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2213260327 - test_loss: 0.2082625308
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2213204049 - test_loss: 0.2082866761
Early Stop Left: 4
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2213177117 - test_loss: 0.2082794101
Early Stop Left: 3
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2213147926 - test_loss: 0.2082922033
Early Stop Left: 2
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2213074015 - test_loss: 0.2082511063
-------- Save Best Model! --------
------- START EPOCH 49 -------

Best micro threshold=0.331877, fscore=0.554
p,r,f1: 0.4834772044338266 0.6480672053498896 0.553801897687244
throttleing by fixed threshold: 0.5
p,r,f1: 0.6405026424726344 0.36891319057662475 0.4681715219258065
{'model': 'vit_large',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3318767845630646,
                 'p': 0.4834772044338266,
                 'r': 0.6480672053498896,
                 'f1': 0.553801897687244},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6405026424726344,
                 'r': 0.36891319057662475,
                 'f1': 0.4681715219258065}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
Epoch: 49 - loss: 0.2213072038 - test_loss: 0.2083841217
Early Stop Left: 4
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2212992612 - test_loss: 0.2082057065
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2212959607 - test_loss: 0.2081997367
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2212942764 - test_loss: 0.2082974011
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2212907043 - test_loss: 0.2081732756
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2212830713 - test_loss: 0.2081742760
Early Stop Left: 4
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2212714219 - test_loss: 0.2081880491
Early Stop Left: 3
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2212684333 - test_loss: 0.2082325177
Early Stop Left: 2
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2212638010 - test_loss: 0.2081805889
Early Stop Left: 1
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2212563562 - test_loss: 0.2081376767
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2212460164 - test_loss: 0.2081446612
Early Stop Left: 4
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2212329041 - test_loss: 0.2081694346
Early Stop Left: 3
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2212269486 - test_loss: 0.2079959562
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2212206823 - test_loss: 0.2080788410
Early Stop Left: 4
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2212104568 - test_loss: 0.2078619628
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2211940433 - test_loss: 0.2080182370
Early Stop Left: 4
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2211847395 - test_loss: 0.2078462316
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2211682990 - test_loss: 0.2078476760
Early Stop Left: 4
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2211545909 - test_loss: 0.2078596442
Early Stop Left: 3
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2211415228 - test_loss: 0.2078547333
Early Stop Left: 2
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2211242691 - test_loss: 0.2078242167
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2211064829 - test_loss: 0.2076066502
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2210895925 - test_loss: 0.2077015701
Early Stop Left: 4
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2210754828 - test_loss: 0.2077398959
Early Stop Left: 3
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2210477020 - test_loss: 0.2076668725
Early Stop Left: 2
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2210283381 - test_loss: 0.2076187909
Early Stop Left: 1
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2210117223 - test_loss: 0.2075141362
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2209869201 - test_loss: 0.2075398275
Early Stop Left: 4
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2209660000 - test_loss: 0.2074942701
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2209397041 - test_loss: 0.2074090720
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2209147456 - test_loss: 0.2074086010
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2208895404 - test_loss: 0.2072703515
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2208683055 - test_loss: 0.2073397579
Early Stop Left: 4
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2208416044 - test_loss: 0.2072299616
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.2208180996 - test_loss: 0.2071679977
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.2207871204 - test_loss: 0.2072749823
Early Stop Left: 4
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.2207613918 - test_loss: 0.2072129252
Early Stop Left: 3
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.2207327369 - test_loss: 0.2071907758
Early Stop Left: 2
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.2207115146 - test_loss: 0.2071497326
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.2206837042 - test_loss: 0.2070702254
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.2206569457 - test_loss: 0.2070980781
Early Stop Left: 4
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.2206258168 - test_loss: 0.2071010056
Early Stop Left: 3
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.2205928642 - test_loss: 0.2069821965
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.2205739135 - test_loss: 0.2070291611
Early Stop Left: 4
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.2205425543 - test_loss: 0.2069405290
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.2205177511 - test_loss: 0.2069210978
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.2204909774 - test_loss: 0.2068049207
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.2204590574 - test_loss: 0.2069406098
Early Stop Left: 4
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.2204387131 - test_loss: 0.2069002943
Early Stop Left: 3
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.2204130554 - test_loss: 0.2068287777
Early Stop Left: 2
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.2203761973 - test_loss: 0.2067684233
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.2203506164 - test_loss: 0.2067148565
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.2203220676 - test_loss: 0.2066843723
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.2202966739 - test_loss: 0.2068733802
Early Stop Left: 4
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.2202610596 - test_loss: 0.2066493322
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.2202284432 - test_loss: 0.2066719383
Early Stop Left: 4
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.2201982928 - test_loss: 0.2066675485
Early Stop Left: 3
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.2201689583 - test_loss: 0.2065901809
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.2201237912 - test_loss: 0.2065402544
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.2201065864 - test_loss: 0.2066578885
Early Stop Left: 4
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.2200679501 - test_loss: 0.2066047011
Early Stop Left: 3
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.2200341686 - test_loss: 0.2065712722
Early Stop Left: 2
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.2199905414 - test_loss: 0.2065157529
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.2199564906 - test_loss: 0.2065902668
Early Stop Left: 4
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.2199174409 - test_loss: 0.2064357364
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.2198731154 - test_loss: 0.2064557866
Early Stop Left: 4
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.2198329227 - test_loss: 0.2064253027
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.2197840148 - test_loss: 0.2064239960
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.2197459107 - test_loss: 0.2064214790
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.2196969238 - test_loss: 0.2064585713
Early Stop Left: 4
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.2196471836 - test_loss: 0.2063437457
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.2195934427 - test_loss: 0.2063846091
Early Stop Left: 4
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.2195500948 - test_loss: 0.2061960370
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.2195002075 - test_loss: 0.2062914985
Early Stop Left: 4
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.2194572098 - test_loss: 0.2061815633
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.2194069512 - test_loss: 0.2061584953
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.2193644702 - test_loss: 0.2061209451
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.2193233077 - test_loss: 0.2060514128
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.2192791173 - test_loss: 0.2060912805
Early Stop Left: 4
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.2192407345 - test_loss: 0.2061141887
Early Stop Left: 3
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.2192018092 - test_loss: 0.2059340054
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.2191643673 - test_loss: 0.2058447902
-------- Save Best Model! --------
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.2191267902 - test_loss: 0.2057746640
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.2190971221 - test_loss: 0.2058013976
Early Stop Left: 4
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.2190631518 - test_loss: 0.2057297723
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.2190363983 - test_loss: 0.2057188393
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.2190069950 - test_loss: 0.2057027000
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.2189794611 - test_loss: 0.2058316624
Early Stop Left: 4
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.2189473433 - test_loss: 0.2056874637
-------- Save Best Model! --------
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.2189202940 - test_loss: 0.2057336651
Early Stop Left: 4
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.2188966959 - test_loss: 0.2054453610
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.2188729626 - test_loss: 0.2055815014
Early Stop Left: 4
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.2188510249 - test_loss: 0.2054647061
Early Stop Left: 3
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.2188291295 - test_loss: 0.2054423883
-------- Save Best Model! --------
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.2188066345 - test_loss: 0.2052658195
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.2187854328 - test_loss: 0.2053353680
Early Stop Left: 4
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.2187735962 - test_loss: 0.2054102524
Early Stop Left: 3
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.2187548152 - test_loss: 0.2051801720
-------- Save Best Model! --------
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.2187381917 - test_loss: 0.2054348833
Early Stop Left: 4
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.2187194262 - test_loss: 0.2052862627
Early Stop Left: 3
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.2187002635 - test_loss: 0.2052836322
Early Stop Left: 2
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.2186847420 - test_loss: 0.2053670266
Early Stop Left: 1
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.2186762021 - test_loss: 0.2051866489
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 19%|█▉        | 20/104 [00:00<00:00, 198.15it/s] 38%|███▊      | 40/104 [00:00<00:00, 196.33it/s] 58%|█████▊    | 60/104 [00:00<00:00, 195.46it/s] 77%|███████▋  | 80/104 [00:00<00:00, 195.20it/s] 96%|█████████▌| 100/104 [00:00<00:00, 189.59it/s]100%|██████████| 104/104 [00:00<00:00, 193.35it/s]
Best micro threshold=0.317660, fscore=0.556
p,r,f1: 0.47984307118819286 0.6613910391610264 0.5561766943511064
throttleing by fixed threshold: 0.5
p,r,f1: 0.6501412496358673 0.36176593101914417 0.4648627047319185
{'model': 'vit_min',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.31766006350517273,
                 'p': 0.47984307118819286,
                 'r': 0.6613910391610264,
                 'f1': 0.5561766943511064},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6501412496358673,
                 'r': 0.36176593101914417,
                 'f1': 0.4648627047319185}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 19,712
Trainable params: 19,712
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 0.9999993
Manual and Torch results cosine similarity (Test): 1.0000004
start table training with fine tuning...
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Retrain for 1 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0678, 0.0678
--- total mse / var(X): 0.0678
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:05<09:50,  5.97s/it]  2%|▏         | 2/100 [00:11<09:45,  5.98s/it]  3%|▎         | 3/100 [00:17<09:24,  5.82s/it]  4%|▍         | 4/100 [00:23<09:09,  5.72s/it]  5%|▌         | 5/100 [00:28<08:41,  5.49s/it]  6%|▌         | 6/100 [00:35<09:30,  6.07s/it]  7%|▋         | 7/100 [00:40<09:08,  5.90s/it]  8%|▊         | 8/100 [00:46<08:58,  5.85s/it]  9%|▉         | 9/100 [00:53<09:25,  6.21s/it] 10%|█         | 10/100 [00:58<08:46,  5.85s/it] 11%|█         | 11/100 [01:04<08:24,  5.67s/it] 12%|█▏        | 12/100 [01:10<08:37,  5.88s/it] 13%|█▎        | 13/100 [01:17<09:01,  6.23s/it] 14%|█▍        | 14/100 [01:24<09:19,  6.51s/it] 15%|█▌        | 15/100 [01:29<08:27,  5.97s/it] 16%|█▌        | 16/100 [01:34<07:49,  5.59s/it] 17%|█▋        | 17/100 [01:40<08:12,  5.93s/it] 18%|█▊        | 18/100 [01:47<08:18,  6.08s/it] 19%|█▉        | 19/100 [01:51<07:36,  5.64s/it] 20%|██        | 20/100 [01:56<07:17,  5.47s/it] 21%|██        | 21/100 [02:03<07:28,  5.68s/it] 22%|██▏       | 22/100 [02:08<07:28,  5.75s/it] 23%|██▎       | 23/100 [02:14<07:12,  5.62s/it] 24%|██▍       | 24/100 [02:19<06:59,  5.52s/it] 25%|██▌       | 25/100 [02:25<07:11,  5.76s/it] 26%|██▌       | 26/100 [02:32<07:15,  5.88s/it] 27%|██▋       | 27/100 [02:37<06:59,  5.75s/it] 28%|██▊       | 28/100 [02:43<06:56,  5.79s/it] 29%|██▉       | 29/100 [02:49<07:00,  5.92s/it] 30%|███       | 30/100 [02:56<07:08,  6.12s/it] 31%|███       | 31/100 [03:02<07:01,  6.11s/it] 32%|███▏      | 32/100 [03:06<06:20,  5.60s/it] 33%|███▎      | 33/100 [03:11<06:05,  5.46s/it] 34%|███▍      | 34/100 [03:19<06:38,  6.04s/it] 35%|███▌      | 35/100 [03:24<06:13,  5.74s/it] 36%|███▌      | 36/100 [03:29<05:59,  5.61s/it] 37%|███▋      | 37/100 [03:35<05:53,  5.60s/it] 38%|███▊      | 38/100 [03:40<05:50,  5.66s/it] 39%|███▉      | 39/100 [03:46<05:47,  5.69s/it] 40%|████      | 40/100 [03:52<05:39,  5.65s/it] 41%|████      | 41/100 [03:57<05:34,  5.66s/it] 42%|████▏     | 42/100 [04:04<05:36,  5.80s/it] 43%|████▎     | 43/100 [04:10<05:46,  6.09s/it] 44%|████▍     | 44/100 [04:16<05:42,  6.11s/it] 45%|████▌     | 45/100 [04:22<05:26,  5.94s/it] 46%|████▌     | 46/100 [04:28<05:19,  5.91s/it] 47%|████▋     | 47/100 [04:33<05:03,  5.73s/it] 48%|████▊     | 48/100 [04:39<04:56,  5.69s/it] 49%|████▉     | 49/100 [04:44<04:49,  5.68s/it] 50%|█████     | 50/100 [04:49<04:33,  5.47s/it] 51%|█████     | 51/100 [04:53<04:07,  5.05s/it] 52%|█████▏    | 52/100 [05:00<04:17,  5.37s/it] 53%|█████▎    | 53/100 [05:06<04:33,  5.82s/it] 54%|█████▍    | 54/100 [05:12<04:20,  5.67s/it] 55%|█████▌    | 55/100 [05:17<04:10,  5.56s/it] 56%|█████▌    | 56/100 [05:23<04:09,  5.66s/it] 57%|█████▋    | 57/100 [05:30<04:15,  5.95s/it] 58%|█████▊    | 58/100 [05:35<04:04,  5.82s/it] 59%|█████▉    | 59/100 [05:40<03:51,  5.64s/it] 60%|██████    | 60/100 [05:47<03:53,  5.85s/it] 61%|██████    | 61/100 [05:53<03:55,  6.05s/it] 62%|██████▏   | 62/100 [06:01<04:05,  6.46s/it] 63%|██████▎   | 63/100 [06:08<04:04,  6.62s/it] 64%|██████▍   | 64/100 [06:13<03:45,  6.26s/it] 65%|██████▌   | 65/100 [06:17<03:20,  5.71s/it] 66%|██████▌   | 66/100 [06:23<03:07,  5.53s/it] 67%|██████▋   | 67/100 [06:28<03:04,  5.59s/it] 68%|██████▊   | 68/100 [06:35<03:07,  5.85s/it] 69%|██████▉   | 69/100 [06:42<03:14,  6.27s/it] 70%|███████   | 70/100 [06:47<02:57,  5.92s/it] 71%|███████   | 71/100 [06:52<02:47,  5.76s/it] 72%|███████▏  | 72/100 [06:58<02:38,  5.67s/it] 73%|███████▎  | 73/100 [07:05<02:43,  6.04s/it] 74%|███████▍  | 74/100 [07:10<02:32,  5.87s/it] 75%|███████▌  | 75/100 [07:16<02:22,  5.69s/it] 76%|███████▌  | 76/100 [07:21<02:14,  5.59s/it] 77%|███████▋  | 77/100 [07:27<02:11,  5.72s/it] 78%|███████▊  | 78/100 [07:34<02:12,  6.00s/it] 79%|███████▉  | 79/100 [07:39<01:59,  5.69s/it] 80%|████████  | 80/100 [07:44<01:53,  5.70s/it] 81%|████████  | 81/100 [07:50<01:47,  5.67s/it] 82%|████████▏ | 82/100 [07:56<01:45,  5.84s/it] 83%|████████▎ | 83/100 [08:03<01:45,  6.18s/it] 84%|████████▍ | 84/100 [08:09<01:38,  6.17s/it] 85%|████████▌ | 85/100 [08:15<01:31,  6.10s/it] 86%|████████▌ | 86/100 [08:22<01:27,  6.23s/it] 87%|████████▋ | 87/100 [08:27<01:17,  5.95s/it] 88%|████████▊ | 88/100 [08:33<01:09,  5.83s/it] 89%|████████▉ | 89/100 [08:38<01:03,  5.73s/it] 90%|█████████ | 90/100 [08:44<00:56,  5.70s/it] 91%|█████████ | 91/100 [08:50<00:53,  5.92s/it] 92%|█████████▏| 92/100 [08:56<00:46,  5.83s/it] 93%|█████████▎| 93/100 [09:01<00:39,  5.67s/it] 94%|█████████▍| 94/100 [09:06<00:32,  5.41s/it] 95%|█████████▌| 95/100 [09:13<00:29,  5.86s/it] 96%|█████████▌| 96/100 [09:20<00:25,  6.28s/it] 97%|█████████▋| 97/100 [09:26<00:18,  6.31s/it] 98%|█████████▊| 98/100 [09:32<00:11,  5.95s/it] 99%|█████████▉| 99/100 [09:37<00:05,  5.74s/it]100%|██████████| 100/100 [09:42<00:00,  5.52s/it]100%|██████████| 100/100 [09:42<00:00,  5.82s/it]
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00851, 0.00851
--- total mse / var(X): 0.00851
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00305, 0.00305
--- total mse / var(X): 0.00305
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0042, 0.0042
--- total mse / var(X): 0.0042
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00116, 0.00116
--- total mse / var(X): 0.00116
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0658, 0.0658
--- total mse / var(X): 0.0658
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:20,  4.84it/s]  2%|▏         | 2/100 [00:00<00:17,  5.49it/s]  3%|▎         | 3/100 [00:00<00:18,  5.16it/s]  4%|▍         | 4/100 [00:00<00:18,  5.22it/s]  5%|▌         | 5/100 [00:00<00:19,  4.95it/s]  6%|▌         | 6/100 [00:01<00:18,  5.00it/s]  7%|▋         | 7/100 [00:01<00:19,  4.89it/s]  8%|▊         | 8/100 [00:01<00:19,  4.63it/s]  9%|▉         | 9/100 [00:01<00:19,  4.69it/s] 10%|█         | 10/100 [00:02<00:18,  4.87it/s] 11%|█         | 11/100 [00:02<00:17,  5.18it/s] 12%|█▏        | 12/100 [00:02<00:16,  5.28it/s] 13%|█▎        | 13/100 [00:02<00:16,  5.23it/s] 14%|█▍        | 14/100 [00:02<00:16,  5.16it/s] 15%|█▌        | 15/100 [00:03<00:17,  4.92it/s] 16%|█▌        | 16/100 [00:03<00:16,  5.13it/s] 17%|█▋        | 17/100 [00:03<00:17,  4.73it/s] 18%|█▊        | 18/100 [00:03<00:18,  4.34it/s] 19%|█▉        | 19/100 [00:03<00:17,  4.60it/s] 20%|██        | 20/100 [00:04<00:17,  4.63it/s] 21%|██        | 21/100 [00:04<00:16,  4.93it/s] 22%|██▏       | 22/100 [00:04<00:15,  4.88it/s] 23%|██▎       | 23/100 [00:04<00:15,  5.11it/s] 24%|██▍       | 24/100 [00:04<00:15,  5.06it/s] 25%|██▌       | 25/100 [00:05<00:14,  5.14it/s] 26%|██▌       | 26/100 [00:05<00:14,  5.01it/s] 27%|██▋       | 27/100 [00:05<00:14,  5.15it/s] 28%|██▊       | 28/100 [00:05<00:14,  4.97it/s] 29%|██▉       | 29/100 [00:05<00:14,  4.90it/s] 30%|███       | 30/100 [00:06<00:13,  5.05it/s] 31%|███       | 31/100 [00:06<00:13,  5.03it/s] 32%|███▏      | 32/100 [00:06<00:13,  5.00it/s] 33%|███▎      | 33/100 [00:06<00:13,  4.93it/s] 34%|███▍      | 34/100 [00:06<00:13,  5.07it/s] 35%|███▌      | 35/100 [00:07<00:13,  4.90it/s] 36%|███▌      | 36/100 [00:07<00:12,  5.12it/s] 37%|███▋      | 37/100 [00:07<00:12,  4.91it/s] 38%|███▊      | 38/100 [00:07<00:12,  5.02it/s] 39%|███▉      | 39/100 [00:07<00:11,  5.17it/s] 40%|████      | 40/100 [00:08<00:11,  5.25it/s] 41%|████      | 41/100 [00:08<00:12,  4.91it/s] 42%|████▏     | 42/100 [00:08<00:12,  4.71it/s] 43%|████▎     | 43/100 [00:08<00:11,  4.88it/s] 44%|████▍     | 44/100 [00:08<00:10,  5.23it/s] 45%|████▌     | 45/100 [00:09<00:10,  5.19it/s] 46%|████▌     | 46/100 [00:09<00:10,  5.37it/s] 47%|████▋     | 47/100 [00:09<00:09,  5.33it/s] 48%|████▊     | 48/100 [00:09<00:09,  5.45it/s] 49%|████▉     | 49/100 [00:09<00:09,  5.32it/s] 50%|█████     | 50/100 [00:09<00:09,  5.18it/s] 51%|█████     | 51/100 [00:10<00:09,  5.17it/s] 52%|█████▏    | 52/100 [00:10<00:09,  5.15it/s] 53%|█████▎    | 53/100 [00:10<00:08,  5.28it/s] 54%|█████▍    | 54/100 [00:10<00:08,  5.30it/s] 55%|█████▌    | 55/100 [00:10<00:08,  5.34it/s] 56%|█████▌    | 56/100 [00:11<00:08,  5.29it/s] 57%|█████▋    | 57/100 [00:11<00:08,  5.19it/s] 58%|█████▊    | 58/100 [00:11<00:07,  5.36it/s] 59%|█████▉    | 59/100 [00:11<00:07,  5.57it/s] 60%|██████    | 60/100 [00:11<00:07,  5.51it/s] 61%|██████    | 61/100 [00:12<00:07,  5.51it/s] 62%|██████▏   | 62/100 [00:12<00:06,  5.55it/s] 63%|██████▎   | 63/100 [00:12<00:06,  5.68it/s] 64%|██████▍   | 64/100 [00:12<00:06,  5.58it/s] 65%|██████▌   | 65/100 [00:12<00:06,  5.43it/s] 66%|██████▌   | 66/100 [00:12<00:06,  5.66it/s] 67%|██████▋   | 67/100 [00:13<00:06,  5.49it/s] 68%|██████▊   | 68/100 [00:13<00:05,  5.49it/s] 69%|██████▉   | 69/100 [00:13<00:05,  5.41it/s] 70%|███████   | 70/100 [00:13<00:05,  5.41it/s] 71%|███████   | 71/100 [00:13<00:05,  5.49it/s] 72%|███████▏  | 72/100 [00:14<00:05,  5.19it/s] 73%|███████▎  | 73/100 [00:14<00:05,  5.24it/s] 74%|███████▍  | 74/100 [00:14<00:04,  5.41it/s] 75%|███████▌  | 75/100 [00:14<00:04,  5.52it/s] 76%|███████▌  | 76/100 [00:14<00:04,  5.52it/s] 77%|███████▋  | 77/100 [00:14<00:03,  5.75it/s] 78%|███████▊  | 78/100 [00:15<00:03,  5.87it/s] 79%|███████▉  | 79/100 [00:15<00:03,  5.68it/s] 80%|████████  | 80/100 [00:15<00:03,  5.79it/s] 81%|████████  | 81/100 [00:15<00:03,  5.62it/s] 82%|████████▏ | 82/100 [00:15<00:03,  5.58it/s] 83%|████████▎ | 83/100 [00:15<00:02,  5.85it/s] 84%|████████▍ | 84/100 [00:16<00:02,  5.73it/s] 85%|████████▌ | 85/100 [00:16<00:02,  5.73it/s] 86%|████████▌ | 86/100 [00:16<00:02,  5.67it/s] 87%|████████▋ | 87/100 [00:16<00:02,  5.69it/s] 88%|████████▊ | 88/100 [00:16<00:02,  5.60it/s] 89%|████████▉ | 89/100 [00:17<00:02,  5.49it/s] 90%|█████████ | 90/100 [00:17<00:01,  5.33it/s] 91%|█████████ | 91/100 [00:17<00:01,  5.59it/s] 92%|█████████▏| 92/100 [00:17<00:01,  5.93it/s] 93%|█████████▎| 93/100 [00:17<00:01,  5.89it/s] 94%|█████████▍| 94/100 [00:17<00:00,  6.12it/s] 95%|█████████▌| 95/100 [00:18<00:00,  5.78it/s] 96%|█████████▌| 96/100 [00:18<00:00,  5.94it/s] 97%|█████████▋| 97/100 [00:18<00:00,  5.92it/s] 98%|█████████▊| 98/100 [00:18<00:00,  6.15it/s] 99%|█████████▉| 99/100 [00:18<00:00,  5.97it/s]100%|██████████| 100/100 [00:18<00:00,  5.86it/s]100%|██████████| 100/100 [00:18<00:00,  5.30it/s]
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.00461, 0.00461
--- total mse / var(X): 0.00461
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:17,  5.68it/s]  2%|▏         | 2/100 [00:00<00:15,  6.47it/s]  3%|▎         | 3/100 [00:00<00:18,  5.28it/s]  4%|▍         | 4/100 [00:00<00:17,  5.43it/s]  5%|▌         | 5/100 [00:00<00:18,  5.27it/s]  6%|▌         | 6/100 [00:01<00:16,  5.55it/s]  7%|▋         | 7/100 [00:01<00:15,  6.06it/s]  9%|▉         | 9/100 [00:01<00:12,  7.29it/s] 10%|█         | 10/100 [00:01<00:11,  7.59it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.53it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.83it/s] 15%|█▌        | 15/100 [00:02<00:09,  9.17it/s] 17%|█▋        | 17/100 [00:02<00:09,  9.01it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.99it/s] 21%|██        | 21/100 [00:02<00:08,  8.91it/s] 22%|██▏       | 22/100 [00:02<00:08,  9.08it/s] 23%|██▎       | 23/100 [00:02<00:09,  8.54it/s] 25%|██▌       | 25/100 [00:03<00:08,  8.86it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.88it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.98it/s] 29%|██▉       | 29/100 [00:03<00:07,  9.06it/s] 31%|███       | 31/100 [00:03<00:07,  9.46it/s] 33%|███▎      | 33/100 [00:04<00:06,  9.62it/s] 35%|███▌      | 35/100 [00:04<00:07,  8.94it/s] 36%|███▌      | 36/100 [00:04<00:08,  7.92it/s] 38%|███▊      | 38/100 [00:04<00:07,  8.06it/s] 39%|███▉      | 39/100 [00:04<00:07,  8.37it/s] 40%|████      | 40/100 [00:04<00:07,  7.85it/s] 41%|████      | 41/100 [00:05<00:07,  8.09it/s] 42%|████▏     | 42/100 [00:05<00:07,  8.10it/s] 43%|████▎     | 43/100 [00:05<00:07,  7.73it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.19it/s] 45%|████▌     | 45/100 [00:05<00:06,  7.88it/s] 47%|████▋     | 47/100 [00:05<00:06,  8.69it/s] 49%|████▉     | 49/100 [00:06<00:05,  8.59it/s] 50%|█████     | 50/100 [00:06<00:05,  8.67it/s] 51%|█████     | 51/100 [00:06<00:05,  8.27it/s] 52%|█████▏    | 52/100 [00:06<00:05,  8.59it/s] 53%|█████▎    | 53/100 [00:06<00:05,  8.07it/s] 55%|█████▌    | 55/100 [00:06<00:05,  8.28it/s] 57%|█████▋    | 57/100 [00:06<00:04,  8.80it/s] 59%|█████▉    | 59/100 [00:07<00:04,  9.49it/s] 60%|██████    | 60/100 [00:07<00:04,  9.03it/s] 61%|██████    | 61/100 [00:07<00:04,  8.99it/s] 63%|██████▎   | 63/100 [00:07<00:03,  9.57it/s] 64%|██████▍   | 64/100 [00:07<00:04,  7.94it/s] 65%|██████▌   | 65/100 [00:07<00:04,  7.29it/s] 66%|██████▌   | 66/100 [00:08<00:05,  6.44it/s] 67%|██████▋   | 67/100 [00:08<00:05,  6.33it/s] 68%|██████▊   | 68/100 [00:08<00:04,  6.48it/s] 69%|██████▉   | 69/100 [00:08<00:05,  6.07it/s] 70%|███████   | 70/100 [00:08<00:04,  6.24it/s] 71%|███████   | 71/100 [00:09<00:06,  4.51it/s] 72%|███████▏  | 72/100 [00:09<00:06,  4.53it/s] 74%|███████▍  | 74/100 [00:09<00:04,  5.61it/s] 75%|███████▌  | 75/100 [00:09<00:04,  5.59it/s] 76%|███████▌  | 76/100 [00:09<00:04,  5.89it/s] 77%|███████▋  | 77/100 [00:10<00:03,  6.04it/s] 78%|███████▊  | 78/100 [00:10<00:03,  6.27it/s] 79%|███████▉  | 79/100 [00:10<00:03,  6.37it/s] 80%|████████  | 80/100 [00:10<00:02,  6.82it/s] 81%|████████  | 81/100 [00:10<00:02,  6.68it/s] 82%|████████▏ | 82/100 [00:10<00:02,  7.33it/s] 83%|████████▎ | 83/100 [00:10<00:02,  7.12it/s] 84%|████████▍ | 84/100 [00:11<00:02,  6.84it/s] 85%|████████▌ | 85/100 [00:11<00:02,  7.10it/s] 86%|████████▌ | 86/100 [00:11<00:02,  6.96it/s] 87%|████████▋ | 87/100 [00:11<00:01,  7.17it/s] 88%|████████▊ | 88/100 [00:11<00:01,  7.48it/s] 89%|████████▉ | 89/100 [00:11<00:01,  7.58it/s] 90%|█████████ | 90/100 [00:11<00:01,  7.29it/s] 91%|█████████ | 91/100 [00:12<00:01,  7.85it/s] 92%|█████████▏| 92/100 [00:12<00:01,  7.93it/s] 93%|█████████▎| 93/100 [00:12<00:00,  8.15it/s] 94%|█████████▍| 94/100 [00:12<00:00,  7.57it/s] 95%|█████████▌| 95/100 [00:12<00:00,  8.10it/s] 96%|█████████▌| 96/100 [00:12<00:00,  7.54it/s] 97%|█████████▋| 97/100 [00:12<00:00,  7.44it/s] 98%|█████████▊| 98/100 [00:12<00:00,  7.15it/s] 99%|█████████▉| 99/100 [00:13<00:00,  7.59it/s]100%|██████████| 100/100 [00:13<00:00,  7.31it/s]100%|██████████| 100/100 [00:13<00:00,  7.56it/s]
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0101, 0.0101
--- total mse / var(X): 0.0101
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:09, 10.54it/s]  4%|▍         | 4/100 [00:00<00:08, 11.05it/s]  6%|▌         | 6/100 [00:00<00:08, 10.62it/s]  8%|▊         | 8/100 [00:00<00:08, 10.29it/s] 10%|█         | 10/100 [00:00<00:08, 10.02it/s] 12%|█▏        | 12/100 [00:01<00:08, 10.40it/s] 14%|█▍        | 14/100 [00:01<00:08, 10.00it/s] 16%|█▌        | 16/100 [00:01<00:08, 10.03it/s] 18%|█▊        | 18/100 [00:01<00:08, 10.14it/s] 20%|██        | 20/100 [00:01<00:08,  9.94it/s] 22%|██▏       | 22/100 [00:02<00:07, 10.49it/s] 24%|██▍       | 24/100 [00:02<00:07, 10.83it/s] 26%|██▌       | 26/100 [00:02<00:06, 11.27it/s] 28%|██▊       | 28/100 [00:02<00:06, 10.77it/s] 30%|███       | 30/100 [00:02<00:07,  9.99it/s] 32%|███▏      | 32/100 [00:03<00:06,  9.82it/s] 33%|███▎      | 33/100 [00:03<00:06,  9.70it/s] 35%|███▌      | 35/100 [00:03<00:06, 10.30it/s] 37%|███▋      | 37/100 [00:03<00:06,  9.54it/s] 38%|███▊      | 38/100 [00:03<00:06,  9.15it/s] 40%|████      | 40/100 [00:04<00:06,  8.72it/s] 41%|████      | 41/100 [00:04<00:06,  8.86it/s] 42%|████▏     | 42/100 [00:04<00:11,  4.90it/s] 43%|████▎     | 43/100 [00:04<00:11,  4.82it/s] 44%|████▍     | 44/100 [00:04<00:10,  5.33it/s] 45%|████▌     | 45/100 [00:05<00:09,  5.91it/s] 46%|████▌     | 46/100 [00:05<00:08,  6.58it/s] 47%|████▋     | 47/100 [00:05<00:07,  6.91it/s] 48%|████▊     | 48/100 [00:05<00:06,  7.51it/s] 49%|████▉     | 49/100 [00:05<00:06,  7.34it/s] 50%|█████     | 50/100 [00:05<00:06,  7.89it/s] 52%|█████▏    | 52/100 [00:05<00:05,  8.75it/s] 53%|█████▎    | 53/100 [00:06<00:05,  8.70it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.53it/s] 57%|█████▋    | 57/100 [00:06<00:04, 10.27it/s] 59%|█████▉    | 59/100 [00:06<00:03, 10.49it/s] 61%|██████    | 61/100 [00:06<00:03, 10.04it/s] 63%|██████▎   | 63/100 [00:06<00:03, 10.55it/s] 65%|██████▌   | 65/100 [00:07<00:03, 11.00it/s] 67%|██████▋   | 67/100 [00:07<00:03, 10.87it/s] 69%|██████▉   | 69/100 [00:07<00:02, 11.27it/s] 71%|███████   | 71/100 [00:07<00:02, 12.09it/s] 73%|███████▎  | 73/100 [00:07<00:02, 12.66it/s] 75%|███████▌  | 75/100 [00:07<00:01, 13.57it/s] 77%|███████▋  | 77/100 [00:07<00:01, 14.36it/s] 79%|███████▉  | 79/100 [00:08<00:01, 15.04it/s] 81%|████████  | 81/100 [00:08<00:01, 15.82it/s] 83%|████████▎ | 83/100 [00:08<00:01, 15.68it/s] 85%|████████▌ | 85/100 [00:08<00:00, 15.93it/s] 87%|████████▋ | 87/100 [00:08<00:00, 16.03it/s] 89%|████████▉ | 89/100 [00:08<00:00, 16.81it/s] 91%|█████████ | 91/100 [00:08<00:00, 16.72it/s] 93%|█████████▎| 93/100 [00:08<00:00, 16.95it/s] 95%|█████████▌| 95/100 [00:09<00:00, 16.67it/s] 97%|█████████▋| 97/100 [00:09<00:00, 15.76it/s] 99%|█████████▉| 99/100 [00:09<00:00, 15.41it/s]100%|██████████| 100/100 [00:09<00:00, 10.65it/s]
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 0.0028, 0.0028
--- total mse / var(X): 0.0028
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:11,  8.53it/s]  3%|▎         | 3/100 [00:00<00:07, 13.07it/s]  5%|▌         | 5/100 [00:00<00:06, 14.62it/s]  7%|▋         | 7/100 [00:00<00:06, 14.40it/s]  9%|▉         | 9/100 [00:00<00:06, 14.44it/s] 11%|█         | 11/100 [00:00<00:06, 13.12it/s] 13%|█▎        | 13/100 [00:01<00:07, 12.21it/s] 15%|█▌        | 15/100 [00:01<00:06, 13.65it/s] 17%|█▋        | 17/100 [00:01<00:05, 13.93it/s] 19%|█▉        | 19/100 [00:01<00:05, 13.99it/s] 21%|██        | 21/100 [00:01<00:05, 14.78it/s] 23%|██▎       | 23/100 [00:01<00:05, 14.96it/s] 25%|██▌       | 25/100 [00:01<00:04, 15.37it/s] 27%|██▋       | 27/100 [00:01<00:04, 15.80it/s] 29%|██▉       | 29/100 [00:02<00:04, 15.76it/s] 32%|███▏      | 32/100 [00:02<00:03, 17.94it/s] 34%|███▍      | 34/100 [00:02<00:03, 17.09it/s] 36%|███▌      | 36/100 [00:02<00:03, 16.68it/s] 38%|███▊      | 38/100 [00:02<00:03, 16.41it/s] 40%|████      | 40/100 [00:02<00:03, 16.94it/s] 42%|████▏     | 42/100 [00:02<00:03, 16.85it/s] 44%|████▍     | 44/100 [00:02<00:03, 17.44it/s] 46%|████▌     | 46/100 [00:02<00:03, 16.90it/s] 48%|████▊     | 48/100 [00:03<00:03, 15.66it/s] 50%|█████     | 50/100 [00:03<00:03, 15.73it/s] 52%|█████▏    | 52/100 [00:03<00:03, 15.85it/s] 55%|█████▌    | 55/100 [00:03<00:02, 17.30it/s] 58%|█████▊    | 58/100 [00:03<00:02, 17.42it/s] 60%|██████    | 60/100 [00:03<00:02, 17.29it/s] 62%|██████▏   | 62/100 [00:03<00:02, 17.74it/s] 64%|██████▍   | 64/100 [00:04<00:02, 16.68it/s] 66%|██████▌   | 66/100 [00:04<00:01, 17.23it/s] 68%|██████▊   | 68/100 [00:04<00:01, 16.66it/s] 70%|███████   | 70/100 [00:04<00:01, 16.25it/s] 72%|███████▏  | 72/100 [00:04<00:01, 16.12it/s] 74%|███████▍  | 74/100 [00:04<00:01, 14.24it/s] 76%|███████▌  | 76/100 [00:04<00:01, 14.89it/s] 78%|███████▊  | 78/100 [00:04<00:01, 15.55it/s] 80%|████████  | 80/100 [00:05<00:01, 15.47it/s] 82%|████████▏ | 82/100 [00:05<00:01, 16.26it/s] 84%|████████▍ | 84/100 [00:05<00:01, 14.79it/s] 86%|████████▌ | 86/100 [00:05<00:00, 14.78it/s] 88%|████████▊ | 88/100 [00:05<00:00, 15.07it/s] 90%|█████████ | 90/100 [00:05<00:00, 15.64it/s] 92%|█████████▏| 92/100 [00:05<00:00, 15.63it/s] 94%|█████████▍| 94/100 [00:05<00:00, 16.29it/s] 96%|█████████▌| 96/100 [00:06<00:00, 16.03it/s] 98%|█████████▊| 98/100 [00:06<00:00, 14.96it/s]100%|██████████| 100/100 [00:06<00:00, 15.43it/s]100%|██████████| 100/100 [00:06<00:00, 15.62it/s]
/data/neelesh/DART_by_app/437/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 64, 8
mse / {var(X_subs), var(X)}: 4.2e-10, 4.2e-10
--- total mse / var(X): 4.2e-10
start table evaluation...
Elapsed time: 63.59747576713562 seconds
Cosine similarity between AMM and exact (Train): 0.9911881
Cosine similarity between AMM and exact (Test): 0.99077725
p,r,f1: 0.4723210412730132 0.6628988998271614 0.5516131056007779
p,r,f1: 0.47583843174270635 0.6233156725152812 0.5396832908894929
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.4723210412730132, 0.6628988998271614, 0.5516131056007779],
           'num_param': 19712},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
               'K_CLUSTER': [64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64,
                             64],
               'cossim_layer_train': [0.9955264925956726,
                                      0.9917498826980591,
                                      0.9901260137557983,
                                      0.991188108921051],
               'cossim_layer_test': [0.9907267093658447,
                                     0.9913447499275208,
                                     0.9898654818534851,
                                     0.9907772541046143],
               'cossim_amm_train': [0.9922031760215759,
                                    0.996056079864502,
                                    0.9951271414756775,
                                    0.9682630896568298,
                                    0.9802771210670471,
                                    0.9847339391708374,
                                    0.989805281162262,
                                    0.9972214102745056],
               'cossim_amm_test': [0.9835509657859802,
                                   0.994701087474823,
                                   0.9936046600341797,
                                   0.9669126868247986,
                                   0.9805551171302795,
                                   0.9827044606208801,
                                   0.9894260168075562,
                                   0.9969425201416016],
               'f1': [0.47583843174270635,
                      0.6233156725152812,
                      0.5396832908894929],
               'lut_num': 8,
               'lut_shapes': [(32, 1, 64),
                              (192, 1, 64),
                              (64, 64, 1),
                              (64, 64, 1),
                              (32, 1, 64),
                              (32, 1, 64),
                              (32, 1, 64),
                              (256, 1, 64)],
               'lut_total_size': 45056}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        1,376
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 6,128
Trainable params: 6,128
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        1,376
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 6,128
Trainable params: 6,128
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5494589365 - test_loss: 0.6932253843
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5115155777 - test_loss: 0.6420348585
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4729940918 - test_loss: 0.5889463488
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4341168755 - test_loss: 0.5357199821
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3981240508 - test_loss: 0.4884180057
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3685690946 - test_loss: 0.4500925558
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3451388754 - test_loss: 0.4185858375
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3258658283 - test_loss: 0.3915783989
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3094856907 - test_loss: 0.3678437640
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2953688065 - test_loss: 0.3467543509
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2831538464 - test_loss: 0.3279385790
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2726004273 - test_loss: 0.3111663429
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2635210284 - test_loss: 0.2962462326
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2557665214 - test_loss: 0.2830081860
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2491850798 - test_loss: 0.2713404325
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2436477614 - test_loss: 0.2611077806
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2390434974 - test_loss: 0.2521704997
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2352446135 - test_loss: 0.2444467324
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2321513523 - test_loss: 0.2378105659
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2296594621 - test_loss: 0.2321783917
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2276764725 - test_loss: 0.2274372449
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2261148211 - test_loss: 0.2234469486
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2249002462 - test_loss: 0.2202034977
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2239749841 - test_loss: 0.2175537719
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2232644970 - test_loss: 0.2153594207
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2227329986 - test_loss: 0.2137215511
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2223404789 - test_loss: 0.2123946232
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2220473421 - test_loss: 0.2113120552
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2218344864 - test_loss: 0.2105271335
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2216840277 - test_loss: 0.2099061641
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2215712501 - test_loss: 0.2094876607
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2214947262 - test_loss: 0.2091608853
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2214461502 - test_loss: 0.2089870912
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2214046201 - test_loss: 0.2088330749
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2213833994 - test_loss: 0.2086454630
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2213632082 - test_loss: 0.2085139022
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2213599992 - test_loss: 0.2085046656
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2213521089 - test_loss: 0.2084204337
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2213402290 - test_loss: 0.2083803268
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2213380162 - test_loss: 0.2083092246
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2213377059 - test_loss: 0.2083295559
Early Stop Left: 4
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2213317197 - test_loss: 0.2082925895
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2213320207 - test_loss: 0.2083065991
Early Stop Left: 4
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2213260327 - test_loss: 0.2082625308
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2213204049 - test_loss: 0.2082866761
Early Stop Left: 4
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2213177117 - test_loss: 0.2082794101
Early Stop Left: 3
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2213147926 - test_loss: 0.2082922033
Early Stop Left: 2
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2213074015 - test_loss: 0.2082511063
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2213072038 - test_loss: 0.2083841217
Early Stop Left: 4
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2212992612 - test_loss: 0.2082057065
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2212959607 - test_loss: 0.2081997367
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2212942764 - test_loss: 0.2082974011
Early Stop Left: 4
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2212907043 - test_loss: 0.2081732756
-------- Save Best Model! --------
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2212830713 - test_loss: 0.2081742760
Early Stop Left: 4
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2212714219 - test_loss: 0.2081880491
Early Stop Left: 3
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2212684333 - test_loss: 0.2082325177
Early Stop Left: 2
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2212638010 - test_loss: 0.2081805889
Early Stop Left: 1
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2212563562 - test_loss: 0.2081376767
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2212460164 - test_loss: 0.2081446612
Early Stop Left: 4
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2212329041 - test_loss: 0.2081694346
Early Stop Left: 3
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2212269486 - test_loss: 0.2079959562
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2212206823 - test_loss: 0.2080788410
Early Stop Left: 4
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2212104568 - test_loss: 0.2078619628
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2211940433 - test_loss: 0.2080182370
Early Stop Left: 4
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2211847395 - test_loss: 0.2078462316
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2211682990 - test_loss: 0.2078476760
Early Stop Left: 4
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2211545909 - test_loss: 0.2078596442
Early Stop Left: 3
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2211415228 - test_loss: 0.2078547333
Early Stop Left: 2
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2211242691 - test_loss: 0.2078242167
-------- Save Best Model! --------
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2211064829 - test_loss: 0.2076066502
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2210895925 - test_loss: 0.2077015701
Early Stop Left: 4
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2210754828 - test_loss: 0.2077398959
Early Stop Left: 3
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2210477020 - test_loss: 0.2076668725
Early Stop Left: 2
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2210283381 - test_loss: 0.2076187909
Early Stop Left: 1
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2210117223 - test_loss: 0.2075141362
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2209869201 - test_loss: 0.2075398275
Early Stop Left: 4
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2209660000 - test_loss: 0.2074942701
-------- Save Best Model! --------
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2209397041 - test_loss: 0.2074090720
-------- Save Best Model! --------
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2209147456 - test_loss: 0.2074086010
-------- Save Best Model! --------
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2208895404 - test_loss: 0.2072703515
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2208683055 - test_loss: 0.2073397579
Early Stop Left: 4
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2208416044 - test_loss: 0.2072299616
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.2208180996 - test_loss: 0.2071679977
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.2207871204 - test_loss: 0.2072749823
Early Stop Left: 4
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.2207613918 - test_loss: 0.2072129252
Early Stop Left: 3
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.2207327369 - test_loss: 0.2071907758
Early Stop Left: 2
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.2207115146 - test_loss: 0.2071497326
-------- Save Best Model! --------
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.2206837042 - test_loss: 0.2070702254
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.2206569457 - test_loss: 0.2070980781
Early Stop Left: 4
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.2206258168 - test_loss: 0.2071010056
Early Stop Left: 3
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.2205928642 - test_loss: 0.2069821965
-------- Save Best Model! --------
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.2205739135 - test_loss: 0.2070291611
Early Stop Left: 4
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.2205425543 - test_loss: 0.2069405290
-------- Save Best Model! --------
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.2205177511 - test_loss: 0.2069210978
-------- Save Best Model! --------
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.2204909774 - test_loss: 0.2068049207
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.2204590574 - test_loss: 0.2069406098
Early Stop Left: 4
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.2204387131 - test_loss: 0.2069002943
Early Stop Left: 3
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.2204130554 - test_loss: 0.2068287777
Early Stop Left: 2
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.2203761973 - test_loss: 0.2067684233
-------- Save Best Model! --------
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.2203506164 - test_loss: 0.2067148565
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.2203220676 - test_loss: 0.2066843723
-------- Save Best Model! --------
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.2202966739 - test_loss: 0.2068733802
Early Stop Left: 4
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.2202610596 - test_loss: 0.2066493322
-------- Save Best Model! --------
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.2202284432 - test_loss: 0.2066719383
Early Stop Left: 4
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.2201982928 - test_loss: 0.2066675485
Early Stop Left: 3
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.2201689583 - test_loss: 0.2065901809
-------- Save Best Model! --------
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.2201237912 - test_loss: 0.2065402544
-------- Save Best Model! --------
------- START EPOCH 108 -------
Epoch: 108 - loss: 0.2201065864 - test_loss: 0.2066578885
Early Stop Left: 4
------- START EPOCH 109 -------
Epoch: 109 - loss: 0.2200679501 - test_loss: 0.2066047011
Early Stop Left: 3
------- START EPOCH 110 -------
Epoch: 110 - loss: 0.2200341686 - test_loss: 0.2065712722
Early Stop Left: 2
------- START EPOCH 111 -------
Epoch: 111 - loss: 0.2199905414 - test_loss: 0.2065157529
-------- Save Best Model! --------
------- START EPOCH 112 -------
Epoch: 112 - loss: 0.2199564906 - test_loss: 0.2065902668
Early Stop Left: 4
------- START EPOCH 113 -------
Epoch: 113 - loss: 0.2199174409 - test_loss: 0.2064357364
-------- Save Best Model! --------
------- START EPOCH 114 -------
Epoch: 114 - loss: 0.2198731154 - test_loss: 0.2064557866
Early Stop Left: 4
------- START EPOCH 115 -------
Epoch: 115 - loss: 0.2198329227 - test_loss: 0.2064253027
-------- Save Best Model! --------
------- START EPOCH 116 -------
Epoch: 116 - loss: 0.2197840148 - test_loss: 0.2064239960
-------- Save Best Model! --------
------- START EPOCH 117 -------
Epoch: 117 - loss: 0.2197459107 - test_loss: 0.2064214790
-------- Save Best Model! --------
------- START EPOCH 118 -------
Epoch: 118 - loss: 0.2196969238 - test_loss: 0.2064585713
Early Stop Left: 4
------- START EPOCH 119 -------
Epoch: 119 - loss: 0.2196471836 - test_loss: 0.2063437457
-------- Save Best Model! --------
------- START EPOCH 120 -------
Epoch: 120 - loss: 0.2195934427 - test_loss: 0.2063846091
Early Stop Left: 4
------- START EPOCH 121 -------
Epoch: 121 - loss: 0.2195500948 - test_loss: 0.2061960370
-------- Save Best Model! --------
------- START EPOCH 122 -------
Epoch: 122 - loss: 0.2195002075 - test_loss: 0.2062914985
Early Stop Left: 4
------- START EPOCH 123 -------
Epoch: 123 - loss: 0.2194572098 - test_loss: 0.2061815633
-------- Save Best Model! --------
------- START EPOCH 124 -------
Epoch: 124 - loss: 0.2194069512 - test_loss: 0.2061584953
-------- Save Best Model! --------
------- START EPOCH 125 -------
Epoch: 125 - loss: 0.2193644702 - test_loss: 0.2061209451
-------- Save Best Model! --------
------- START EPOCH 126 -------
Epoch: 126 - loss: 0.2193233077 - test_loss: 0.2060514128
-------- Save Best Model! --------
------- START EPOCH 127 -------
Epoch: 127 - loss: 0.2192791173 - test_loss: 0.2060912805
Early Stop Left: 4
------- START EPOCH 128 -------
Epoch: 128 - loss: 0.2192407345 - test_loss: 0.2061141887
Early Stop Left: 3
------- START EPOCH 129 -------
Epoch: 129 - loss: 0.2192018092 - test_loss: 0.2059340054
-------- Save Best Model! --------
------- START EPOCH 130 -------
Epoch: 130 - loss: 0.2191643673 - test_loss: 0.2058447902
-------- Save Best Model! --------
------- START EPOCH 131 -------
Epoch: 131 - loss: 0.2191267902 - test_loss: 0.2057746640
-------- Save Best Model! --------
------- START EPOCH 132 -------
Epoch: 132 - loss: 0.2190971221 - test_loss: 0.2058013976
Early Stop Left: 4
------- START EPOCH 133 -------
Epoch: 133 - loss: 0.2190631518 - test_loss: 0.2057297723
-------- Save Best Model! --------
------- START EPOCH 134 -------
Epoch: 134 - loss: 0.2190363983 - test_loss: 0.2057188393
-------- Save Best Model! --------
------- START EPOCH 135 -------
Epoch: 135 - loss: 0.2190069950 - test_loss: 0.2057027000
-------- Save Best Model! --------
------- START EPOCH 136 -------
Epoch: 136 - loss: 0.2189794611 - test_loss: 0.2058316624
Early Stop Left: 4
------- START EPOCH 137 -------
Epoch: 137 - loss: 0.2189473433 - test_loss: 0.2056874637
-------- Save Best Model! --------
------- START EPOCH 138 -------
Epoch: 138 - loss: 0.2189202940 - test_loss: 0.2057336651
Early Stop Left: 4
------- START EPOCH 139 -------
Epoch: 139 - loss: 0.2188966959 - test_loss: 0.2054453610
-------- Save Best Model! --------
------- START EPOCH 140 -------
Epoch: 140 - loss: 0.2188729626 - test_loss: 0.2055815014
Early Stop Left: 4
------- START EPOCH 141 -------
Epoch: 141 - loss: 0.2188510249 - test_loss: 0.2054647061
Early Stop Left: 3
------- START EPOCH 142 -------
Epoch: 142 - loss: 0.2188291295 - test_loss: 0.2054423883
-------- Save Best Model! --------
------- START EPOCH 143 -------
Epoch: 143 - loss: 0.2188066345 - test_loss: 0.2052658195
-------- Save Best Model! --------
------- START EPOCH 144 -------
Epoch: 144 - loss: 0.2187854328 - test_loss: 0.2053353680
Early Stop Left: 4
------- START EPOCH 145 -------
Epoch: 145 - loss: 0.2187735962 - test_loss: 0.2054102524
Early Stop Left: 3
------- START EPOCH 146 -------
Epoch: 146 - loss: 0.2187548152 - test_loss: 0.2051801720
-------- Save Best Model! --------
------- START EPOCH 147 -------
Epoch: 147 - loss: 0.2187381917 - test_loss: 0.2054348833
Early Stop Left: 4
------- START EPOCH 148 -------
Epoch: 148 - loss: 0.2187194262 - test_loss: 0.2052862627
Early Stop Left: 3
------- START EPOCH 149 -------
Epoch: 149 - loss: 0.2187002635 - test_loss: 0.2052836322
Early Stop Left: 2
------- START EPOCH 150 -------
Epoch: 150 - loss: 0.2186847420 - test_loss: 0.2053670266
Early Stop Left: 1
------- START EPOCH 151 -------
Epoch: 151 - loss: 0.2186762021 - test_loss: 0.2051866489
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 20%|██        | 21/104 [00:00<00:00, 200.22it/s] 40%|████      | 42/104 [00:00<00:00, 197.01it/s] 60%|█████▉    | 62/104 [00:00<00:00, 188.73it/s] 79%|███████▉  | 82/104 [00:00<00:00, 191.05it/s] 98%|█████████▊| 102/104 [00:00<00:00, 193.20it/s]100%|██████████| 104/104 [00:00<00:00, 194.54it/s]
Best micro threshold=0.317660, fscore=0.556
p,r,f1: 0.47984307118819286 0.6613910391610264 0.5561766943511064
throttleing by fixed threshold: 0.5
p,r,f1: 0.6501412496358673 0.36176593101914417 0.4648627047319185
{'model': 'vit_min',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.31766006350517273,
                 'p': 0.47984307118819286,
                 'r': 0.6613910391610264,
                 'f1': 0.5561766943511064},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6501412496358673,
                 'r': 0.36176593101914417,
                 'f1': 0.4648627047319185}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0000001
Manual and Torch results cosine similarity (Test): 0.9999999
start table training with fine tuning...
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
Retrain for 1 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.019, 0.0282
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.000233, 0.000121
--- total mse / var(X): 0.0141
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<03:07,  1.90s/it]  2%|▏         | 2/100 [00:04<03:37,  2.21s/it]  3%|▎         | 3/100 [00:06<03:17,  2.04s/it]  4%|▍         | 4/100 [00:15<07:36,  4.75s/it]  5%|▌         | 5/100 [00:17<06:04,  3.83s/it]  6%|▌         | 6/100 [00:19<05:09,  3.29s/it]  7%|▋         | 7/100 [00:21<04:31,  2.92s/it]  8%|▊         | 8/100 [00:23<04:02,  2.63s/it]  9%|▉         | 9/100 [00:25<03:37,  2.39s/it] 10%|█         | 10/100 [00:27<03:09,  2.11s/it] 11%|█         | 11/100 [00:28<02:58,  2.00s/it] 12%|█▏        | 12/100 [00:30<02:50,  1.94s/it] 13%|█▎        | 13/100 [00:32<02:44,  1.89s/it] 14%|█▍        | 14/100 [00:34<02:42,  1.89s/it] 15%|█▌        | 15/100 [00:36<02:37,  1.85s/it] 16%|█▌        | 16/100 [00:37<02:36,  1.87s/it] 17%|█▋        | 17/100 [00:39<02:36,  1.89s/it] 18%|█▊        | 18/100 [00:41<02:34,  1.88s/it] 19%|█▉        | 19/100 [00:43<02:34,  1.91s/it] 20%|██        | 20/100 [00:45<02:32,  1.91s/it] 21%|██        | 21/100 [00:47<02:29,  1.89s/it] 22%|██▏       | 22/100 [00:49<02:28,  1.90s/it] 23%|██▎       | 23/100 [00:51<02:24,  1.87s/it] 24%|██▍       | 24/100 [00:53<02:22,  1.87s/it] 25%|██▌       | 25/100 [00:54<02:21,  1.88s/it] 26%|██▌       | 26/100 [00:56<02:20,  1.90s/it] 27%|██▋       | 27/100 [00:58<02:19,  1.91s/it] 28%|██▊       | 28/100 [01:00<02:18,  1.93s/it] 29%|██▉       | 29/100 [01:02<02:18,  1.95s/it] 30%|███       | 30/100 [01:05<02:31,  2.17s/it] 31%|███       | 31/100 [01:08<02:43,  2.37s/it] 32%|███▏      | 32/100 [01:10<02:44,  2.42s/it] 33%|███▎      | 33/100 [01:13<02:42,  2.43s/it] 34%|███▍      | 34/100 [01:15<02:41,  2.45s/it] 35%|███▌      | 35/100 [01:18<02:38,  2.44s/it] 36%|███▌      | 36/100 [01:20<02:39,  2.49s/it] 37%|███▋      | 37/100 [01:23<02:38,  2.52s/it] 38%|███▊      | 38/100 [01:26<02:37,  2.54s/it] 39%|███▉      | 39/100 [01:28<02:34,  2.54s/it] 40%|████      | 40/100 [01:31<02:34,  2.57s/it] 41%|████      | 41/100 [01:33<02:31,  2.56s/it] 42%|████▏     | 42/100 [01:36<02:24,  2.49s/it] 43%|████▎     | 43/100 [01:38<02:22,  2.51s/it] 44%|████▍     | 44/100 [01:41<02:21,  2.53s/it] 45%|████▌     | 45/100 [01:43<02:20,  2.56s/it] 46%|████▌     | 46/100 [01:46<02:17,  2.55s/it] 47%|████▋     | 47/100 [01:48<02:15,  2.56s/it] 48%|████▊     | 48/100 [01:51<02:13,  2.56s/it] 49%|████▉     | 49/100 [01:54<02:22,  2.79s/it] 50%|█████     | 50/100 [01:58<02:32,  3.05s/it] 51%|█████     | 51/100 [02:01<02:35,  3.18s/it] 52%|█████▏    | 52/100 [02:05<02:38,  3.30s/it] 53%|█████▎    | 53/100 [02:09<02:39,  3.40s/it] 54%|█████▍    | 54/100 [02:13<02:42,  3.53s/it] 55%|█████▌    | 55/100 [02:16<02:40,  3.57s/it] 56%|█████▌    | 56/100 [02:20<02:38,  3.60s/it] 57%|█████▋    | 57/100 [02:24<02:36,  3.64s/it] 58%|█████▊    | 58/100 [02:27<02:33,  3.65s/it] 59%|█████▉    | 59/100 [02:31<02:30,  3.67s/it] 60%|██████    | 60/100 [02:34<02:24,  3.61s/it] 61%|██████    | 61/100 [02:38<02:19,  3.58s/it] 62%|██████▏   | 62/100 [02:42<02:17,  3.62s/it] 63%|██████▎   | 63/100 [02:46<02:17,  3.71s/it] 64%|██████▍   | 64/100 [02:49<02:15,  3.76s/it] 65%|██████▌   | 65/100 [02:53<02:14,  3.83s/it] 66%|██████▌   | 66/100 [02:57<02:10,  3.84s/it] 67%|██████▋   | 67/100 [03:01<02:08,  3.89s/it] 68%|██████▊   | 68/100 [03:05<02:05,  3.92s/it] 69%|██████▉   | 69/100 [03:09<01:59,  3.87s/it] 70%|███████   | 70/100 [03:13<01:55,  3.86s/it] 71%|███████   | 71/100 [03:17<01:51,  3.84s/it] 72%|███████▏  | 72/100 [03:20<01:47,  3.82s/it] 73%|███████▎  | 73/100 [03:24<01:44,  3.86s/it] 74%|███████▍  | 74/100 [03:28<01:40,  3.88s/it] 75%|███████▌  | 75/100 [03:32<01:37,  3.91s/it] 76%|███████▌  | 76/100 [03:36<01:33,  3.88s/it] 77%|███████▋  | 77/100 [03:40<01:29,  3.87s/it] 78%|███████▊  | 78/100 [03:44<01:27,  4.00s/it] 79%|███████▉  | 79/100 [03:48<01:23,  3.99s/it] 80%|████████  | 80/100 [03:52<01:19,  3.98s/it] 81%|████████  | 81/100 [03:56<01:16,  4.04s/it] 82%|████████▏ | 82/100 [04:00<01:11,  3.99s/it] 83%|████████▎ | 83/100 [04:04<01:08,  4.02s/it] 84%|████████▍ | 84/100 [04:08<01:04,  4.02s/it] 85%|████████▌ | 85/100 [04:12<01:00,  4.01s/it] 86%|████████▌ | 86/100 [04:16<00:56,  4.05s/it] 87%|████████▋ | 87/100 [04:21<00:53,  4.15s/it] 88%|████████▊ | 88/100 [04:25<00:49,  4.14s/it] 89%|████████▉ | 89/100 [04:29<00:46,  4.23s/it] 90%|█████████ | 90/100 [04:33<00:41,  4.11s/it] 91%|█████████ | 91/100 [04:37<00:35,  3.99s/it] 92%|█████████▏| 92/100 [04:41<00:31,  3.88s/it] 93%|█████████▎| 93/100 [04:44<00:26,  3.80s/it] 94%|█████████▍| 94/100 [04:48<00:22,  3.78s/it] 95%|█████████▌| 95/100 [04:51<00:18,  3.69s/it] 96%|█████████▌| 96/100 [04:55<00:14,  3.75s/it] 97%|█████████▋| 97/100 [04:59<00:11,  3.80s/it] 98%|█████████▊| 98/100 [05:03<00:07,  3.78s/it] 99%|█████████▉| 99/100 [05:06<00:03,  3.69s/it]100%|██████████| 100/100 [05:10<00:00,  3.70s/it]100%|██████████| 100/100 [05:10<00:00,  3.11s/it]===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        2,672
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 7,424
Trainable params: 7,424
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        2,672
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 7,424
Trainable params: 7,424
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5514068846 - test_loss: 0.6960440994
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.5128875817 - test_loss: 0.6440843619
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.4730917569 - test_loss: 0.5871908435
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.4304902815 - test_loss: 0.5283627332
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.3917318937 - test_loss: 0.4794894294
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.3617804142 - test_loss: 0.4417147502
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.3385870993 - test_loss: 0.4106716216
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.3195512693 - test_loss: 0.3839059461
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.3034164225 - test_loss: 0.3603585870
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.2895985080 - test_loss: 0.3395000527
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.2777394333 - test_loss: 0.3209710431
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.2675861914 - test_loss: 0.3045472795
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.2589500956 - test_loss: 0.2900226970
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.2516602473 - test_loss: 0.2772206403
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.2455573507 - test_loss: 0.2659949507
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.2404950117 - test_loss: 0.2562148153
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.2363384887 - test_loss: 0.2477364867
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.2329697401 - test_loss: 0.2404681271
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.2302663500 - test_loss: 0.2342688702
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.2281313605 - test_loss: 0.2290664764
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.2264558489 - test_loss: 0.2247255735
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.2251612058 - test_loss: 0.2211237613
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.2241723265 - test_loss: 0.2182177166
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.2234229483 - test_loss: 0.2158541866
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.2228648811 - test_loss: 0.2140526673
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.2224514950 - test_loss: 0.2126291002
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.2221345714 - test_loss: 0.2114683715
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.2219227879 - test_loss: 0.2106750746
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.2217535383 - test_loss: 0.2100275438
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.2216287968 - test_loss: 0.2096542789
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.2215389013 - test_loss: 0.2091864766
-------- Save Best Model! --------
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.2214670731 - test_loss: 0.2090076804
-------- Save Best Model! --------
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.2214165909 - test_loss: 0.2088510115
-------- Save Best Model! --------
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.2213760970 - test_loss: 0.2086692225
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.2213486207 - test_loss: 0.2085623221
-------- Save Best Model! --------
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.2213206351 - test_loss: 0.2084975393
-------- Save Best Model! --------
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.2213003392 - test_loss: 0.2084874978
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.2212848429 - test_loss: 0.2083963518
-------- Save Best Model! --------
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.2212635783 - test_loss: 0.2083950802
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.2212493693 - test_loss: 0.2082282043
-------- Save Best Model! --------
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.2212289552 - test_loss: 0.2082056561
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.2212119716 - test_loss: 0.2081929881
-------- Save Best Model! --------
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.2211893716 - test_loss: 0.2082381830
Early Stop Left: 4
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.2211718537 - test_loss: 0.2080851674
-------- Save Best Model! --------
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.2211524554 - test_loss: 0.2081231994
Early Stop Left: 4
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.2211216107 - test_loss: 0.2080370805
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.2210993021 - test_loss: 0.2079515586
-------- Save Best Model! --------
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.2210701692 - test_loss: 0.2078484701
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.2210361871 - test_loss: 0.2079460945
Early Stop Left: 4
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.2210106734 - test_loss: 0.2078151293
-------- Save Best Model! --------
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.2209687293 - test_loss: 0.2077873378
-------- Save Best Model! --------
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.2209349905 - test_loss: 0.2076843309
-------- Save Best Model! --------
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.2208902398 - test_loss: 0.2077049604
Early Stop Left: 4
------- START EPOCH 54 -------
Epoch: 54 - loss: 0.2208563927 - test_loss: 0.2075731582
-------- Save Best Model! --------
------- START EPOCH 55 -------
Epoch: 55 - loss: 0.2208183589 - test_loss: 0.2075502944
-------- Save Best Model! --------
------- START EPOCH 56 -------
Epoch: 56 - loss: 0.2207777542 - test_loss: 0.2073398237
-------- Save Best Model! --------
------- START EPOCH 57 -------
Epoch: 57 - loss: 0.2207422292 - test_loss: 0.2073157971
-------- Save Best Model! --------
------- START EPOCH 58 -------
Epoch: 58 - loss: 0.2206956813 - test_loss: 0.2072420478
-------- Save Best Model! --------
------- START EPOCH 59 -------
Epoch: 59 - loss: 0.2206682467 - test_loss: 0.2072220826
-------- Save Best Model! --------
------- START EPOCH 60 -------
Epoch: 60 - loss: 0.2206285441 - test_loss: 0.2071482605
-------- Save Best Model! --------
------- START EPOCH 61 -------
Epoch: 61 - loss: 0.2205973621 - test_loss: 0.2070777759
-------- Save Best Model! --------
------- START EPOCH 62 -------
Epoch: 62 - loss: 0.2205623991 - test_loss: 0.2070233211
-------- Save Best Model! --------
------- START EPOCH 63 -------
Epoch: 63 - loss: 0.2205280768 - test_loss: 0.2070100542
-------- Save Best Model! --------
------- START EPOCH 64 -------
Epoch: 64 - loss: 0.2204907210 - test_loss: 0.2070473573
Early Stop Left: 4
------- START EPOCH 65 -------
Epoch: 65 - loss: 0.2204577789 - test_loss: 0.2069850881
-------- Save Best Model! --------
------- START EPOCH 66 -------
Epoch: 66 - loss: 0.2204248714 - test_loss: 0.2068749476
-------- Save Best Model! --------
------- START EPOCH 67 -------
Epoch: 67 - loss: 0.2203848459 - test_loss: 0.2067282320
-------- Save Best Model! --------
------- START EPOCH 68 -------
Epoch: 68 - loss: 0.2203598761 - test_loss: 0.2068118268
Early Stop Left: 4
------- START EPOCH 69 -------
Epoch: 69 - loss: 0.2203217232 - test_loss: 0.2068443369
Early Stop Left: 3
------- START EPOCH 70 -------
Epoch: 70 - loss: 0.2202808620 - test_loss: 0.2066540777
-------- Save Best Model! --------
------- START EPOCH 71 -------
Epoch: 71 - loss: 0.2202428279 - test_loss: 0.2066281653
-------- Save Best Model! --------
------- START EPOCH 72 -------
Epoch: 72 - loss: 0.2202015826 - test_loss: 0.2066382900
Early Stop Left: 4
------- START EPOCH 73 -------
Epoch: 73 - loss: 0.2201554394 - test_loss: 0.2065504798
-------- Save Best Model! --------
------- START EPOCH 74 -------
Epoch: 74 - loss: 0.2201083964 - test_loss: 0.2066076778
Early Stop Left: 4
------- START EPOCH 75 -------
Epoch: 75 - loss: 0.2200559370 - test_loss: 0.2065079845
-------- Save Best Model! --------
------- START EPOCH 76 -------
Epoch: 76 - loss: 0.2199914896 - test_loss: 0.2065863477
Early Stop Left: 4
------- START EPOCH 77 -------
Epoch: 77 - loss: 0.2199232977 - test_loss: 0.2065968624
Early Stop Left: 3
------- START EPOCH 78 -------
Epoch: 78 - loss: 0.2198430965 - test_loss: 0.2065417185
Early Stop Left: 2
------- START EPOCH 79 -------
Epoch: 79 - loss: 0.2197690377 - test_loss: 0.2066037148
Early Stop Left: 1
------- START EPOCH 80 -------
Epoch: 80 - loss: 0.2196773773 - test_loss: 0.2064306879
-------- Save Best Model! --------
------- START EPOCH 81 -------
Epoch: 81 - loss: 0.2195966674 - test_loss: 0.2063636786
-------- Save Best Model! --------
------- START EPOCH 82 -------
Epoch: 82 - loss: 0.2195194470 - test_loss: 0.2063553520
-------- Save Best Model! --------
------- START EPOCH 83 -------
Epoch: 83 - loss: 0.2194483837 - test_loss: 0.2062371034
-------- Save Best Model! --------
------- START EPOCH 84 -------
Epoch: 84 - loss: 0.2193865046 - test_loss: 0.2060827101
-------- Save Best Model! --------
------- START EPOCH 85 -------
Epoch: 85 - loss: 0.2193241525 - test_loss: 0.2061879681
Early Stop Left: 4
------- START EPOCH 86 -------
Epoch: 86 - loss: 0.2192695224 - test_loss: 0.2060852926
Early Stop Left: 3
------- START EPOCH 87 -------
Epoch: 87 - loss: 0.2192236626 - test_loss: 0.2061520583
Early Stop Left: 2
------- START EPOCH 88 -------
Epoch: 88 - loss: 0.2191763148 - test_loss: 0.2058223132
-------- Save Best Model! --------
------- START EPOCH 89 -------
Epoch: 89 - loss: 0.2191363094 - test_loss: 0.2056604723
-------- Save Best Model! --------
------- START EPOCH 90 -------
Epoch: 90 - loss: 0.2190936487 - test_loss: 0.2057195847
Early Stop Left: 4
------- START EPOCH 91 -------
Epoch: 91 - loss: 0.2190556423 - test_loss: 0.2057679850
Early Stop Left: 3
------- START EPOCH 92 -------
Epoch: 92 - loss: 0.2190200368 - test_loss: 0.2054301420
-------- Save Best Model! --------
------- START EPOCH 93 -------
Epoch: 93 - loss: 0.2189832679 - test_loss: 0.2055455829
Early Stop Left: 4
------- START EPOCH 94 -------
Epoch: 94 - loss: 0.2189574596 - test_loss: 0.2054770559
Early Stop Left: 3
------- START EPOCH 95 -------
Epoch: 95 - loss: 0.2189209530 - test_loss: 0.2053644100
-------- Save Best Model! --------
------- START EPOCH 96 -------
Epoch: 96 - loss: 0.2188926547 - test_loss: 0.2053189354
-------- Save Best Model! --------
------- START EPOCH 97 -------
Epoch: 97 - loss: 0.2188705241 - test_loss: 0.2052919816
-------- Save Best Model! --------
------- START EPOCH 98 -------
Epoch: 98 - loss: 0.2188494941 - test_loss: 0.2054943974
Early Stop Left: 4
------- START EPOCH 99 -------
Epoch: 99 - loss: 0.2188175800 - test_loss: 0.2057169700
Early Stop Left: 3
------- START EPOCH 100 -------
Epoch: 100 - loss: 0.2187951641 - test_loss: 0.2052885337
-------- Save Best Model! --------
------- START EPOCH 101 -------
Epoch: 101 - loss: 0.2187759515 - test_loss: 0.2053020919
Early Stop Left: 4
------- START EPOCH 102 -------
Epoch: 102 - loss: 0.2187569759 - test_loss: 0.2051219667
-------- Save Best Model! --------
------- START EPOCH 103 -------
Epoch: 103 - loss: 0.2187313419 - test_loss: 0.2054627893
Early Stop Left: 4
------- START EPOCH 104 -------
Epoch: 104 - loss: 0.2187100797 - test_loss: 0.2054302645
Early Stop Left: 3
------- START EPOCH 105 -------
Epoch: 105 - loss: 0.2186893781 - test_loss: 0.2054103513
Early Stop Left: 2
------- START EPOCH 106 -------
Epoch: 106 - loss: 0.2186736523 - test_loss: 0.2054532551
Early Stop Left: 1
------- START EPOCH 107 -------
Epoch: 107 - loss: 0.2186570576 - test_loss: 0.2057033241
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 11%|█         | 11/104 [00:00<00:00, 109.32it/s] 21%|██        | 22/104 [00:00<00:00, 108.88it/s] 32%|███▏      | 33/104 [00:00<00:00, 107.89it/s] 42%|████▏     | 44/104 [00:00<00:00, 108.64it/s] 53%|█████▎    | 55/104 [00:00<00:00, 108.97it/s] 63%|██████▎   | 66/104 [00:00<00:00, 109.23it/s] 74%|███████▍  | 77/104 [00:00<00:00, 109.36it/s] 85%|████████▍ | 88/104 [00:00<00:00, 109.50it/s] 95%|█████████▌| 99/104 [00:00<00:00, 109.50it/s]100%|██████████| 104/104 [00:00<00:00, 109.99it/s]
Best micro threshold=0.321033, fscore=0.555
p,r,f1: 0.48106824071408005 0.6550071340657766 0.5547222246609367
throttleing by fixed threshold: 0.5
p,r,f1: 0.6515562082850681 0.3562188885935995 0.46061195417594325
{'model': 'vit_min',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3210330009460449,
                 'p': 0.48106824071408005,
                 'r': 0.6550071340657766,
                 'f1': 0.5547222246609367},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6515562082850681,
                 'r': 0.3562188885935995,
                 'f1': 0.46061195417594325}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        2,672
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 7,424
Trainable params: 7,424
Non-trainable params: 0
===========================================================================
Manual and Torch results cosine similarity (Train): 1.0
Manual and Torch results cosine similarity (Test): 0.9999999
start table training with fine tuning...
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Retrain for 1 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.22, 0.22
--- total mse / var(X): 0.22
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:58,  1.80s/it]  2%|▏         | 2/100 [00:03<03:06,  1.91s/it]  3%|▎         | 3/100 [00:05<03:11,  1.97s/it]  4%|▍         | 4/100 [00:07<03:05,  1.94s/it]  5%|▌         | 5/100 [00:09<02:59,  1.89s/it]  6%|▌         | 6/100 [00:11<02:57,  1.89s/it]  7%|▋         | 7/100 [00:13<02:56,  1.90s/it]  8%|▊         | 8/100 [00:15<03:00,  1.96s/it]  9%|▉         | 9/100 [00:17<03:07,  2.06s/it] 10%|█         | 10/100 [00:20<03:13,  2.15s/it] 11%|█         | 11/100 [00:22<03:12,  2.16s/it] 12%|█▏        | 12/100 [00:24<03:04,  2.09s/it] 13%|█▎        | 13/100 [00:26<03:04,  2.12s/it] 14%|█▍        | 14/100 [00:28<02:55,  2.04s/it] 15%|█▌        | 15/100 [00:30<02:48,  1.98s/it] 16%|█▌        | 16/100 [00:31<02:42,  1.93s/it] 17%|█▋        | 17/100 [00:33<02:42,  1.95s/it] 18%|█▊        | 18/100 [00:35<02:44,  2.01s/it] 19%|█▉        | 19/100 [00:38<02:53,  2.15s/it] 20%|██        | 20/100 [00:40<02:46,  2.08s/it] 21%|██        | 21/100 [00:42<02:49,  2.15s/it] 22%|██▏       | 22/100 [00:45<02:55,  2.25s/it] 23%|██▎       | 23/100 [00:47<02:49,  2.21s/it] 24%|██▍       | 24/100 [00:49<02:50,  2.24s/it] 25%|██▌       | 25/100 [00:51<02:38,  2.12s/it] 26%|██▌       | 26/100 [00:53<02:34,  2.09s/it] 27%|██▋       | 27/100 [00:55<02:30,  2.06s/it] 28%|██▊       | 28/100 [00:57<02:35,  2.15s/it] 29%|██▉       | 29/100 [01:00<02:45,  2.33s/it] 30%|███       | 30/100 [01:02<02:38,  2.26s/it] 31%|███       | 31/100 [01:04<02:35,  2.25s/it] 32%|███▏      | 32/100 [01:07<02:40,  2.36s/it] 33%|███▎      | 33/100 [01:09<02:35,  2.32s/it] 34%|███▍      | 34/100 [01:12<02:33,  2.33s/it] 35%|███▌      | 35/100 [01:14<02:25,  2.23s/it] 36%|███▌      | 36/100 [01:15<02:15,  2.12s/it] 37%|███▋      | 37/100 [01:17<02:10,  2.07s/it] 38%|███▊      | 38/100 [01:20<02:10,  2.11s/it] 39%|███▉      | 39/100 [01:22<02:09,  2.13s/it] 40%|████      | 40/100 [01:24<02:03,  2.06s/it] 41%|████      | 41/100 [01:26<02:01,  2.06s/it] 42%|████▏     | 42/100 [01:28<02:00,  2.08s/it] 43%|████▎     | 43/100 [01:30<02:03,  2.17s/it] 44%|████▍     | 44/100 [01:32<01:59,  2.14s/it] 45%|████▌     | 45/100 [01:34<01:53,  2.06s/it] 46%|████▌     | 46/100 [01:36<01:47,  1.99s/it] 47%|████▋     | 47/100 [01:38<01:50,  2.08s/it] 48%|████▊     | 48/100 [01:40<01:48,  2.09s/it] 49%|████▉     | 49/100 [01:42<01:43,  2.03s/it] 50%|█████     | 50/100 [01:44<01:42,  2.05s/it] 51%|█████     | 51/100 [01:47<01:43,  2.12s/it] 52%|█████▏    | 52/100 [01:49<01:40,  2.09s/it] 53%|█████▎    | 53/100 [01:51<01:39,  2.11s/it] 54%|█████▍    | 54/100 [01:53<01:33,  2.03s/it] 55%|█████▌    | 55/100 [01:55<01:28,  1.96s/it] 56%|█████▌    | 56/100 [01:57<01:31,  2.08s/it] 57%|█████▋    | 57/100 [02:00<01:37,  2.27s/it] 58%|█████▊    | 58/100 [02:02<01:36,  2.30s/it] 59%|█████▉    | 59/100 [02:04<01:33,  2.28s/it] 60%|██████    | 60/100 [02:06<01:28,  2.21s/it] 61%|██████    | 61/100 [02:08<01:25,  2.20s/it] 62%|██████▏   | 62/100 [02:10<01:19,  2.10s/it] 63%|██████▎   | 63/100 [02:12<01:16,  2.06s/it] 64%|██████▍   | 64/100 [02:14<01:11,  1.99s/it] 65%|██████▌   | 65/100 [02:16<01:11,  2.04s/it] 66%|██████▌   | 66/100 [02:18<01:08,  2.00s/it] 67%|██████▋   | 67/100 [02:20<01:04,  1.96s/it] 68%|██████▊   | 68/100 [02:22<01:01,  1.92s/it] 69%|██████▉   | 69/100 [02:24<01:00,  1.96s/it] 70%|███████   | 70/100 [02:26<00:59,  2.00s/it] 71%|███████   | 71/100 [02:28<00:58,  2.01s/it] 72%|███████▏  | 72/100 [02:30<00:55,  1.97s/it] 73%|███████▎  | 73/100 [02:32<00:54,  2.00s/it] 74%|███████▍  | 74/100 [02:35<00:56,  2.18s/it] 75%|███████▌  | 75/100 [02:37<00:59,  2.37s/it] 76%|███████▌  | 76/100 [02:39<00:53,  2.23s/it] 77%|███████▋  | 77/100 [02:41<00:50,  2.21s/it] 78%|███████▊  | 78/100 [02:44<00:50,  2.29s/it] 79%|███████▉  | 79/100 [02:47<00:50,  2.42s/it] 80%|████████  | 80/100 [02:50<00:51,  2.55s/it] 81%|████████  | 81/100 [02:52<00:49,  2.61s/it] 82%|████████▏ | 82/100 [02:55<00:47,  2.63s/it] 83%|████████▎ | 83/100 [02:57<00:40,  2.41s/it] 84%|████████▍ | 84/100 [02:59<00:38,  2.38s/it] 85%|████████▌ | 85/100 [03:02<00:36,  2.47s/it] 86%|████████▌ | 86/100 [03:04<00:32,  2.29s/it] 87%|████████▋ | 87/100 [03:06<00:30,  2.38s/it] 88%|████████▊ | 88/100 [03:08<00:27,  2.29s/it] 89%|████████▉ | 89/100 [03:11<00:24,  2.24s/it] 90%|█████████ | 90/100 [03:12<00:21,  2.16s/it] 91%|█████████ | 91/100 [03:15<00:19,  2.21s/it] 92%|█████████▏| 92/100 [03:17<00:17,  2.21s/it] 93%|█████████▎| 93/100 [03:19<00:14,  2.12s/it] 94%|█████████▍| 94/100 [03:21<00:13,  2.24s/it] 95%|█████████▌| 95/100 [03:23<00:10,  2.10s/it] 96%|█████████▌| 96/100 [03:25<00:08,  2.02s/it] 97%|█████████▋| 97/100 [03:27<00:06,  2.07s/it] 98%|█████████▊| 98/100 [03:29<00:04,  2.12s/it] 99%|█████████▉| 99/100 [03:31<00:02,  2.02s/it]100%|██████████| 100/100 [03:34<00:00,  2.11s/it]100%|██████████| 100/100 [03:34<00:00,  2.14s/it]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00296, 0.00302
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00247, 0.00242
--- total mse / var(X): 0.00272
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.002, 0.00211
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00286, 0.0027
--- total mse / var(X): 0.0024
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00251, 0.00235
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00228, 0.00243
--- total mse / var(X): 0.00239
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.000884, 0.000889
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.000896, 0.000891
--- total mse / var(X): 0.00089
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00269, 0.0026
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00489, 0.00506
--- total mse / var(X): 0.00383
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:33,  2.95it/s]  2%|▏         | 2/100 [00:00<00:31,  3.09it/s]  3%|▎         | 3/100 [00:01<00:34,  2.83it/s]  4%|▍         | 4/100 [00:01<00:40,  2.39it/s]  5%|▌         | 5/100 [00:01<00:37,  2.56it/s]  6%|▌         | 6/100 [00:02<00:36,  2.60it/s]  7%|▋         | 7/100 [00:02<00:34,  2.67it/s]  8%|▊         | 8/100 [00:02<00:32,  2.80it/s]  9%|▉         | 9/100 [00:03<00:33,  2.72it/s] 10%|█         | 10/100 [00:03<00:34,  2.63it/s] 11%|█         | 11/100 [00:04<00:35,  2.53it/s] 12%|█▏        | 12/100 [00:04<00:31,  2.77it/s] 13%|█▎        | 13/100 [00:04<00:31,  2.80it/s] 14%|█▍        | 14/100 [00:05<00:29,  2.95it/s] 15%|█▌        | 15/100 [00:05<00:29,  2.92it/s] 16%|█▌        | 16/100 [00:05<00:27,  3.08it/s] 17%|█▋        | 17/100 [00:05<00:24,  3.34it/s] 18%|█▊        | 18/100 [00:06<00:22,  3.57it/s] 19%|█▉        | 19/100 [00:06<00:23,  3.41it/s] 20%|██        | 20/100 [00:06<00:26,  3.05it/s] 21%|██        | 21/100 [00:07<00:26,  2.93it/s] 22%|██▏       | 22/100 [00:07<00:30,  2.58it/s] 23%|██▎       | 23/100 [00:08<00:29,  2.62it/s] 24%|██▍       | 24/100 [00:08<00:28,  2.66it/s] 25%|██▌       | 25/100 [00:08<00:27,  2.75it/s] 26%|██▌       | 26/100 [00:09<00:27,  2.67it/s] 27%|██▋       | 27/100 [00:09<00:29,  2.45it/s] 28%|██▊       | 28/100 [00:10<00:34,  2.08it/s] 29%|██▉       | 29/100 [00:10<00:33,  2.15it/s] 30%|███       | 30/100 [00:11<00:33,  2.08it/s] 31%|███       | 31/100 [00:11<00:32,  2.11it/s] 32%|███▏      | 32/100 [00:12<00:31,  2.19it/s] 33%|███▎      | 33/100 [00:12<00:29,  2.31it/s] 34%|███▍      | 34/100 [00:13<00:30,  2.17it/s] 35%|███▌      | 35/100 [00:13<00:29,  2.18it/s] 36%|███▌      | 36/100 [00:14<00:31,  2.06it/s] 37%|███▋      | 37/100 [00:14<00:30,  2.08it/s] 38%|███▊      | 38/100 [00:14<00:27,  2.23it/s] 39%|███▉      | 39/100 [00:15<00:26,  2.34it/s] 40%|████      | 40/100 [00:15<00:25,  2.36it/s] 41%|████      | 41/100 [00:16<00:23,  2.48it/s] 42%|████▏     | 42/100 [00:16<00:24,  2.34it/s] 43%|████▎     | 43/100 [00:17<00:24,  2.35it/s] 44%|████▍     | 44/100 [00:17<00:24,  2.28it/s] 45%|████▌     | 45/100 [00:17<00:24,  2.24it/s] 46%|████▌     | 46/100 [00:18<00:23,  2.34it/s] 47%|████▋     | 47/100 [00:18<00:22,  2.40it/s] 48%|████▊     | 48/100 [00:19<00:23,  2.22it/s] 49%|████▉     | 49/100 [00:19<00:23,  2.22it/s] 50%|█████     | 50/100 [00:20<00:22,  2.27it/s] 51%|█████     | 51/100 [00:20<00:23,  2.05it/s] 52%|█████▏    | 52/100 [00:21<00:24,  1.93it/s] 53%|█████▎    | 53/100 [00:21<00:23,  2.04it/s] 54%|█████▍    | 54/100 [00:22<00:22,  2.09it/s] 55%|█████▌    | 55/100 [00:22<00:20,  2.17it/s] 56%|█████▌    | 56/100 [00:23<00:20,  2.20it/s] 57%|█████▋    | 57/100 [00:23<00:18,  2.38it/s] 58%|█████▊    | 58/100 [00:23<00:17,  2.43it/s] 59%|█████▉    | 59/100 [00:24<00:16,  2.53it/s] 60%|██████    | 60/100 [00:24<00:14,  2.70it/s] 61%|██████    | 61/100 [00:24<00:13,  2.88it/s] 62%|██████▏   | 62/100 [00:25<00:12,  3.04it/s] 63%|██████▎   | 63/100 [00:25<00:12,  3.05it/s] 64%|██████▍   | 64/100 [00:25<00:13,  2.76it/s] 65%|██████▌   | 65/100 [00:26<00:13,  2.69it/s] 66%|██████▌   | 66/100 [00:26<00:13,  2.55it/s] 67%|██████▋   | 67/100 [00:27<00:14,  2.20it/s] 68%|██████▊   | 68/100 [00:27<00:15,  2.11it/s] 69%|██████▉   | 69/100 [00:28<00:14,  2.10it/s] 70%|███████   | 70/100 [00:28<00:14,  2.13it/s] 71%|███████   | 71/100 [00:29<00:13,  2.16it/s] 72%|███████▏  | 72/100 [00:29<00:13,  2.14it/s] 73%|███████▎  | 73/100 [00:30<00:13,  2.04it/s] 74%|███████▍  | 74/100 [00:30<00:13,  1.94it/s] 75%|███████▌  | 75/100 [00:31<00:12,  1.99it/s] 76%|███████▌  | 76/100 [00:31<00:11,  2.12it/s] 77%|███████▋  | 77/100 [00:32<00:10,  2.11it/s] 78%|███████▊  | 78/100 [00:32<00:09,  2.21it/s] 79%|███████▉  | 79/100 [00:32<00:09,  2.25it/s] 80%|████████  | 80/100 [00:33<00:09,  2.18it/s] 81%|████████  | 81/100 [00:33<00:08,  2.20it/s] 82%|████████▏ | 82/100 [00:34<00:08,  2.12it/s] 83%|████████▎ | 83/100 [00:34<00:07,  2.21it/s] 84%|████████▍ | 84/100 [00:35<00:07,  2.22it/s] 85%|████████▌ | 85/100 [00:35<00:06,  2.19it/s] 86%|████████▌ | 86/100 [00:36<00:06,  2.30it/s] 87%|████████▋ | 87/100 [00:36<00:05,  2.33it/s] 88%|████████▊ | 88/100 [00:37<00:05,  2.20it/s] 89%|████████▉ | 89/100 [00:37<00:05,  2.17it/s] 90%|█████████ | 90/100 [00:37<00:04,  2.29it/s] 91%|█████████ | 91/100 [00:38<00:04,  2.24it/s] 92%|█████████▏| 92/100 [00:38<00:03,  2.20it/s] 93%|█████████▎| 93/100 [00:39<00:03,  2.05it/s] 94%|█████████▍| 94/100 [00:39<00:02,  2.01it/s] 95%|█████████▌| 95/100 [00:40<00:02,  2.15it/s] 96%|█████████▌| 96/100 [00:40<00:01,  2.25it/s] 97%|█████████▋| 97/100 [00:41<00:01,  2.40it/s] 98%|█████████▊| 98/100 [00:41<00:00,  2.32it/s] 99%|█████████▉| 99/100 [00:41<00:00,  2.52it/s]100%|██████████| 100/100 [00:42<00:00,  2.66it/s]100%|██████████| 100/100 [00:42<00:00,  2.37it/s]
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0139, 0.0139
--- total mse / var(X): 0.0139
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.195, 0.195
--- total mse / var(X): 0.195
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.255, 0.255
--- total mse / var(X): 0.255
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0101, 0.0101
--- total mse / var(X): 0.0101
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.202, 0.202
--- total mse / var(X): 0.202
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:50,  1.97it/s]  2%|▏         | 2/100 [00:00<00:39,  2.50it/s]  3%|▎         | 3/100 [00:01<00:33,  2.93it/s]  4%|▍         | 4/100 [00:01<00:32,  2.97it/s]  5%|▌         | 5/100 [00:02<00:40,  2.35it/s]  6%|▌         | 6/100 [00:02<00:43,  2.14it/s]  7%|▋         | 7/100 [00:02<00:40,  2.31it/s]  8%|▊         | 8/100 [00:03<00:36,  2.55it/s]  9%|▉         | 9/100 [00:03<00:32,  2.79it/s] 10%|█         | 10/100 [00:03<00:27,  3.28it/s] 11%|█         | 11/100 [00:03<00:26,  3.40it/s] 12%|█▏        | 12/100 [00:04<00:24,  3.56it/s] 13%|█▎        | 13/100 [00:04<00:22,  3.80it/s] 14%|█▍        | 14/100 [00:04<00:20,  4.18it/s] 15%|█▌        | 15/100 [00:04<00:19,  4.30it/s] 16%|█▌        | 16/100 [00:05<00:21,  3.93it/s] 17%|█▋        | 17/100 [00:05<00:20,  4.10it/s] 18%|█▊        | 18/100 [00:05<00:19,  4.20it/s] 19%|█▉        | 19/100 [00:05<00:18,  4.47it/s] 20%|██        | 20/100 [00:05<00:16,  4.86it/s] 21%|██        | 21/100 [00:06<00:18,  4.22it/s] 22%|██▏       | 22/100 [00:06<00:19,  4.01it/s] 23%|██▎       | 23/100 [00:06<00:18,  4.13it/s] 24%|██▍       | 24/100 [00:06<00:17,  4.30it/s] 25%|██▌       | 25/100 [00:07<00:16,  4.46it/s] 26%|██▌       | 26/100 [00:07<00:17,  4.29it/s] 27%|██▋       | 27/100 [00:07<00:16,  4.35it/s] 28%|██▊       | 28/100 [00:07<00:15,  4.71it/s] 29%|██▉       | 29/100 [00:07<00:13,  5.33it/s] 30%|███       | 30/100 [00:08<00:11,  5.86it/s] 31%|███       | 31/100 [00:08<00:14,  4.77it/s] 32%|███▏      | 32/100 [00:08<00:14,  4.62it/s] 33%|███▎      | 33/100 [00:08<00:16,  3.95it/s] 34%|███▍      | 34/100 [00:09<00:23,  2.75it/s] 35%|███▌      | 35/100 [00:10<00:35,  1.84it/s] 36%|███▌      | 36/100 [00:11<00:37,  1.71it/s] 37%|███▋      | 37/100 [00:12<00:43,  1.46it/s] 38%|███▊      | 38/100 [00:12<00:44,  1.38it/s] 39%|███▉      | 39/100 [00:13<00:39,  1.54it/s] 40%|████      | 40/100 [00:13<00:37,  1.61it/s] 41%|████      | 41/100 [00:14<00:37,  1.59it/s] 42%|████▏     | 42/100 [00:14<00:29,  2.00it/s] 43%|████▎     | 43/100 [00:15<00:23,  2.47it/s] 44%|████▍     | 44/100 [00:15<00:18,  3.06it/s] 45%|████▌     | 45/100 [00:15<00:15,  3.66it/s] 46%|████▌     | 46/100 [00:15<00:14,  3.72it/s] 47%|████▋     | 47/100 [00:15<00:12,  4.23it/s] 48%|████▊     | 48/100 [00:15<00:11,  4.39it/s] 49%|████▉     | 49/100 [00:16<00:12,  3.99it/s] 50%|█████     | 50/100 [00:16<00:12,  4.04it/s] 51%|█████     | 51/100 [00:17<00:19,  2.51it/s] 52%|█████▏    | 52/100 [00:18<00:28,  1.68it/s] 53%|█████▎    | 53/100 [00:18<00:28,  1.62it/s] 54%|█████▍    | 54/100 [00:19<00:26,  1.76it/s] 55%|█████▌    | 55/100 [00:19<00:25,  1.78it/s] 56%|█████▌    | 56/100 [00:20<00:20,  2.12it/s] 57%|█████▋    | 57/100 [00:20<00:16,  2.55it/s] 58%|█████▊    | 58/100 [00:20<00:14,  2.92it/s] 59%|█████▉    | 59/100 [00:20<00:12,  3.16it/s] 60%|██████    | 60/100 [00:21<00:11,  3.51it/s] 61%|██████    | 61/100 [00:21<00:10,  3.82it/s] 62%|██████▏   | 62/100 [00:21<00:09,  4.14it/s] 63%|██████▎   | 63/100 [00:21<00:08,  4.40it/s] 64%|██████▍   | 64/100 [00:21<00:08,  4.08it/s] 65%|██████▌   | 65/100 [00:22<00:08,  4.36it/s] 66%|██████▌   | 66/100 [00:22<00:07,  4.62it/s] 67%|██████▋   | 67/100 [00:22<00:07,  4.61it/s] 68%|██████▊   | 68/100 [00:22<00:07,  4.57it/s] 69%|██████▉   | 69/100 [00:23<00:06,  4.55it/s] 70%|███████   | 70/100 [00:23<00:06,  4.94it/s] 71%|███████   | 71/100 [00:23<00:05,  5.12it/s] 72%|███████▏  | 72/100 [00:23<00:05,  4.86it/s] 73%|███████▎  | 73/100 [00:24<00:07,  3.64it/s] 74%|███████▍  | 74/100 [00:24<00:08,  3.02it/s] 75%|███████▌  | 75/100 [00:24<00:08,  2.78it/s] 76%|███████▌  | 76/100 [00:25<00:07,  3.05it/s] 77%|███████▋  | 77/100 [00:25<00:06,  3.51it/s] 78%|███████▊  | 78/100 [00:25<00:05,  4.11it/s] 79%|███████▉  | 79/100 [00:25<00:04,  4.44it/s] 80%|████████  | 80/100 [00:25<00:04,  4.96it/s] 81%|████████  | 81/100 [00:25<00:03,  5.35it/s] 82%|████████▏ | 82/100 [00:26<00:03,  5.59it/s] 83%|████████▎ | 83/100 [00:26<00:03,  5.64it/s] 84%|████████▍ | 84/100 [00:26<00:02,  6.02it/s] 85%|████████▌ | 85/100 [00:26<00:02,  6.07it/s] 86%|████████▌ | 86/100 [00:26<00:02,  5.88it/s] 87%|████████▋ | 87/100 [00:27<00:02,  5.57it/s] 88%|████████▊ | 88/100 [00:27<00:02,  5.40it/s] 89%|████████▉ | 89/100 [00:27<00:02,  5.29it/s] 90%|█████████ | 90/100 [00:27<00:01,  5.25it/s] 91%|█████████ | 91/100 [00:27<00:01,  5.49it/s] 92%|█████████▏| 92/100 [00:27<00:01,  5.74it/s] 93%|█████████▎| 93/100 [00:28<00:01,  6.01it/s] 94%|█████████▍| 94/100 [00:28<00:00,  6.29it/s] 95%|█████████▌| 95/100 [00:28<00:00,  6.38it/s] 96%|█████████▌| 96/100 [00:28<00:00,  6.57it/s] 97%|█████████▋| 97/100 [00:28<00:00,  6.77it/s] 98%|█████████▊| 98/100 [00:28<00:00,  6.77it/s] 99%|█████████▉| 99/100 [00:28<00:00,  7.07it/s]100%|██████████| 100/100 [00:29<00:00,  6.72it/s]100%|██████████| 100/100 [00:29<00:00,  3.44it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00792, 0.00797
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00743, 0.00738
--- total mse / var(X): 0.00768
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:37,  2.66it/s]  2%|▏         | 2/100 [00:00<00:38,  2.55it/s]  3%|▎         | 3/100 [00:01<00:41,  2.35it/s]  4%|▍         | 4/100 [00:01<00:45,  2.09it/s]  5%|▌         | 5/100 [00:02<00:47,  2.02it/s]  6%|▌         | 6/100 [00:02<00:41,  2.25it/s]  7%|▋         | 7/100 [00:03<00:38,  2.43it/s]  8%|▊         | 8/100 [00:03<00:35,  2.57it/s]  9%|▉         | 9/100 [00:03<00:33,  2.71it/s] 10%|█         | 10/100 [00:04<00:32,  2.78it/s] 11%|█         | 11/100 [00:04<00:30,  2.88it/s] 12%|█▏        | 12/100 [00:04<00:29,  2.96it/s] 13%|█▎        | 13/100 [00:04<00:29,  2.99it/s] 14%|█▍        | 14/100 [00:05<00:28,  2.99it/s] 15%|█▌        | 15/100 [00:05<00:28,  3.01it/s] 16%|█▌        | 16/100 [00:05<00:27,  3.08it/s] 17%|█▋        | 17/100 [00:06<00:26,  3.13it/s] 18%|█▊        | 18/100 [00:06<00:28,  2.90it/s] 19%|█▉        | 19/100 [00:07<00:27,  2.92it/s] 20%|██        | 20/100 [00:07<00:27,  2.95it/s] 21%|██        | 21/100 [00:07<00:24,  3.18it/s] 22%|██▏       | 22/100 [00:07<00:25,  3.11it/s] 23%|██▎       | 23/100 [00:08<00:24,  3.15it/s] 24%|██▍       | 24/100 [00:08<00:22,  3.35it/s] 25%|██▌       | 25/100 [00:08<00:21,  3.52it/s] 26%|██▌       | 26/100 [00:09<00:21,  3.38it/s] 27%|██▋       | 27/100 [00:09<00:22,  3.18it/s] 28%|██▊       | 28/100 [00:09<00:28,  2.57it/s] 29%|██▉       | 29/100 [00:10<00:28,  2.52it/s] 30%|███       | 30/100 [00:10<00:28,  2.46it/s] 31%|███       | 31/100 [00:11<00:28,  2.40it/s] 32%|███▏      | 32/100 [00:11<00:26,  2.60it/s] 33%|███▎      | 33/100 [00:11<00:24,  2.74it/s] 34%|███▍      | 34/100 [00:12<00:24,  2.71it/s] 35%|███▌      | 35/100 [00:12<00:23,  2.80it/s] 36%|███▌      | 36/100 [00:12<00:23,  2.74it/s] 37%|███▋      | 37/100 [00:13<00:23,  2.73it/s] 38%|███▊      | 38/100 [00:13<00:22,  2.82it/s] 39%|███▉      | 39/100 [00:13<00:20,  2.99it/s] 40%|████      | 40/100 [00:14<00:20,  2.86it/s] 41%|████      | 41/100 [00:14<00:20,  2.91it/s] 42%|████▏     | 42/100 [00:14<00:19,  3.05it/s] 43%|████▎     | 43/100 [00:15<00:21,  2.67it/s] 44%|████▍     | 44/100 [00:15<00:20,  2.70it/s] 45%|████▌     | 45/100 [00:16<00:22,  2.42it/s] 46%|████▌     | 46/100 [00:16<00:20,  2.60it/s] 47%|████▋     | 47/100 [00:16<00:19,  2.69it/s] 48%|████▊     | 48/100 [00:17<00:19,  2.73it/s] 49%|████▉     | 49/100 [00:17<00:18,  2.77it/s] 50%|█████     | 50/100 [00:18<00:20,  2.42it/s] 51%|█████     | 51/100 [00:18<00:20,  2.41it/s] 52%|█████▏    | 52/100 [00:19<00:21,  2.28it/s] 53%|█████▎    | 53/100 [00:19<00:21,  2.17it/s] 54%|█████▍    | 54/100 [00:20<00:19,  2.33it/s] 55%|█████▌    | 55/100 [00:20<00:19,  2.26it/s] 56%|█████▌    | 56/100 [00:20<00:18,  2.39it/s] 57%|█████▋    | 57/100 [00:21<00:18,  2.37it/s] 58%|█████▊    | 58/100 [00:21<00:17,  2.39it/s] 59%|█████▉    | 59/100 [00:22<00:16,  2.47it/s] 60%|██████    | 60/100 [00:22<00:16,  2.47it/s] 61%|██████    | 61/100 [00:22<00:16,  2.37it/s] 62%|██████▏   | 62/100 [00:23<00:15,  2.42it/s] 63%|██████▎   | 63/100 [00:23<00:14,  2.56it/s] 64%|██████▍   | 64/100 [00:24<00:13,  2.66it/s] 65%|██████▌   | 65/100 [00:24<00:12,  2.76it/s] 66%|██████▌   | 66/100 [00:24<00:11,  2.93it/s] 67%|██████▋   | 67/100 [00:24<00:11,  2.90it/s] 68%|██████▊   | 68/100 [00:25<00:10,  3.08it/s] 69%|██████▉   | 69/100 [00:25<00:10,  3.03it/s] 70%|███████   | 70/100 [00:25<00:10,  2.88it/s] 71%|███████   | 71/100 [00:26<00:10,  2.90it/s] 72%|███████▏  | 72/100 [00:26<00:09,  2.96it/s] 73%|███████▎  | 73/100 [00:26<00:08,  3.10it/s] 74%|███████▍  | 74/100 [00:27<00:08,  3.07it/s] 75%|███████▌  | 75/100 [00:27<00:07,  3.15it/s] 76%|███████▌  | 76/100 [00:27<00:07,  3.04it/s] 77%|███████▋  | 77/100 [00:28<00:07,  3.07it/s] 78%|███████▊  | 78/100 [00:28<00:07,  3.09it/s] 79%|███████▉  | 79/100 [00:28<00:07,  2.97it/s] 80%|████████  | 80/100 [00:29<00:07,  2.81it/s] 81%|████████  | 81/100 [00:29<00:06,  2.87it/s] 82%|████████▏ | 82/100 [00:29<00:06,  2.93it/s] 83%|████████▎ | 83/100 [00:30<00:05,  3.01it/s] 84%|████████▍ | 84/100 [00:30<00:05,  2.89it/s] 85%|████████▌ | 85/100 [00:31<00:05,  2.90it/s] 86%|████████▌ | 86/100 [00:31<00:04,  3.01it/s] 87%|████████▋ | 87/100 [00:31<00:04,  2.85it/s] 88%|████████▊ | 88/100 [00:32<00:04,  2.90it/s] 89%|████████▉ | 89/100 [00:32<00:03,  2.81it/s] 90%|█████████ | 90/100 [00:32<00:03,  2.81it/s] 91%|█████████ | 91/100 [00:33<00:03,  2.62it/s] 92%|█████████▏| 92/100 [00:33<00:03,  2.66it/s] 93%|█████████▎| 93/100 [00:33<00:02,  2.73it/s] 94%|█████████▍| 94/100 [00:34<00:02,  2.56it/s] 95%|█████████▌| 95/100 [00:34<00:01,  2.74it/s] 96%|█████████▌| 96/100 [00:35<00:01,  2.74it/s] 97%|█████████▋| 97/100 [00:35<00:01,  2.89it/s] 98%|█████████▊| 98/100 [00:35<00:00,  3.00it/s] 99%|█████████▉| 99/100 [00:36<00:00,  2.72it/s]100%|██████████| 100/100 [00:36<00:00,  2.83it/s]100%|██████████| 100/100 [00:36<00:00,  2.75it/s]
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 4.77e-06, 4.77e-06
--- total mse / var(X): 4.77e-06
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:04,  1.53it/s]  2%|▏         | 2/100 [00:00<00:45,  2.15it/s]  3%|▎         | 3/100 [00:01<00:34,  2.79it/s]  4%|▍         | 4/100 [00:01<00:25,  3.72it/s]  5%|▌         | 5/100 [00:01<00:20,  4.58it/s]  6%|▌         | 6/100 [00:01<00:17,  5.37it/s]  7%|▋         | 7/100 [00:01<00:16,  5.54it/s]  8%|▊         | 8/100 [00:01<00:15,  5.90it/s]  9%|▉         | 9/100 [00:02<00:15,  5.90it/s] 10%|█         | 10/100 [00:02<00:15,  5.92it/s] 11%|█         | 11/100 [00:02<00:15,  5.90it/s] 12%|█▏        | 12/100 [00:02<00:16,  5.47it/s] 13%|█▎        | 13/100 [00:02<00:14,  5.95it/s] 14%|█▍        | 14/100 [00:02<00:14,  6.13it/s] 15%|█▌        | 15/100 [00:03<00:13,  6.12it/s] 16%|█▌        | 16/100 [00:03<00:20,  4.14it/s] 17%|█▋        | 17/100 [00:03<00:22,  3.65it/s] 18%|█▊        | 18/100 [00:04<00:24,  3.38it/s] 19%|█▉        | 19/100 [00:04<00:23,  3.49it/s] 20%|██        | 20/100 [00:04<00:23,  3.44it/s] 21%|██        | 21/100 [00:04<00:19,  4.08it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.15it/s] 23%|██▎       | 23/100 [00:05<00:19,  4.01it/s] 24%|██▍       | 24/100 [00:05<00:16,  4.57it/s] 25%|██▌       | 25/100 [00:05<00:16,  4.49it/s] 26%|██▌       | 26/100 [00:05<00:15,  4.80it/s] 27%|██▋       | 27/100 [00:06<00:14,  5.19it/s] 28%|██▊       | 28/100 [00:06<00:12,  5.70it/s] 29%|██▉       | 29/100 [00:06<00:15,  4.45it/s] 30%|███       | 30/100 [00:06<00:18,  3.73it/s] 31%|███       | 31/100 [00:07<00:16,  4.07it/s] 32%|███▏      | 32/100 [00:07<00:17,  3.94it/s] 33%|███▎      | 33/100 [00:07<00:15,  4.45it/s] 34%|███▍      | 34/100 [00:07<00:13,  5.05it/s] 35%|███▌      | 35/100 [00:07<00:12,  5.30it/s] 36%|███▌      | 36/100 [00:08<00:11,  5.60it/s] 37%|███▋      | 37/100 [00:08<00:10,  5.96it/s] 38%|███▊      | 38/100 [00:08<00:10,  5.84it/s] 39%|███▉      | 39/100 [00:08<00:13,  4.41it/s] 40%|████      | 40/100 [00:08<00:11,  5.02it/s] 41%|████      | 41/100 [00:09<00:11,  5.26it/s] 42%|████▏     | 42/100 [00:09<00:11,  5.24it/s] 43%|████▎     | 43/100 [00:09<00:10,  5.67it/s] 44%|████▍     | 44/100 [00:09<00:10,  5.59it/s] 45%|████▌     | 45/100 [00:09<00:09,  6.07it/s] 46%|████▌     | 46/100 [00:09<00:08,  6.04it/s] 47%|████▋     | 47/100 [00:09<00:08,  6.58it/s] 48%|████▊     | 48/100 [00:10<00:07,  7.24it/s] 49%|████▉     | 49/100 [00:10<00:08,  6.35it/s] 50%|█████     | 50/100 [00:10<00:08,  6.21it/s] 51%|█████     | 51/100 [00:10<00:07,  6.57it/s] 52%|█████▏    | 52/100 [00:10<00:07,  6.71it/s] 53%|█████▎    | 53/100 [00:10<00:06,  6.92it/s] 54%|█████▍    | 54/100 [00:10<00:06,  7.09it/s] 55%|█████▌    | 55/100 [00:11<00:06,  7.20it/s] 56%|█████▌    | 56/100 [00:11<00:05,  7.43it/s] 57%|█████▋    | 57/100 [00:11<00:05,  7.64it/s] 58%|█████▊    | 58/100 [00:11<00:05,  7.63it/s] 59%|█████▉    | 59/100 [00:11<00:05,  7.89it/s] 60%|██████    | 60/100 [00:11<00:05,  7.92it/s] 61%|██████    | 61/100 [00:11<00:04,  8.32it/s] 62%|██████▏   | 62/100 [00:11<00:04,  7.89it/s] 63%|██████▎   | 63/100 [00:12<00:04,  7.86it/s] 64%|██████▍   | 64/100 [00:12<00:04,  8.14it/s] 66%|██████▌   | 66/100 [00:12<00:03,  8.66it/s] 67%|██████▋   | 67/100 [00:12<00:04,  7.98it/s] 68%|██████▊   | 68/100 [00:12<00:03,  8.39it/s] 69%|██████▉   | 69/100 [00:12<00:03,  8.58it/s] 70%|███████   | 70/100 [00:12<00:03,  8.70it/s] 71%|███████   | 71/100 [00:13<00:03,  8.95it/s] 72%|███████▏  | 72/100 [00:13<00:03,  8.79it/s] 73%|███████▎  | 73/100 [00:13<00:03,  8.82it/s] 74%|███████▍  | 74/100 [00:13<00:03,  8.59it/s] 75%|███████▌  | 75/100 [00:13<00:02,  8.34it/s] 76%|███████▌  | 76/100 [00:13<00:03,  7.11it/s] 77%|███████▋  | 77/100 [00:13<00:03,  7.20it/s] 78%|███████▊  | 78/100 [00:13<00:03,  7.15it/s] 79%|███████▉  | 79/100 [00:14<00:03,  6.81it/s] 80%|████████  | 80/100 [00:14<00:03,  5.97it/s] 81%|████████  | 81/100 [00:14<00:03,  4.94it/s] 82%|████████▏ | 82/100 [00:14<00:03,  5.58it/s] 83%|████████▎ | 83/100 [00:14<00:02,  6.05it/s] 84%|████████▍ | 84/100 [00:15<00:02,  5.75it/s] 85%|████████▌ | 85/100 [00:15<00:02,  5.55it/s] 86%|████████▌ | 86/100 [00:15<00:02,  5.25it/s] 87%|████████▋ | 87/100 [00:15<00:02,  5.19it/s] 88%|████████▊ | 88/100 [00:15<00:02,  5.12it/s] 89%|████████▉ | 89/100 [00:16<00:02,  5.22it/s] 90%|█████████ | 90/100 [00:16<00:01,  5.45it/s] 91%|█████████ | 91/100 [00:16<00:01,  5.76it/s] 92%|█████████▏| 92/100 [00:16<00:01,  6.29it/s] 93%|█████████▎| 93/100 [00:16<00:01,  6.70it/s] 94%|█████████▍| 94/100 [00:16<00:00,  6.58it/s] 95%|█████████▌| 95/100 [00:16<00:00,  6.33it/s] 96%|█████████▌| 96/100 [00:17<00:00,  6.71it/s] 97%|█████████▋| 97/100 [00:17<00:00,  7.27it/s] 98%|█████████▊| 98/100 [00:17<00:00,  5.53it/s] 99%|█████████▉| 99/100 [00:17<00:00,  5.55it/s]100%|██████████| 100/100 [00:17<00:00,  6.33it/s]100%|██████████| 100/100 [00:17<00:00,  5.62it/s]
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0135, 0.0135
--- total mse / var(X): 0.0135
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:25,  3.90it/s]  2%|▏         | 2/100 [00:00<00:20,  4.73it/s]  3%|▎         | 3/100 [00:00<00:23,  4.08it/s]  4%|▍         | 4/100 [00:01<00:34,  2.79it/s]  5%|▌         | 5/100 [00:01<00:28,  3.31it/s]  6%|▌         | 6/100 [00:01<00:22,  4.15it/s]  7%|▋         | 7/100 [00:01<00:22,  4.06it/s]  8%|▊         | 8/100 [00:02<00:20,  4.46it/s]  9%|▉         | 9/100 [00:02<00:18,  4.87it/s] 10%|█         | 10/100 [00:02<00:16,  5.34it/s] 11%|█         | 11/100 [00:02<00:15,  5.85it/s] 12%|█▏        | 12/100 [00:02<00:13,  6.50it/s] 13%|█▎        | 13/100 [00:02<00:12,  6.90it/s] 14%|█▍        | 14/100 [00:02<00:12,  7.12it/s] 15%|█▌        | 15/100 [00:02<00:11,  7.29it/s] 16%|█▌        | 16/100 [00:03<00:12,  6.90it/s] 17%|█▋        | 17/100 [00:03<00:12,  6.64it/s] 18%|█▊        | 18/100 [00:03<00:12,  6.42it/s] 19%|█▉        | 19/100 [00:03<00:11,  6.85it/s] 20%|██        | 20/100 [00:03<00:11,  6.73it/s] 21%|██        | 21/100 [00:03<00:11,  6.68it/s] 22%|██▏       | 22/100 [00:04<00:11,  6.73it/s] 23%|██▎       | 23/100 [00:04<00:11,  6.60it/s] 24%|██▍       | 24/100 [00:04<00:11,  6.81it/s] 25%|██▌       | 25/100 [00:04<00:10,  7.18it/s] 26%|██▌       | 26/100 [00:04<00:09,  7.59it/s] 27%|██▋       | 27/100 [00:04<00:10,  7.12it/s] 28%|██▊       | 28/100 [00:04<00:11,  6.41it/s] 29%|██▉       | 29/100 [00:05<00:11,  6.36it/s] 30%|███       | 30/100 [00:05<00:12,  5.51it/s] 31%|███       | 31/100 [00:05<00:12,  5.61it/s] 32%|███▏      | 32/100 [00:05<00:12,  5.31it/s] 33%|███▎      | 33/100 [00:05<00:11,  5.90it/s] 34%|███▍      | 34/100 [00:06<00:11,  5.51it/s] 35%|███▌      | 35/100 [00:06<00:10,  6.02it/s] 36%|███▌      | 36/100 [00:06<00:11,  5.38it/s] 37%|███▋      | 37/100 [00:06<00:10,  5.77it/s] 38%|███▊      | 38/100 [00:06<00:09,  6.48it/s] 39%|███▉      | 39/100 [00:06<00:09,  6.14it/s] 40%|████      | 40/100 [00:07<00:10,  5.89it/s] 41%|████      | 41/100 [00:07<00:13,  4.33it/s] 42%|████▏     | 42/100 [00:07<00:12,  4.81it/s] 43%|████▎     | 43/100 [00:07<00:13,  4.23it/s] 44%|████▍     | 44/100 [00:08<00:16,  3.46it/s] 46%|████▌     | 46/100 [00:08<00:11,  4.88it/s] 48%|████▊     | 48/100 [00:08<00:09,  5.71it/s] 49%|████▉     | 49/100 [00:08<00:08,  5.77it/s] 50%|█████     | 50/100 [00:09<00:07,  6.31it/s] 51%|█████     | 51/100 [00:09<00:07,  6.43it/s] 52%|█████▏    | 52/100 [00:09<00:07,  6.56it/s] 53%|█████▎    | 53/100 [00:09<00:06,  6.77it/s] 54%|█████▍    | 54/100 [00:09<00:06,  7.17it/s] 55%|█████▌    | 55/100 [00:09<00:05,  7.63it/s] 56%|█████▌    | 56/100 [00:09<00:05,  7.74it/s] 57%|█████▋    | 57/100 [00:09<00:05,  7.23it/s] 58%|█████▊    | 58/100 [00:10<00:05,  7.02it/s] 59%|█████▉    | 59/100 [00:10<00:06,  6.29it/s] 60%|██████    | 60/100 [00:10<00:05,  6.71it/s] 61%|██████    | 61/100 [00:10<00:06,  6.10it/s] 62%|██████▏   | 62/100 [00:10<00:07,  5.30it/s] 63%|██████▎   | 63/100 [00:11<00:06,  5.69it/s] 64%|██████▍   | 64/100 [00:11<00:06,  5.82it/s] 65%|██████▌   | 65/100 [00:11<00:05,  6.16it/s] 66%|██████▌   | 66/100 [00:11<00:05,  6.21it/s] 67%|██████▋   | 67/100 [00:11<00:05,  6.14it/s] 68%|██████▊   | 68/100 [00:11<00:05,  5.95it/s] 69%|██████▉   | 69/100 [00:11<00:04,  6.67it/s] 70%|███████   | 70/100 [00:12<00:04,  6.11it/s] 71%|███████   | 71/100 [00:12<00:04,  6.70it/s] 72%|███████▏  | 72/100 [00:12<00:03,  7.06it/s] 73%|███████▎  | 73/100 [00:12<00:04,  6.59it/s] 74%|███████▍  | 74/100 [00:12<00:03,  7.22it/s] 75%|███████▌  | 75/100 [00:12<00:03,  7.73it/s] 76%|███████▌  | 76/100 [00:12<00:03,  7.86it/s] 77%|███████▋  | 77/100 [00:13<00:02,  7.76it/s] 78%|███████▊  | 78/100 [00:13<00:02,  7.63it/s] 80%|████████  | 80/100 [00:13<00:02,  8.39it/s] 81%|████████  | 81/100 [00:13<00:02,  8.24it/s] 82%|████████▏ | 82/100 [00:13<00:02,  7.50it/s] 83%|████████▎ | 83/100 [00:13<00:02,  7.64it/s] 84%|████████▍ | 84/100 [00:13<00:02,  7.09it/s] 85%|████████▌ | 85/100 [00:14<00:02,  5.50it/s] 86%|████████▌ | 86/100 [00:14<00:02,  6.17it/s] 87%|████████▋ | 87/100 [00:14<00:02,  5.91it/s] 88%|████████▊ | 88/100 [00:14<00:02,  5.95it/s] 89%|████████▉ | 89/100 [00:14<00:01,  6.21it/s] 90%|█████████ | 90/100 [00:15<00:02,  4.98it/s] 91%|█████████ | 91/100 [00:15<00:01,  4.58it/s] 92%|█████████▏| 92/100 [00:15<00:02,  3.84it/s] 93%|█████████▎| 93/100 [00:15<00:01,  4.02it/s] 94%|█████████▍| 94/100 [00:16<00:01,  4.45it/s] 95%|█████████▌| 95/100 [00:16<00:01,  4.61it/s] 96%|█████████▌| 96/100 [00:16<00:00,  4.32it/s] 97%|█████████▋| 97/100 [00:16<00:00,  4.66it/s] 98%|█████████▊| 98/100 [00:16<00:00,  5.31it/s] 99%|█████████▉| 99/100 [00:17<00:00,  5.66it/s]100%|██████████| 100/100 [00:17<00:00,  6.00it/s]100%|██████████| 100/100 [00:17<00:00,  5.81it/s]
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.000851, 0.000851
--- total mse / var(X): 0.000851
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:23,  4.16it/s]  2%|▏         | 2/100 [00:00<00:18,  5.24it/s]  3%|▎         | 3/100 [00:00<00:16,  6.02it/s]  4%|▍         | 4/100 [00:00<00:19,  4.92it/s]  5%|▌         | 5/100 [00:00<00:17,  5.52it/s]  6%|▌         | 6/100 [00:01<00:16,  5.80it/s]  7%|▋         | 7/100 [00:01<00:15,  5.82it/s]  8%|▊         | 8/100 [00:01<00:15,  6.11it/s]  9%|▉         | 9/100 [00:01<00:15,  5.86it/s] 10%|█         | 10/100 [00:01<00:14,  6.26it/s] 11%|█         | 11/100 [00:01<00:15,  5.87it/s] 12%|█▏        | 12/100 [00:02<00:13,  6.33it/s] 13%|█▎        | 13/100 [00:02<00:12,  6.90it/s] 14%|█▍        | 14/100 [00:02<00:13,  6.33it/s] 15%|█▌        | 15/100 [00:02<00:12,  6.57it/s] 16%|█▌        | 16/100 [00:02<00:12,  6.75it/s] 17%|█▋        | 17/100 [00:02<00:11,  6.99it/s] 18%|█▊        | 18/100 [00:02<00:11,  6.90it/s] 19%|█▉        | 19/100 [00:03<00:11,  7.33it/s] 20%|██        | 20/100 [00:03<00:10,  7.35it/s] 21%|██        | 21/100 [00:03<00:10,  7.25it/s] 22%|██▏       | 22/100 [00:03<00:11,  6.78it/s] 23%|██▎       | 23/100 [00:03<00:11,  6.99it/s] 24%|██▍       | 24/100 [00:03<00:11,  6.58it/s] 25%|██▌       | 25/100 [00:03<00:11,  6.34it/s] 26%|██▌       | 26/100 [00:04<00:10,  6.86it/s] 27%|██▋       | 27/100 [00:04<00:12,  5.90it/s] 28%|██▊       | 28/100 [00:04<00:10,  6.57it/s] 29%|██▉       | 29/100 [00:04<00:10,  6.93it/s] 30%|███       | 30/100 [00:04<00:09,  7.62it/s] 31%|███       | 31/100 [00:04<00:09,  7.46it/s] 32%|███▏      | 32/100 [00:04<00:09,  7.00it/s] 33%|███▎      | 33/100 [00:05<00:09,  6.71it/s] 34%|███▍      | 34/100 [00:05<00:12,  5.16it/s] 35%|███▌      | 35/100 [00:05<00:13,  4.98it/s] 36%|███▌      | 36/100 [00:05<00:12,  5.02it/s] 37%|███▋      | 37/100 [00:05<00:11,  5.48it/s] 38%|███▊      | 38/100 [00:06<00:10,  6.18it/s] 39%|███▉      | 39/100 [00:06<00:08,  6.86it/s] 40%|████      | 40/100 [00:06<00:08,  6.82it/s] 41%|████      | 41/100 [00:06<00:08,  7.02it/s] 42%|████▏     | 42/100 [00:06<00:09,  6.31it/s] 43%|████▎     | 43/100 [00:06<00:08,  6.52it/s] 44%|████▍     | 44/100 [00:06<00:07,  7.20it/s] 45%|████▌     | 45/100 [00:07<00:08,  6.65it/s] 46%|████▌     | 46/100 [00:07<00:07,  7.18it/s] 47%|████▋     | 47/100 [00:07<00:07,  6.80it/s] 48%|████▊     | 48/100 [00:07<00:06,  7.47it/s] 49%|████▉     | 49/100 [00:07<00:07,  6.92it/s] 50%|█████     | 50/100 [00:07<00:06,  7.29it/s] 51%|█████     | 51/100 [00:07<00:07,  6.85it/s] 52%|█████▏    | 52/100 [00:08<00:06,  6.90it/s] 53%|█████▎    | 53/100 [00:08<00:07,  5.92it/s] 54%|█████▍    | 54/100 [00:08<00:07,  6.44it/s] 55%|█████▌    | 55/100 [00:08<00:08,  5.02it/s] 56%|█████▌    | 56/100 [00:08<00:07,  5.62it/s] 57%|█████▋    | 57/100 [00:09<00:07,  5.42it/s] 58%|█████▊    | 58/100 [00:09<00:07,  5.91it/s] 59%|█████▉    | 59/100 [00:09<00:08,  5.12it/s] 60%|██████    | 60/100 [00:09<00:08,  4.50it/s] 61%|██████    | 61/100 [00:10<00:12,  3.13it/s] 62%|██████▏   | 62/100 [00:10<00:12,  3.11it/s] 63%|██████▎   | 63/100 [00:10<00:11,  3.30it/s] 64%|██████▍   | 64/100 [00:10<00:09,  3.88it/s] 65%|██████▌   | 65/100 [00:11<00:08,  4.35it/s] 66%|██████▌   | 66/100 [00:11<00:06,  5.09it/s] 67%|██████▋   | 67/100 [00:11<00:05,  5.69it/s] 68%|██████▊   | 68/100 [00:11<00:05,  6.33it/s] 69%|██████▉   | 69/100 [00:11<00:05,  6.18it/s] 70%|███████   | 70/100 [00:11<00:04,  6.04it/s] 71%|███████   | 71/100 [00:11<00:04,  6.66it/s] 72%|███████▏  | 72/100 [00:12<00:04,  6.43it/s] 73%|███████▎  | 73/100 [00:12<00:03,  6.94it/s] 74%|███████▍  | 74/100 [00:12<00:03,  6.55it/s] 75%|███████▌  | 75/100 [00:12<00:04,  6.20it/s] 76%|███████▌  | 76/100 [00:12<00:03,  6.08it/s] 77%|███████▋  | 77/100 [00:12<00:03,  6.50it/s] 78%|███████▊  | 78/100 [00:13<00:03,  7.22it/s] 80%|████████  | 80/100 [00:13<00:03,  5.99it/s] 81%|████████  | 81/100 [00:13<00:03,  6.24it/s] 82%|████████▏ | 82/100 [00:13<00:03,  5.72it/s] 84%|████████▍ | 84/100 [00:14<00:02,  6.13it/s] 85%|████████▌ | 85/100 [00:14<00:03,  4.40it/s] 86%|████████▌ | 86/100 [00:14<00:02,  4.68it/s] 87%|████████▋ | 87/100 [00:14<00:02,  4.64it/s] 88%|████████▊ | 88/100 [00:15<00:02,  4.71it/s] 89%|████████▉ | 89/100 [00:15<00:02,  5.15it/s] 90%|█████████ | 90/100 [00:15<00:01,  5.75it/s] 91%|█████████ | 91/100 [00:15<00:01,  6.11it/s] 92%|█████████▏| 92/100 [00:15<00:01,  6.36it/s] 93%|█████████▎| 93/100 [00:15<00:01,  5.98it/s] 94%|█████████▍| 94/100 [00:15<00:00,  6.38it/s] 95%|█████████▌| 95/100 [00:16<00:00,  7.15it/s] 96%|█████████▌| 96/100 [00:16<00:00,  7.67it/s] 98%|█████████▊| 98/100 [00:16<00:00,  8.53it/s] 99%|█████████▉| 99/100 [00:16<00:00,  8.12it/s]100%|██████████| 100/100 [00:16<00:00,  8.01it/s]100%|██████████| 100/100 [00:16<00:00,  6.00it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00557, 0.00529
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00422, 0.00443
--- total mse / var(X): 0.00486
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:24,  3.96it/s]  2%|▏         | 2/100 [00:00<00:26,  3.70it/s]  3%|▎         | 3/100 [00:00<00:24,  3.89it/s]  4%|▍         | 4/100 [00:01<00:25,  3.78it/s]  5%|▌         | 5/100 [00:01<00:25,  3.71it/s]  6%|▌         | 6/100 [00:01<00:24,  3.81it/s]  7%|▋         | 7/100 [00:01<00:22,  4.19it/s]  8%|▊         | 8/100 [00:01<00:21,  4.37it/s]  9%|▉         | 9/100 [00:02<00:20,  4.42it/s] 10%|█         | 10/100 [00:02<00:20,  4.35it/s] 11%|█         | 11/100 [00:03<00:31,  2.78it/s] 12%|█▏        | 12/100 [00:03<00:36,  2.40it/s] 13%|█▎        | 13/100 [00:04<00:43,  2.02it/s] 14%|█▍        | 14/100 [00:04<00:36,  2.36it/s] 15%|█▌        | 15/100 [00:04<00:32,  2.63it/s] 16%|█▌        | 16/100 [00:05<00:28,  2.90it/s] 17%|█▋        | 17/100 [00:05<00:27,  3.05it/s] 18%|█▊        | 18/100 [00:05<00:28,  2.92it/s] 19%|█▉        | 19/100 [00:05<00:24,  3.30it/s] 20%|██        | 20/100 [00:06<00:22,  3.48it/s] 21%|██        | 21/100 [00:06<00:21,  3.67it/s] 22%|██▏       | 22/100 [00:06<00:22,  3.42it/s] 23%|██▎       | 23/100 [00:07<00:21,  3.54it/s] 24%|██▍       | 24/100 [00:07<00:21,  3.50it/s] 25%|██▌       | 25/100 [00:07<00:20,  3.62it/s] 26%|██▌       | 26/100 [00:07<00:19,  3.83it/s] 27%|██▋       | 27/100 [00:08<00:17,  4.11it/s] 28%|██▊       | 28/100 [00:08<00:17,  4.01it/s] 29%|██▉       | 29/100 [00:08<00:16,  4.38it/s] 30%|███       | 30/100 [00:08<00:16,  4.36it/s] 31%|███       | 31/100 [00:08<00:16,  4.30it/s] 32%|███▏      | 32/100 [00:09<00:15,  4.29it/s] 33%|███▎      | 33/100 [00:09<00:15,  4.32it/s] 34%|███▍      | 34/100 [00:09<00:17,  3.80it/s] 35%|███▌      | 35/100 [00:09<00:16,  3.94it/s] 36%|███▌      | 36/100 [00:10<00:15,  4.25it/s] 37%|███▋      | 37/100 [00:10<00:15,  4.07it/s] 38%|███▊      | 38/100 [00:10<00:14,  4.32it/s] 39%|███▉      | 39/100 [00:11<00:16,  3.74it/s] 40%|████      | 40/100 [00:11<00:19,  3.14it/s] 41%|████      | 41/100 [00:11<00:19,  3.09it/s] 42%|████▏     | 42/100 [00:12<00:18,  3.21it/s] 43%|████▎     | 43/100 [00:12<00:17,  3.31it/s] 44%|████▍     | 44/100 [00:12<00:15,  3.62it/s] 45%|████▌     | 45/100 [00:13<00:18,  2.91it/s] 46%|████▌     | 46/100 [00:13<00:18,  2.89it/s] 47%|████▋     | 47/100 [00:13<00:17,  2.98it/s] 48%|████▊     | 48/100 [00:14<00:17,  2.98it/s] 49%|████▉     | 49/100 [00:14<00:16,  3.08it/s] 50%|█████     | 50/100 [00:14<00:15,  3.27it/s] 51%|█████     | 51/100 [00:14<00:15,  3.15it/s] 52%|█████▏    | 52/100 [00:15<00:14,  3.26it/s] 53%|█████▎    | 53/100 [00:15<00:16,  2.86it/s] 54%|█████▍    | 54/100 [00:15<00:14,  3.14it/s] 55%|█████▌    | 55/100 [00:16<00:15,  2.99it/s] 56%|█████▌    | 56/100 [00:16<00:16,  2.72it/s] 57%|█████▋    | 57/100 [00:17<00:15,  2.86it/s] 58%|█████▊    | 58/100 [00:17<00:14,  2.89it/s] 59%|█████▉    | 59/100 [00:17<00:14,  2.89it/s] 60%|██████    | 60/100 [00:18<00:13,  2.96it/s] 61%|██████    | 61/100 [00:18<00:14,  2.75it/s] 62%|██████▏   | 62/100 [00:18<00:13,  2.91it/s] 63%|██████▎   | 63/100 [00:19<00:12,  3.08it/s] 64%|██████▍   | 64/100 [00:19<00:11,  3.26it/s] 65%|██████▌   | 65/100 [00:19<00:10,  3.25it/s] 66%|██████▌   | 66/100 [00:19<00:09,  3.45it/s] 67%|██████▋   | 67/100 [00:20<00:09,  3.36it/s] 68%|██████▊   | 68/100 [00:20<00:09,  3.28it/s] 69%|██████▉   | 69/100 [00:20<00:08,  3.50it/s] 70%|███████   | 70/100 [00:21<00:08,  3.44it/s] 71%|███████   | 71/100 [00:21<00:08,  3.42it/s] 72%|███████▏  | 72/100 [00:21<00:07,  3.70it/s] 73%|███████▎  | 73/100 [00:21<00:07,  3.70it/s] 74%|███████▍  | 74/100 [00:22<00:07,  3.69it/s] 75%|███████▌  | 75/100 [00:22<00:06,  3.90it/s] 76%|███████▌  | 76/100 [00:22<00:06,  3.96it/s] 77%|███████▋  | 77/100 [00:22<00:05,  3.91it/s] 78%|███████▊  | 78/100 [00:23<00:05,  3.80it/s] 79%|███████▉  | 79/100 [00:23<00:05,  3.96it/s] 80%|████████  | 80/100 [00:23<00:04,  4.00it/s] 81%|████████  | 81/100 [00:23<00:04,  4.52it/s] 82%|████████▏ | 82/100 [00:23<00:03,  5.01it/s] 83%|████████▎ | 83/100 [00:24<00:03,  5.43it/s] 84%|████████▍ | 84/100 [00:24<00:02,  5.88it/s] 85%|████████▌ | 85/100 [00:24<00:02,  6.30it/s] 86%|████████▌ | 86/100 [00:24<00:02,  6.25it/s] 87%|████████▋ | 87/100 [00:24<00:02,  6.01it/s] 88%|████████▊ | 88/100 [00:24<00:02,  5.86it/s] 89%|████████▉ | 89/100 [00:25<00:01,  6.10it/s] 90%|█████████ | 90/100 [00:25<00:01,  6.45it/s] 91%|█████████ | 91/100 [00:25<00:01,  6.62it/s] 92%|█████████▏| 92/100 [00:25<00:01,  6.59it/s] 93%|█████████▎| 93/100 [00:25<00:01,  6.40it/s] 94%|█████████▍| 94/100 [00:25<00:01,  5.48it/s] 95%|█████████▌| 95/100 [00:26<00:00,  5.53it/s] 96%|█████████▌| 96/100 [00:26<00:00,  4.87it/s] 97%|█████████▋| 97/100 [00:26<00:00,  5.21it/s] 98%|█████████▊| 98/100 [00:26<00:00,  5.47it/s] 99%|█████████▉| 99/100 [00:26<00:00,  5.72it/s]100%|██████████| 100/100 [00:26<00:00,  5.93it/s]100%|██████████| 100/100 [00:26<00:00,  3.71it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00243, 0.00195
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00191, 0.00229
--- total mse / var(X): 0.00212
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]
/data/neelesh/DART_by_app/437/src/kmeans.py:46: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans1 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, :D//2])
/data/neelesh/DART_by_app/437/src/kmeans.py:47: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.
  kmeans2 = KMeans(n_clusters=sqrt_k, random_state=0).fit(X[:, D//2:])
Retrain for 100 epochs
running kmeans in subspace 1/1... kmeans: clustering in subspaces first; k, sqrt(k) = 16, 4
mse / {var(X_subs), var(X)}: 0.0521, 0.0521
--- total mse / var(X): 0.0521
start table evaluation...
Elapsed time: 234.2809591293335 seconds
Cosine similarity between AMM and exact (Train): 0.9875547
Cosine similarity between AMM and exact (Test): 0.9888359
p,r,f1: 0.48107800546561563 0.6549750920266213 0.5547172250969162
p,r,f1: 0.48658026375625285 0.6050234378092293 0.539376082227896
done
{'model': {'name': 'ViT',
           'layer': 4,
           'dim': 32,
           'f1': [0.48107800546561563, 0.6549750920266213, 0.5547172250969162],
           'num_param': 7424},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
               'K_CLUSTER': [16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16,
                             16],
               'cossim_layer_train': [0.986562967300415,
                                      0.971410870552063,
                                      0.9642720222473145,
                                      0.9875547289848328],
               'cossim_layer_test': [0.9743908047676086,
                                     0.9684057831764221,
                                     0.960952877998352,
                                     0.9888359308242798],
               'cossim_amm_train': [0.9723803997039795,
                                    0.9885459542274475,
                                    0.8941786885261536,
                                    0.8585846424102783,
                                    0.9098088145256042,
                                    0.9681177139282227,
                                    0.9690326452255249,
                                    0.9959721565246582],
               'cossim_amm_test': [0.9465979933738708,
                                   0.9879653453826904,
                                   0.8915908932685852,
                                   0.8578232526779175,
                                   0.9105263352394104,
                                   0.9590330719947815,
                                   0.9645199179649353,
                                   0.9963338375091553],
               'f1': [0.48658026375625285,
                      0.6050234378092293,
                      0.539376082227896],
               'lut_num': 8,
               'lut_shapes': [(16, 1, 16),
                              (96, 1, 16),
                              (16, 16, 1),
                              (16, 16, 1),
                              (16, 1, 16),
                              (16, 1, 16),
                              (16, 1, 16),
                              (256, 1, 16)],
               'lut_total_size': 7168}}
  1%|          | 1/100 [00:05<09:03,  5.49s/it]  2%|▏         | 2/100 [00:11<09:15,  5.67s/it]  3%|▎         | 3/100 [00:17<09:21,  5.78s/it]  4%|▍         | 4/100 [00:23<09:32,  5.96s/it]  5%|▌         | 5/100 [00:28<09:12,  5.81s/it]  6%|▌         | 6/100 [00:34<08:41,  5.55s/it]  7%|▋         | 7/100 [00:38<08:17,  5.35s/it]  8%|▊         | 8/100 [00:43<07:54,  5.16s/it]  9%|▉         | 9/100 [00:48<07:38,  5.04s/it] 10%|█         | 10/100 [00:54<08:08,  5.43s/it] 11%|█         | 11/100 [00:59<07:52,  5.31s/it] 12%|█▏        | 12/100 [01:04<07:20,  5.01s/it] 13%|█▎        | 13/100 [01:08<06:53,  4.75s/it] 14%|█▍        | 14/100 [01:13<06:55,  4.84s/it] 15%|█▌        | 15/100 [01:17<06:35,  4.65s/it] 16%|█▌        | 16/100 [01:22<06:26,  4.60s/it] 17%|█▋        | 17/100 [01:28<06:57,  5.03s/it] 18%|█▊        | 18/100 [01:33<07:10,  5.25s/it] 19%|█▉        | 19/100 [01:39<07:13,  5.35s/it] 20%|██        | 20/100 [01:45<07:23,  5.54s/it] 21%|██        | 21/100 [01:50<07:18,  5.55s/it] 22%|██▏       | 22/100 [01:57<07:24,  5.70s/it] 23%|██▎       | 23/100 [02:02<07:15,  5.65s/it] 24%|██▍       | 24/100 [02:08<07:19,  5.79s/it] 25%|██▌       | 25/100 [02:12<06:31,  5.22s/it] 26%|██▌       | 26/100 [02:17<06:11,  5.03s/it] 27%|██▋       | 27/100 [02:21<06:01,  4.95s/it] 28%|██▊       | 28/100 [02:28<06:23,  5.33s/it] 29%|██▉       | 29/100 [02:32<06:04,  5.13s/it] 30%|███       | 30/100 [02:37<05:53,  5.05s/it] 31%|███       | 31/100 [02:42<05:52,  5.11s/it] 32%|███▏      | 32/100 [02:48<06:02,  5.33s/it] 33%|███▎      | 33/100 [02:53<05:53,  5.27s/it] 34%|███▍      | 34/100 [02:58<05:42,  5.18s/it] 35%|███▌      | 35/100 [03:03<05:31,  5.10s/it] 36%|███▌      | 36/100 [03:09<05:44,  5.38s/it] 37%|███▋      | 37/100 [03:15<05:53,  5.61s/it] 38%|███▊      | 38/100 [03:20<05:35,  5.41s/it] 39%|███▉      | 39/100 [03:26<05:25,  5.34s/it] 40%|████      | 40/100 [03:31<05:16,  5.28s/it] 41%|████      | 41/100 [03:36<05:17,  5.39s/it] 42%|████▏     | 42/100 [03:42<05:10,  5.36s/it] 43%|████▎     | 43/100 [03:46<04:47,  5.04s/it] 44%|████▍     | 44/100 [03:51<04:39,  4.99s/it] 45%|████▌     | 45/100 [03:55<04:28,  4.89s/it] 46%|████▌     | 46/100 [04:00<04:24,  4.90s/it] 47%|████▋     | 47/100 [04:06<04:38,  5.25s/it] 48%|████▊     | 48/100 [04:12<04:31,  5.21s/it] 49%|████▉     | 49/100 [04:17<04:24,  5.18s/it] 50%|█████     | 50/100 [04:22<04:17,  5.16s/it] 51%|█████     | 51/100 [04:28<04:24,  5.41s/it] 52%|█████▏    | 52/100 [04:33<04:17,  5.37s/it] 53%|█████▎    | 53/100 [04:39<04:20,  5.54s/it] 54%|█████▍    | 54/100 [04:44<04:02,  5.28s/it] 55%|█████▌    | 55/100 [04:48<03:42,  4.93s/it] 56%|█████▌    | 56/100 [04:52<03:28,  4.74s/it] 57%|█████▋    | 57/100 [04:57<03:24,  4.76s/it] 58%|█████▊    | 58/100 [05:01<03:08,  4.49s/it] 59%|█████▉    | 59/100 [05:04<02:54,  4.25s/it] 60%|██████    | 60/100 [05:09<02:48,  4.21s/it] 61%|██████    | 61/100 [05:13<02:47,  4.29s/it] 62%|██████▏   | 62/100 [05:19<03:06,  4.90s/it] 63%|██████▎   | 63/100 [05:25<03:05,  5.01s/it] 64%|██████▍   | 64/100 [05:29<02:53,  4.81s/it] 65%|██████▌   | 65/100 [05:33<02:42,  4.64s/it] 66%|██████▌   | 66/100 [05:39<02:53,  5.10s/it] 67%|██████▋   | 67/100 [05:43<02:35,  4.72s/it] 68%|██████▊   | 68/100 [05:47<02:26,  4.57s/it] 69%|██████▉   | 69/100 [05:53<02:30,  4.85s/it] 70%|███████   | 70/100 [05:57<02:20,  4.69s/it] 71%|███████   | 71/100 [06:01<02:09,  4.48s/it] 72%|███████▏  | 72/100 [06:06<02:09,  4.64s/it] 73%|███████▎  | 73/100 [06:12<02:12,  4.90s/it] 74%|███████▍  | 74/100 [06:16<01:59,  4.59s/it] 75%|███████▌  | 75/100 [06:21<02:01,  4.86s/it] 76%|███████▌  | 76/100 [06:26<01:55,  4.80s/it] 77%|███████▋  | 77/100 [06:30<01:45,  4.61s/it] 78%|███████▊  | 78/100 [06:34<01:37,  4.43s/it] 79%|███████▉  | 79/100 [06:38<01:30,  4.29s/it] 80%|████████  | 80/100 [06:43<01:30,  4.50s/it] 81%|████████  | 81/100 [06:48<01:30,  4.75s/it] 82%|████████▏ | 82/100 [06:52<01:22,  4.59s/it] 83%|████████▎ | 83/100 [06:55<01:07,  3.99s/it] 84%|████████▍ | 84/100 [06:58<00:58,  3.66s/it] 85%|████████▌ | 85/100 [07:01<00:52,  3.48s/it] 86%|████████▌ | 86/100 [07:05<00:51,  3.68s/it] 87%|████████▋ | 87/100 [07:10<00:52,  4.02s/it] 88%|████████▊ | 88/100 [07:14<00:46,  3.90s/it] 89%|████████▉ | 89/100 [07:19<00:46,  4.22s/it] 90%|█████████ | 90/100 [07:22<00:39,  3.97s/it] 91%|█████████ | 91/100 [07:25<00:34,  3.80s/it] 92%|█████████▏| 92/100 [07:29<00:29,  3.70s/it] 93%|█████████▎| 93/100 [07:33<00:25,  3.70s/it] 94%|█████████▍| 94/100 [07:38<00:25,  4.23s/it] 95%|█████████▌| 95/100 [07:43<00:21,  4.38s/it] 96%|█████████▌| 96/100 [07:47<00:17,  4.37s/it] 97%|█████████▋| 97/100 [07:51<00:12,  4.33s/it] 98%|█████████▊| 98/100 [07:56<00:08,  4.35s/it] 99%|█████████▉| 99/100 [08:01<00:04,  4.57s/it]100%|██████████| 100/100 [08:06<00:00,  4.90s/it]100%|██████████| 100/100 [08:06<00:00,  4.87s/it]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0052, 0.00469
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00416, 0.00456
--- total mse / var(X): 0.00463
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00359, 0.00374
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00353, 0.00338
--- total mse / var(X): 0.00356
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0035, 0.00367
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00346, 0.00329
--- total mse / var(X): 0.00348
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00455, 0.00128
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00196, 0.00337
--- total mse / var(X): 0.00233
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00308, 0.00289
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0116, 0.0123
--- total mse / var(X): 0.00759
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:16,  6.18it/s]  2%|▏         | 2/100 [00:00<00:18,  5.19it/s]  3%|▎         | 3/100 [00:00<00:18,  5.37it/s]  4%|▍         | 4/100 [00:00<00:19,  4.99it/s]  5%|▌         | 5/100 [00:00<00:18,  5.09it/s]  6%|▌         | 6/100 [00:01<00:18,  5.15it/s]  7%|▋         | 7/100 [00:01<00:18,  5.06it/s]  8%|▊         | 8/100 [00:01<00:17,  5.32it/s]  9%|▉         | 9/100 [00:01<00:16,  5.48it/s] 10%|█         | 10/100 [00:01<00:16,  5.50it/s] 11%|█         | 11/100 [00:02<00:15,  5.79it/s] 12%|█▏        | 12/100 [00:02<00:16,  5.42it/s] 13%|█▎        | 13/100 [00:02<00:15,  5.72it/s] 14%|█▍        | 14/100 [00:02<00:14,  5.88it/s] 15%|█▌        | 15/100 [00:02<00:14,  5.90it/s] 16%|█▌        | 16/100 [00:02<00:13,  6.13it/s] 17%|█▋        | 17/100 [00:03<00:13,  6.16it/s] 18%|█▊        | 18/100 [00:03<00:12,  6.41it/s] 19%|█▉        | 19/100 [00:03<00:11,  6.80it/s] 20%|██        | 20/100 [00:03<00:11,  6.90it/s] 21%|██        | 21/100 [00:03<00:10,  7.21it/s] 22%|██▏       | 22/100 [00:03<00:10,  7.57it/s] 23%|██▎       | 23/100 [00:03<00:10,  7.58it/s] 24%|██▍       | 24/100 [00:03<00:10,  7.51it/s] 25%|██▌       | 25/100 [00:04<00:10,  7.41it/s] 26%|██▌       | 26/100 [00:04<00:09,  7.45it/s] 27%|██▋       | 27/100 [00:04<00:09,  7.63it/s] 28%|██▊       | 28/100 [00:04<00:09,  7.58it/s] 29%|██▉       | 29/100 [00:04<00:09,  7.35it/s] 30%|███       | 30/100 [00:04<00:09,  7.36it/s] 31%|███       | 31/100 [00:04<00:09,  7.46it/s] 32%|███▏      | 32/100 [00:05<00:09,  7.33it/s] 33%|███▎      | 33/100 [00:05<00:09,  7.31it/s] 34%|███▍      | 34/100 [00:05<00:08,  7.43it/s] 35%|███▌      | 35/100 [00:05<00:08,  7.40it/s] 36%|███▌      | 36/100 [00:05<00:08,  7.52it/s] 37%|███▋      | 37/100 [00:05<00:08,  7.27it/s] 38%|███▊      | 38/100 [00:05<00:08,  7.19it/s] 39%|███▉      | 39/100 [00:05<00:08,  7.20it/s] 40%|████      | 40/100 [00:06<00:07,  7.60it/s] 42%|████▏     | 42/100 [00:06<00:06,  8.59it/s] 43%|████▎     | 43/100 [00:06<00:07,  7.40it/s] 44%|████▍     | 44/100 [00:06<00:07,  7.56it/s] 45%|████▌     | 45/100 [00:06<00:07,  7.59it/s] 46%|████▌     | 46/100 [00:06<00:07,  7.65it/s] 47%|████▋     | 47/100 [00:07<00:07,  7.57it/s] 49%|████▉     | 49/100 [00:07<00:06,  8.34it/s] 51%|█████     | 51/100 [00:07<00:05,  9.05it/s] 52%|█████▏    | 52/100 [00:07<00:05,  9.21it/s] 54%|█████▍    | 54/100 [00:07<00:04,  9.88it/s] 55%|█████▌    | 55/100 [00:07<00:04,  9.90it/s] 57%|█████▋    | 57/100 [00:07<00:04, 10.23it/s] 59%|█████▉    | 59/100 [00:08<00:03, 10.67it/s] 61%|██████    | 61/100 [00:08<00:03, 11.24it/s] 63%|██████▎   | 63/100 [00:08<00:03, 11.41it/s] 65%|██████▌   | 65/100 [00:08<00:03, 11.53it/s] 67%|██████▋   | 67/100 [00:08<00:02, 11.20it/s] 69%|██████▉   | 69/100 [00:09<00:02, 11.37it/s] 71%|███████   | 71/100 [00:09<00:02, 11.42it/s] 73%|███████▎  | 73/100 [00:09<00:02, 11.36it/s] 75%|███████▌  | 75/100 [00:09<00:02, 11.06it/s] 77%|███████▋  | 77/100 [00:09<00:02, 10.87it/s] 79%|███████▉  | 79/100 [00:09<00:01, 10.89it/s] 81%|████████  | 81/100 [00:10<00:01, 10.77it/s] 83%|████████▎ | 83/100 [00:10<00:01, 10.82it/s] 85%|████████▌ | 85/100 [00:10<00:01, 10.62it/s] 87%|████████▋ | 87/100 [00:10<00:01, 10.54it/s] 89%|████████▉ | 89/100 [00:10<00:01, 10.78it/s] 91%|█████████ | 91/100 [00:11<00:00, 11.22it/s] 93%|█████████▎| 93/100 [00:11<00:00, 11.36it/s] 95%|█████████▌| 95/100 [00:11<00:00, 11.48it/s] 97%|█████████▋| 97/100 [00:11<00:00, 11.47it/s] 99%|█████████▉| 99/100 [00:11<00:00, 11.39it/s]100%|██████████| 100/100 [00:11<00:00,  8.47it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00774, 0.0134
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0262, 0.00721
--- total mse / var(X): 0.0103
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:06, 16.25it/s]  4%|▍         | 4/100 [00:00<00:06, 15.51it/s]  6%|▌         | 6/100 [00:00<00:05, 15.83it/s]  8%|▊         | 8/100 [00:00<00:05, 15.94it/s] 10%|█         | 10/100 [00:00<00:05, 15.67it/s] 12%|█▏        | 12/100 [00:00<00:05, 16.08it/s] 14%|█▍        | 14/100 [00:00<00:05, 16.10it/s] 16%|█▌        | 16/100 [00:01<00:05, 15.97it/s] 18%|█▊        | 18/100 [00:01<00:05, 15.67it/s] 20%|██        | 20/100 [00:01<00:05, 15.63it/s] 22%|██▏       | 22/100 [00:01<00:05, 15.48it/s] 24%|██▍       | 24/100 [00:01<00:04, 15.59it/s] 26%|██▌       | 26/100 [00:01<00:04, 15.87it/s] 28%|██▊       | 28/100 [00:01<00:04, 16.23it/s] 30%|███       | 30/100 [00:01<00:04, 15.93it/s] 32%|███▏      | 32/100 [00:02<00:04, 15.61it/s] 34%|███▍      | 34/100 [00:02<00:04, 15.59it/s] 36%|███▌      | 36/100 [00:02<00:04, 14.81it/s] 38%|███▊      | 38/100 [00:02<00:04, 14.34it/s] 40%|████      | 40/100 [00:02<00:03, 15.01it/s] 42%|████▏     | 42/100 [00:02<00:03, 14.88it/s] 44%|████▍     | 44/100 [00:02<00:03, 14.34it/s] 46%|████▌     | 46/100 [00:03<00:03, 14.08it/s] 48%|████▊     | 48/100 [00:03<00:03, 14.51it/s] 50%|█████     | 50/100 [00:03<00:03, 14.23it/s] 52%|█████▏    | 52/100 [00:03<00:03, 14.71it/s] 54%|█████▍    | 54/100 [00:03<00:03, 14.59it/s] 56%|█████▌    | 56/100 [00:03<00:02, 14.86it/s] 58%|█████▊    | 58/100 [00:03<00:02, 14.42it/s] 60%|██████    | 60/100 [00:03<00:02, 14.69it/s] 62%|██████▏   | 62/100 [00:04<00:02, 15.30it/s] 64%|██████▍   | 64/100 [00:04<00:02, 15.75it/s] 66%|██████▌   | 66/100 [00:04<00:02, 15.93it/s] 68%|██████▊   | 68/100 [00:04<00:02, 15.46it/s] 70%|███████   | 70/100 [00:04<00:01, 15.61it/s] 72%|███████▏  | 72/100 [00:04<00:01, 15.84it/s] 74%|███████▍  | 74/100 [00:04<00:01, 15.43it/s] 76%|███████▌  | 76/100 [00:04<00:01, 15.45it/s] 78%|███████▊  | 78/100 [00:05<00:01, 14.19it/s] 80%|████████  | 80/100 [00:05<00:01, 14.61it/s] 82%|████████▏ | 82/100 [00:05<00:01, 15.05it/s] 84%|████████▍ | 84/100 [00:05<00:01, 15.22it/s] 86%|████████▌ | 86/100 [00:05<00:00, 15.03it/s] 88%|████████▊ | 88/100 [00:05<00:00, 15.04it/s] 90%|█████████ | 90/100 [00:05<00:00, 15.28it/s] 92%|█████████▏| 92/100 [00:06<00:00, 15.39it/s] 94%|█████████▍| 94/100 [00:06<00:00, 15.69it/s] 96%|█████████▌| 96/100 [00:06<00:00, 15.73it/s] 98%|█████████▊| 98/100 [00:06<00:00, 16.09it/s]100%|██████████| 100/100 [00:06<00:00, 15.91it/s]100%|██████████| 100/100 [00:06<00:00, 15.30it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0069, 0.00619
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00538, 0.00593
--- total mse / var(X): 0.00606
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:06, 16.06it/s]  5%|▌         | 5/100 [00:00<00:05, 17.87it/s]  7%|▋         | 7/100 [00:00<00:05, 17.72it/s]  9%|▉         | 9/100 [00:00<00:05, 17.69it/s] 12%|█▏        | 12/100 [00:00<00:04, 19.38it/s] 15%|█▌        | 15/100 [00:00<00:04, 20.18it/s] 18%|█▊        | 18/100 [00:00<00:04, 20.43it/s] 21%|██        | 21/100 [00:01<00:04, 19.23it/s] 24%|██▍       | 24/100 [00:01<00:03, 19.65it/s] 27%|██▋       | 27/100 [00:01<00:03, 20.27it/s] 30%|███       | 30/100 [00:01<00:03, 19.93it/s] 33%|███▎      | 33/100 [00:01<00:03, 20.25it/s] 36%|███▌      | 36/100 [00:01<00:03, 20.09it/s] 39%|███▉      | 39/100 [00:01<00:02, 21.02it/s] 42%|████▏     | 42/100 [00:02<00:02, 21.91it/s] 45%|████▌     | 45/100 [00:02<00:02, 23.29it/s] 48%|████▊     | 48/100 [00:02<00:02, 23.54it/s] 51%|█████     | 51/100 [00:02<00:02, 22.92it/s] 54%|█████▍    | 54/100 [00:02<00:02, 22.84it/s] 57%|█████▋    | 57/100 [00:02<00:01, 22.91it/s] 60%|██████    | 60/100 [00:02<00:01, 23.50it/s] 63%|██████▎   | 63/100 [00:02<00:01, 23.74it/s] 66%|██████▌   | 66/100 [00:03<00:01, 23.61it/s] 69%|██████▉   | 69/100 [00:03<00:01, 22.99it/s] 72%|███████▏  | 72/100 [00:03<00:01, 23.29it/s] 75%|███████▌  | 75/100 [00:03<00:01, 24.39it/s] 78%|███████▊  | 78/100 [00:03<00:00, 24.50it/s] 81%|████████  | 81/100 [00:03<00:00, 25.21it/s] 84%|████████▍ | 84/100 [00:03<00:00, 26.38it/s] 87%|████████▋ | 87/100 [00:03<00:00, 25.79it/s] 90%|█████████ | 90/100 [00:04<00:00, 26.06it/s] 93%|█████████▎| 93/100 [00:04<00:00, 25.32it/s] 96%|█████████▌| 96/100 [00:04<00:00, 25.34it/s] 99%|█████████▉| 99/100 [00:04<00:00, 25.34it/s]100%|██████████| 100/100 [00:04<00:00, 22.50it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.0024, 0.00238
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00315, 0.00316
--- total mse / var(X): 0.00277
Retrain weight
  0%|          | 0/100 [00:00<?, ?it/s]  3%|▎         | 3/100 [00:00<00:04, 19.68it/s]  6%|▌         | 6/100 [00:00<00:04, 20.56it/s]  9%|▉         | 9/100 [00:00<00:03, 23.28it/s] 13%|█▎        | 13/100 [00:00<00:03, 26.58it/s] 17%|█▋        | 17/100 [00:00<00:02, 30.21it/s] 21%|██        | 21/100 [00:00<00:02, 31.67it/s] 25%|██▌       | 25/100 [00:00<00:02, 33.76it/s] 29%|██▉       | 29/100 [00:00<00:02, 33.62it/s] 33%|███▎      | 33/100 [00:01<00:01, 33.97it/s] 37%|███▋      | 37/100 [00:01<00:01, 32.57it/s] 41%|████      | 41/100 [00:01<00:01, 29.69it/s] 46%|████▌     | 46/100 [00:01<00:01, 32.67it/s] 50%|█████     | 50/100 [00:01<00:01, 34.19it/s] 54%|█████▍    | 54/100 [00:01<00:01, 33.61it/s] 58%|█████▊    | 58/100 [00:01<00:01, 34.62it/s] 62%|██████▏   | 62/100 [00:01<00:01, 34.03it/s] 66%|██████▌   | 66/100 [00:02<00:01, 33.70it/s] 70%|███████   | 70/100 [00:02<00:00, 32.66it/s] 74%|███████▍  | 74/100 [00:02<00:00, 32.31it/s] 78%|███████▊  | 78/100 [00:02<00:00, 33.68it/s] 82%|████████▏ | 82/100 [00:02<00:00, 33.99it/s] 86%|████████▌ | 86/100 [00:02<00:00, 34.03it/s] 90%|█████████ | 90/100 [00:02<00:00, 34.74it/s] 94%|█████████▍| 94/100 [00:02<00:00, 34.28it/s] 98%|█████████▊| 98/100 [00:03<00:00, 17.44it/s]100%|██████████| 100/100 [00:03<00:00, 28.66it/s]
Retrain for 100 epochs
running kmeans in subspace 1/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00559, 0.00503
running kmeans in subspace 2/2... kmeans: clustering in subspaces first; k, sqrt(k) = 256, 16
mse / {var(X_subs), var(X)}: 0.00385, 0.00424
--- total mse / var(X): 0.00464
start table evaluation...
Elapsed time: 271.75452876091003 seconds
Cosine similarity between AMM and exact (Train): 0.9930449
Cosine similarity between AMM and exact (Test): 0.99203867
p,r,f1: 0.48347321841617935 0.648070975001555 0.5538006590794149
p,r,f1: 0.4864086727383374 0.6161627584803027 0.5436507804522731
done
{'model': {'name': 'ViT',
           'layer': 6,
           'dim': 32,
           'f1': [0.48347321841617935, 0.648070975001555, 0.5538006590794149],
           'num_param': 30176},
 'estimator': {'method': 'PQ_KMEANS',
               'N_SUBSPACE': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
               'K_CLUSTER': [256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256,
                             256],
               'cossim_layer_train': [0.9991484880447388,
                                      0.9993904829025269,
                                      0.9988287687301636,
                                      0.9952769875526428,
                                      0.9946227669715881,
                                      0.993044912815094],
               'cossim_layer_test': [0.9971420764923096,
                                     0.9989528656005859,
                                     0.9981765747070312,
                                     0.9931584596633911,
                                     0.9922434687614441,
                                     0.9920386075973511],
               'cossim_amm_train': [0.9983296394348145,
                                    0.9988391399383545,
                                    0.9962396621704102,
                                    0.9954076409339905,
                                    0.996756374835968,
                                    0.996417760848999,
                                    0.9945548176765442,
                                    0.9944981932640076,
                                    0.9871667623519897,
                                    0.9688805341720581,
                                    0.9728015065193176,
                                    0.9908976554870605,
                                    0.9930897951126099,
                                    0.9979036450386047],
               'cossim_amm_test': [0.9943303465843201,
                                   0.9977584481239319,
                                   0.994316816329956,
                                   0.994026243686676,
                                   0.9953800439834595,
                                   0.9945332407951355,
                                   0.9917237162590027,
                                   0.9916188716888428,
                                   0.9828877449035645,
                                   0.959700882434845,
                                   0.959425687789917,
                                   0.9866815209388733,
                                   0.9903607368469238,
                                   0.9976317882537842],
               'f1': [0.4864086727383374,
                      0.6161627584803027,
                      0.5436507804522731],
               'lut_num': 14,
               'lut_shapes': [(32, 2, 256),
                              (192, 2, 256),
                              (256, 256, 2),
                              (256, 256, 2),
                              (32, 2, 256),
                              (32, 2, 256),
                              (32, 2, 256),
                              (192, 2, 256),
                              (256, 256, 2),
                              (256, 256, 2),
                              (32, 2, 256),
                              (32, 2, 256),
                              (32, 2, 256),
                              (256, 2, 256)],
               'lut_total_size': 966656}}
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        2,672
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 7,424
Trainable params: 7,424
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               192
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 176
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        2,672
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              32
│    └─Linear: 2-5                                 4,352
===========================================================================
Total params: 7,424
Trainable params: 7,424
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.5261780889 - test_loss: 0.3633785744
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.2917415153 - test_loss: 0.2384613532
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.2150852586 - test_loss: 0.1955791634
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.1878086451 - test_loss: 0.1796328380
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.1770084661 - test_loss: 0.1729140431
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.1721980767 - test_loss: 0.1697053706
-------- Save Best Model! --------
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.1697886863 - test_loss: 0.1680347659
-------- Save Best Model! --------
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.1684935475 - test_loss: 0.1670633582
-------- Save Best Model! --------
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.1676594472 - test_loss: 0.1664202161
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.1670715680 - test_loss: 0.1660280907
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.1665654419 - test_loss: 0.1657949635
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.1660356467 - test_loss: 0.1656581755
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.1654394504 - test_loss: 0.1658289346
Early Stop Left: 4
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.1647366537 - test_loss: 0.1661539700
Early Stop Left: 3
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.1639488469 - test_loss: 0.1659614900
Early Stop Left: 2
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.1632458234 - test_loss: 0.1655371787
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.1626285881 - test_loss: 0.1654048725
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.1620581138 - test_loss: 0.1651739330
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.1613849723 - test_loss: 0.1649914252
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.1608032278 - test_loss: 0.1650590840
Early Stop Left: 4
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.1604162044 - test_loss: 0.1655694082
Early Stop Left: 3
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.1601255930 - test_loss: 0.1655314082
Early Stop Left: 2
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.1597940844 - test_loss: 0.1655437297
Early Stop Left: 1
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.1594979062 - test_loss: 0.1655220138
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 15%|█▌        | 16/104 [00:00<00:00, 157.25it/s] 32%|███▏      | 33/104 [00:00<00:00, 161.88it/s] 48%|████▊     | 50/104 [00:00<00:00, 163.89it/s] 64%|██████▍   | 67/104 [00:00<00:00, 163.20it/s] 81%|████████  | 84/104 [00:00<00:00, 163.01it/s] 97%|█████████▋| 101/104 [00:00<00:00, 164.67it/s]100%|██████████| 104/104 [00:00<00:00, 164.88it/s]
Best micro threshold=0.301758, fscore=0.552
p,r,f1: 0.48508880981980823 0.6398418254161224 0.5518208853809811
throttleing by fixed threshold: 0.5
p,r,f1: 0.6356408329510815 0.36827423461935 0.46635447322129686
{'model': 'vit_min',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.3017576038837433,
                 'p': 0.48508880981980823,
                 'r': 0.6398418254161224,
                 'f1': 0.5518208853809811},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6356408329510815,
                 'r': 0.36827423461935,
                 'f1': 0.46635447322129686}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
TMAP                                               384
├─Sequential: 1-1                                  --
│    └─Rearrange: 2-1                              --
│    └─Linear: 2-2                                 352
├─Dropout: 1-2                                     --
├─Transformer: 1-3                                 --
│    └─ModuleList: 2-3                             --
│    │    └─ModuleList: 3-1                        10,464
│    │    └─ModuleList: 3-2                        10,464
├─Identity: 1-4                                    --
├─Sequential: 1-5                                  --
│    └─LayerNorm: 2-4                              64
│    └─Linear: 2-5                                 8,448
===========================================================================
Total params: 30,176
Trainable params: 30,176
Non-trainable params: 0
===========================================================================
Loading data for model
Data loaded successfully for stu
------- START EPOCH 1 -------
Epoch: 1 - loss: 0.4131140939 - test_loss: 0.2506519934
-------- Save Best Model! --------
------- START EPOCH 2 -------
Epoch: 2 - loss: 0.2105920316 - test_loss: 0.1856295435
-------- Save Best Model! --------
------- START EPOCH 3 -------
Epoch: 3 - loss: 0.1787922723 - test_loss: 0.1722835393
-------- Save Best Model! --------
------- START EPOCH 4 -------
Epoch: 4 - loss: 0.1709942480 - test_loss: 0.1682712443
-------- Save Best Model! --------
------- START EPOCH 5 -------
Epoch: 5 - loss: 0.1681595482 - test_loss: 0.1670202034
-------- Save Best Model! --------
------- START EPOCH 6 -------
Epoch: 6 - loss: 0.1666086534 - test_loss: 0.1674054982
Early Stop Left: 4
------- START EPOCH 7 -------
Epoch: 7 - loss: 0.1651261483 - test_loss: 0.1678621131
Early Stop Left: 3
------- START EPOCH 8 -------
Epoch: 8 - loss: 0.1631372809 - test_loss: 0.1673395422
Early Stop Left: 2
------- START EPOCH 9 -------
Epoch: 9 - loss: 0.1616433461 - test_loss: 0.1667871502
-------- Save Best Model! --------
------- START EPOCH 10 -------
Epoch: 10 - loss: 0.1605794407 - test_loss: 0.1653986610
-------- Save Best Model! --------
------- START EPOCH 11 -------
Epoch: 11 - loss: 0.1597982725 - test_loss: 0.1643737908
-------- Save Best Model! --------
------- START EPOCH 12 -------
Epoch: 12 - loss: 0.1590242664 - test_loss: 0.1624511710
-------- Save Best Model! --------
------- START EPOCH 13 -------
Epoch: 13 - loss: 0.1583462105 - test_loss: 0.1599027830
-------- Save Best Model! --------
------- START EPOCH 14 -------
Epoch: 14 - loss: 0.1576240845 - test_loss: 0.1581536337
-------- Save Best Model! --------
------- START EPOCH 15 -------
Epoch: 15 - loss: 0.1569940937 - test_loss: 0.1571800909
-------- Save Best Model! --------
------- START EPOCH 16 -------
Epoch: 16 - loss: 0.1563263117 - test_loss: 0.1565466126
-------- Save Best Model! --------
------- START EPOCH 17 -------
Epoch: 17 - loss: 0.1555440873 - test_loss: 0.1559967734
-------- Save Best Model! --------
------- START EPOCH 18 -------
Epoch: 18 - loss: 0.1545801931 - test_loss: 0.1553137052
-------- Save Best Model! --------
------- START EPOCH 19 -------
Epoch: 19 - loss: 0.1530858407 - test_loss: 0.1546775920
-------- Save Best Model! --------
------- START EPOCH 20 -------
Epoch: 20 - loss: 0.1508648621 - test_loss: 0.1542535952
-------- Save Best Model! --------
------- START EPOCH 21 -------
Epoch: 21 - loss: 0.1485866906 - test_loss: 0.1534037402
-------- Save Best Model! --------
------- START EPOCH 22 -------
Epoch: 22 - loss: 0.1461162663 - test_loss: 0.1522428652
-------- Save Best Model! --------
------- START EPOCH 23 -------
Epoch: 23 - loss: 0.1446639568 - test_loss: 0.1514175015
-------- Save Best Model! --------
------- START EPOCH 24 -------
Epoch: 24 - loss: 0.1434781445 - test_loss: 0.1511770479
-------- Save Best Model! --------
------- START EPOCH 25 -------
Epoch: 25 - loss: 0.1425747236 - test_loss: 0.1511255162
-------- Save Best Model! --------
------- START EPOCH 26 -------
Epoch: 26 - loss: 0.1417390866 - test_loss: 0.1504128305
-------- Save Best Model! --------
------- START EPOCH 27 -------
Epoch: 27 - loss: 0.1409548420 - test_loss: 0.1496044134
-------- Save Best Model! --------
------- START EPOCH 28 -------
Epoch: 28 - loss: 0.1402411658 - test_loss: 0.1493730580
-------- Save Best Model! --------
------- START EPOCH 29 -------
Epoch: 29 - loss: 0.1395387011 - test_loss: 0.1489704384
-------- Save Best Model! --------
------- START EPOCH 30 -------
Epoch: 30 - loss: 0.1388234809 - test_loss: 0.1480037673
-------- Save Best Model! --------
------- START EPOCH 31 -------
Epoch: 31 - loss: 0.1382078460 - test_loss: 0.1486945851
Early Stop Left: 4
------- START EPOCH 32 -------
Epoch: 32 - loss: 0.1375844318 - test_loss: 0.1481680520
Early Stop Left: 3
------- START EPOCH 33 -------
Epoch: 33 - loss: 0.1369661448 - test_loss: 0.1485791579
Early Stop Left: 2
------- START EPOCH 34 -------
Epoch: 34 - loss: 0.1364155116 - test_loss: 0.1469186372
-------- Save Best Model! --------
------- START EPOCH 35 -------
Epoch: 35 - loss: 0.1357234989 - test_loss: 0.1490925462
Early Stop Left: 4
------- START EPOCH 36 -------
Epoch: 36 - loss: 0.1352587989 - test_loss: 0.1471679968
Early Stop Left: 3
------- START EPOCH 37 -------
Epoch: 37 - loss: 0.1346432558 - test_loss: 0.1467119231
-------- Save Best Model! --------
------- START EPOCH 38 -------
Epoch: 38 - loss: 0.1340045434 - test_loss: 0.1474164589
Early Stop Left: 4
------- START EPOCH 39 -------
Epoch: 39 - loss: 0.1333791322 - test_loss: 0.1462390802
-------- Save Best Model! --------
------- START EPOCH 40 -------
Epoch: 40 - loss: 0.1328696893 - test_loss: 0.1466897318
Early Stop Left: 4
------- START EPOCH 41 -------
Epoch: 41 - loss: 0.1324276062 - test_loss: 0.1462022305
-------- Save Best Model! --------
------- START EPOCH 42 -------
Epoch: 42 - loss: 0.1318857037 - test_loss: 0.1464613043
Early Stop Left: 4
------- START EPOCH 43 -------
Epoch: 43 - loss: 0.1312219675 - test_loss: 0.1464807475
Early Stop Left: 3
------- START EPOCH 44 -------
Epoch: 44 - loss: 0.1306652210 - test_loss: 0.1464967822
Early Stop Left: 2
------- START EPOCH 45 -------
Epoch: 45 - loss: 0.1301339686 - test_loss: 0.1460351306
-------- Save Best Model! --------
------- START EPOCH 46 -------
Epoch: 46 - loss: 0.1296729904 - test_loss: 0.1453980859
-------- Save Best Model! --------
------- START EPOCH 47 -------
Epoch: 47 - loss: 0.1293712717 - test_loss: 0.1470139249
Early Stop Left: 4
------- START EPOCH 48 -------
Epoch: 48 - loss: 0.1289820308 - test_loss: 0.1450088807
-------- Save Best Model! --------
------- START EPOCH 49 -------
Epoch: 49 - loss: 0.1286481427 - test_loss: 0.1458986300
Early Stop Left: 4
------- START EPOCH 50 -------
Epoch: 50 - loss: 0.1280088067 - test_loss: 0.1455729443
Early Stop Left: 3
------- START EPOCH 51 -------
Epoch: 51 - loss: 0.1278667711 - test_loss: 0.1453005053
Early Stop Left: 2
------- START EPOCH 52 -------
Epoch: 52 - loss: 0.1274889600 - test_loss: 0.1452175158
Early Stop Left: 1
------- START EPOCH 53 -------
Epoch: 53 - loss: 0.1272196986 - test_loss: 0.1457132341
Early Stop Left: 0
-------- Early Stop! --------
Validation start
  0%|          | 0/104 [00:00<?, ?it/s] 17%|█▋        | 18/104 [00:00<00:00, 174.63it/s] 35%|███▍      | 36/104 [00:00<00:00, 176.61it/s] 52%|█████▏    | 54/104 [00:00<00:00, 177.34it/s] 69%|██████▉   | 72/104 [00:00<00:00, 177.88it/s] 87%|████████▋ | 90/104 [00:00<00:00, 177.46it/s]100%|██████████| 104/104 [00:00<00:00, 178.52it/s]
Best micro threshold=0.247297, fscore=0.587
p,r,f1: 0.5107300622901019 0.690798091802327 0.5872710535402829
throttleing by fixed threshold: 0.5
p,r,f1: 0.6828755304188697 0.41919091966306854 0.5194881335701832
{'model': 'vit_large',
 'app': '437.leslie3d-s0.txt.xz',
 'validation': [{'method': 'micro precision f1',
                 'threshold': 0.24729731678962708,
                 'p': 0.5107300622901019,
                 'r': 0.690798091802327,
                 'f1': 0.5872710535402829},
                {'method': 'fixed threshold 0.5',
                 'threshold': 0.5,
                 'p': 0.6828755304188697,
                 'r': 0.41919091966306854,
                 'f1': 0.5194881335701832}]}
saving tensor pickle data
tensor data saved
saving test dataframe
done data saving for amm
